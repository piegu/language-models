{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning BERT (base or large) on a Question-Answering task by using the library adapter-transformers (script version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Credit**: [Hugging Face](https://huggingface.co/) and [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)\n",
    "- **Author**: [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/)\n",
    "- **Date**: 02/07/2021\n",
    "- **Blog post**: []()\n",
    "- **Link to the folder in github with this notebook and all necessary scripts**: [question-answering with adapters](https://github.com/piegu/language-models/tree/master/adapters/question-answering/)\n",
    "- **Link to the adapters in the AdapterHub**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective here is to **fine-tune a Masked Language Model (MLM) like BERT (base or large) for a QA task by training adapters (library [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)), not the embeddings and transformers layers of the MLM model**, and to compare results with BERT model fully fine-tune for the same task.\n",
    "\n",
    "The interest is obvious: if you need models for different NLP tasks, instead of fine-tuning and storing one model by NLP task, **you store only one MLM model and the trained tasks adapters which sizes are about 3% of the MLM model one**. More, the loading of these adapters in production is very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to fine-tune one of the [ü§ó Transformers](https://github.com/huggingface/transformers) model to a question answering task, which is the task of extracting the answer to a question from a given context. We will use the library [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers) and see how to easily load a dataset for these kinds of tasks and use the `Trainer` API to fine-tune a model on it.\n",
    "\n",
    "![Widget inference representing the QA task](images/question_answering_adapter.png)\n",
    "\n",
    "**Note:** This notebook finetunes models that answer question by taking a substring of a context, not by generating new text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook is built to run on any question answering task with the same format as SQUAD (version 1 or 2), with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a token classification head and a fast tokenizer (check on [this table](https://huggingface.co/transformers/index.html#bigtable) if this is the case). It might just need some small adjustments if you decide to use a different dataset than the one used here. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History and Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an adaptation of the following notebooks and scripts for **fine-tuning a (transformer) Masked Language Model (MLM) like BERT (base or large) on the QA task with any QA dataset** (we use here the [Portuguese Squad 1.1 dataset](https://forum.ailab.unb.br/t/datasets-em-portugues/251/4)):\n",
    "- **from [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)** | notebook [04_Cross_Lingual_Transfer.ipynb](https://github.com/Adapter-Hub/adapter-transformers/blob/master/notebooks/04_Cross_Lingual_Transfer.ipynb) and script [run_qa.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/question-answering/run_qa.py) (this script was adapted from the script [run_qa.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py) of HF)\n",
    "- **from [transformers](https://github.com/huggingface/transformers) of Hugging Face** | notebook [question_answering.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb) and script [run_qa.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py) \n",
    "\n",
    "In order to speed up the fine-tuning of the model on only one GPU, the library [DeepSpeed](https://www.deepspeed.ai/) could be used by applying the configuration provided by HF in the notebook [transformers + deepspeed CLI](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb) but as the library adapter-transformers is not synchronized with the last version of the library transformers of HF, we keep that option for the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major changes from original notebooks and scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script [run_qa.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/question-answering/run_qa.py) allows to evaluate the model performance against f1 metric at the end of each epoch, and not against validation loss as done in the notebook [question_answering.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb). This is very important as we consider the metric when selecting a model, not the loss. Therefore, we decided to launch this script inside this notebook (by simulating terminal command line) instead of running code in cells as done in the notebook of HF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More, we updated the script [run_qa.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/question-answering/run_qa.py) to [run_qa_adapter.py](https://github.com/piegu/language-models/blob/master/adapters/question-answering/run_qa_adapter.py) with the following changes:\n",
    "- **EarlyStopping** by selecting the model with the highest eval f1 (patience of 3 before ending the training)\n",
    "- **MAD-X 2.0** that allows not to train adapters in the last transformer layer (read page 6 of [UNKs Everywhere: Adapting Multilingual Language Models to New Scripts](https://arxiv.org/pdf/2012.15562.pdf))\n",
    "- **Stack method** for the lang and task adapters when a lang adapter is loaded ([doc](https://docs.adapterhub.ml/adapter_composition.html?highlight=stack#stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "#root path\n",
    "root = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "Pytorch: 1.9.0\n",
      "adapter-transformers: 2.0.1\n",
      "HF transformers: 4.5.1\n",
      "tokenizers: 0.10.3\n",
      "datasets: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import sys; print('python:',sys.version)\n",
    "\n",
    "import torch; print('Pytorch:',torch.__version__)\n",
    "\n",
    "import transformers; print('adapter-transformers:',transformers.__version__)\n",
    "import transformers; print('HF transformers:',transformers.__hf_version__)\n",
    "import tokenizers; print('tokenizers:',tokenizers.__version__)\n",
    "import datasets; print('datasets:',datasets.__version__)\n",
    "\n",
    "# import deepspeed; print('deepspeed:',deepspeed.__version__)\n",
    "\n",
    "# Versions used in the virtuel environment of this notebook:\n",
    "\n",
    "# python: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
    "# [GCC 7.5.0]\n",
    "# Pytorch: 1.9.0\n",
    "# adapter-transformers: 2.0.1\n",
    "# transformers: 4.5.1\n",
    "# tokenizers: 0.10.3\n",
    "# datasets: 1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create symbolic links to the folder with the scripts to run or download them in the same folder of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ln -s ~/adapter-transformers/examples/question-answering/run_qa_adapter.py\n",
    "# ln -s ~/adapter-transformers/examples/question-answering/trainer_qa.py\n",
    "# ln -s ~/adapter-transformers/examples/question-answering/utils_qa.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a MLM BERT base or large in the dataset language\n",
    "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "# model_checkpoint = \"neuralmind/bert-large-portuguese-cased\"\n",
    "\n",
    "# SQuAD 1.1 in Portuguese\n",
    "dataset_name = \"squad11pt\"\n",
    "\n",
    "# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n",
    "# answers are allowed or not).\n",
    "version_2_with_negative = False # If true, some of the examples do not have an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"qa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments\n",
    "batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_train_epochs = 10.\n",
    "early_stopping_patience = 3\n",
    "\n",
    "adam_epsilon = 1e-6\n",
    "\n",
    "fp16 = True\n",
    "ds = False # If True, we use DeepSpeed\n",
    "\n",
    "# best model\n",
    "load_best_model_at_end = True \n",
    "metric_for_best_model = \"f1\"\n",
    "greater_is_better = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train adapter\n",
    "train_adapter = True # we want to train an adapter\n",
    "load_adapter = None # we do not upload an existing adapter \n",
    "\n",
    "# lang adapter\n",
    "with_adapters_mlm = False # if False, we do not upload an existing lang adapter\n",
    "\n",
    "if with_adapters_mlm:\n",
    "    adapter_composition = \"stack\" # we will stack the lang and task adapters\n",
    "else:\n",
    "    adapter_composition = None\n",
    "\n",
    "# if True, do not put adapter in the last transformer layer\n",
    "madx2 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "n_gpu = 1 # train on just one GPU\n",
    "gpu = 0 # select the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select GPU 0\n",
    "import os\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "if gpu == 0:\n",
    "    os.environ['MASTER_PORT'] = '9996' # modify if RuntimeError: Address already in use # GPU 0\n",
    "elif gpu == 1:\n",
    "    os.environ['MASTER_PORT'] = '9995'\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = str(gpu)\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training arguments of the HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the training argument\n",
    "do_train = True \n",
    "do_eval = True \n",
    "\n",
    "# if you want to test the trainer, set up the following variables\n",
    "max_train_samples = 200 # None\n",
    "max_val_samples = 50 # None\n",
    "\n",
    "# epochs, bs, GA\n",
    "evaluation_strategy = \"epoch\" \n",
    "\n",
    "# fp16\n",
    "fp16_opt_level = 'O1'\n",
    "fp16_backend = \"auto\"\n",
    "fp16_full_eval = False\n",
    "\n",
    "# optimizer (AdamW)\n",
    "weight_decay = 0.01 # 0.0\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "\n",
    "# scheduler\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.0\n",
    "warmup_steps = 0\n",
    "\n",
    "# logs\n",
    "logging_strategy = \"steps\"\n",
    "logging_first_step = True # False\n",
    "logging_steps = 500     # if strategy = \"steps\"\n",
    "eval_steps = logging_steps # logging_steps\n",
    "\n",
    "# checkpoints\n",
    "save_strategy = \"epoch\" # steps\n",
    "save_steps = 500 # if save_strategy = \"steps\"\n",
    "save_total_limit = 1 # None\n",
    "\n",
    "# no cuda, seed\n",
    "no_cuda = False\n",
    "seed = 42\n",
    "\n",
    "# bar\n",
    "disable_tqdm = False # True\n",
    "remove_unused_columns = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder for training outputs\n",
    "\n",
    "outputs = model_checkpoint.replace('/','-') + '_' + dataset_name + '/'\n",
    "if with_adapters_mlm:\n",
    "    outputs = outputs + 'mlm_' + str(task) + '_AdCompo' + str(adapter_composition)\n",
    "else:\n",
    "    outputs = outputs + str(task)\n",
    "outputs = outputs \\\n",
    "+ '_lr' + str(learning_rate) \\\n",
    "+ '_bs' + str(batch_size) \\\n",
    "+ '_eps' + str(adam_epsilon) \\\n",
    "+ '_epochs' + str(num_train_epochs) \\\n",
    "+ '_wamlm' + str(with_adapters_mlm) \\\n",
    "+ '_madx2' + str(madx2) \\\n",
    "+ '_ds' + str(ds) \\\n",
    "+ '_fp16' + str(fp16) \\\n",
    "+ '_best' + str(load_best_model_at_end) \\\n",
    "+ '_metric' + str(metric_for_best_model)\n",
    "\n",
    "# path to outputs\n",
    "path_to_outputs = root/'models_outputs'/outputs\n",
    "\n",
    "# subfolder for model outputs\n",
    "output_dir = path_to_outputs/'output_dir' \n",
    "overwrite_output_dir = True # False\n",
    "\n",
    "# logs\n",
    "logging_dir = path_to_outputs/'logging_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum total input sequence length after tokenization. Sequences longer\n",
    "# than this will be truncated, sequences shorter will be padded.\n",
    "max_seq_length = 384\n",
    "\n",
    "# Whether to pad all samples to `max_seq_length`.\n",
    "# If False, will pad the samples dynamically when batching to the maximum length in the batch (which can\n",
    "# be faster on GPU but will be slower on TPU).\n",
    "pad_to_max_length = True\n",
    "    \n",
    "# The threshold used to select the null answer: if the best answer has a score that is less than\n",
    "# the score of the null answer minus this threshold, the null answer is selected for this example.\n",
    "# Only useful when `version_2_with_negative=True`.\n",
    "null_score_diff_threshold = 0.0\n",
    "\n",
    "# When splitting up a long document into chunks, how much stride to take between chunks\n",
    "doc_stride = 128\n",
    "    \n",
    "# The total number of n-best predictions to generate when looking for an answer.\n",
    "n_best_size = 20\n",
    " \n",
    "# The maximum length of an answer that can be generated. This is needed because the start\n",
    "# and end predictions are not conditioned on one another.\n",
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapters config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task adapter config\n",
    "adapter_config = \"pfeiffer\" # houlsby is possible, too\n",
    "adapter_non_linearity = 'gelu' # relu is possible, too\n",
    "adapter_reduction_factor = 16\n",
    "language = 'pt' # pt = Portuguese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lang adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_adapters_mlm:\n",
    "    \n",
    "    # hyperparameters used for fine-tuning the MLM with lang adapter\n",
    "    learning_rate_mlm = 1e-4\n",
    "    batch_size_mlm = 32\n",
    "    gradient_accumulation_steps_mlm = 1\n",
    "    adam_epsilon_mlm = 1e-6\n",
    "    num_train_epoch_mlm = 100.\n",
    "    madx2_mlm = madx2\n",
    "    ds_mlm = False\n",
    "    fp16_mlm = True\n",
    "    load_best_model_at_end_mlm = True\n",
    "    metric_for_best_model_mlm = \"loss\"\n",
    "    \n",
    "    # path to lang adapter\n",
    "    outputs_mlm = model_checkpoint.replace('/','-') + '_' + dataset_name + '/mlm' \\\n",
    "    + '_lr' + str(learning_rate_mlm) \\\n",
    "    + '_bs' + str(batch_size_mlm) \\\n",
    "    + '_GAS' + str(gradient_accumulation_steps_mlm) \\\n",
    "    + '_eps' + str(adam_epsilon_mlm) \\\n",
    "    + '_epochs' + str(num_train_epoch_mlm) \\\n",
    "    + '_madx2' + str(madx2_mlm) \\\n",
    "    + '_ds' + str(ds_mlm) \\\n",
    "    + '_fp16' + str(fp16_mlm) \\\n",
    "    + '_best' + str(load_best_model_at_end_mlm) \\\n",
    "    + '_metric' + str(metric_for_best_model_mlm)\n",
    "\n",
    "    path_to_outputs = root/'models_outputs'/outputs_mlm\n",
    "    \n",
    "    # Config of the lang adapter\n",
    "    lang_adapter_path = path_to_outputs/'adapters-mlm/'\n",
    "    \n",
    "    load_lang_adapter = lang_adapter_path\n",
    "    lang_adapter_config = str(lang_adapter_path + \"/adapter_config.json\")\n",
    "    lang_adapter_non_linearity = 'gelu'\n",
    "    lang_adapter_reduction_factor = 2\n",
    "    language_mlm = language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r_n9OWV3l-Q"
   },
   "source": [
    "## 6. Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if dataset_name == \"squad11pt\":\n",
    "    \n",
    "#     # create dataset folder \n",
    "#     path_to_dataset = root/'data'/dataset_name\n",
    "#     path_to_dataset.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "#     # Get dataset SQUAD in Portuguese\n",
    "#     %cd {path_to_dataset}\n",
    "#     !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn\" -O squad-pt.tar.gz && rm -rf /tmp/cookies.txt\n",
    "\n",
    "#     # unzip \n",
    "#     !tar -xvf squad-pt.tar.gz\n",
    "\n",
    "#     # Get the train and validation json file in the HF script format \n",
    "#     # inspiration: file squad.py at https://github.com/huggingface/datasets/tree/master/datasets/squad\n",
    "\n",
    "#     import json \n",
    "#     files = ['squad-train-v1.1.json','squad-dev-v1.1.json']\n",
    "\n",
    "#     for file in files:\n",
    "\n",
    "#         # Opening JSON file & returns JSON object as a dictionary \n",
    "#         f = open(file, encoding=\"utf-8\") \n",
    "#         data = json.load(f) \n",
    "\n",
    "#         # Iterating through the json list \n",
    "#         entry_list = list()\n",
    "#         id_list = list()\n",
    "\n",
    "#         for row in data['data']: \n",
    "#             title = row['title']\n",
    "\n",
    "#             for paragraph in row['paragraphs']:\n",
    "#                 context = paragraph['context']\n",
    "\n",
    "#                 for qa in paragraph['qas']:\n",
    "#                     entry = {}\n",
    "\n",
    "#                     qa_id = qa['id']\n",
    "#                     question = qa['question']\n",
    "#                     answers = qa['answers']\n",
    "\n",
    "#                     entry['id'] = qa_id\n",
    "#                     entry['title'] = title.strip()\n",
    "#                     entry['context'] = context.strip()\n",
    "#                     entry['question'] = question.strip()\n",
    "\n",
    "#                     answer_starts = [answer[\"answer_start\"] for answer in answers]\n",
    "#                     answer_texts = [answer[\"text\"].strip() for answer in answers]\n",
    "#                     entry['answers'] = {}\n",
    "#                     entry['answers']['answer_start'] = answer_starts\n",
    "#                     entry['answers']['text'] = answer_texts\n",
    "\n",
    "#                     entry_list.append(entry)\n",
    "\n",
    "#         reverse_entry_list = entry_list[::-1]\n",
    "\n",
    "#         # for entries with same id, keep only last one (corrected texts by the group Deep Learning Brasil)\n",
    "#         unique_ids_list = list()\n",
    "#         unique_entry_list = list()\n",
    "#         for entry in reverse_entry_list:\n",
    "#             qa_id = entry['id']\n",
    "#             if qa_id not in unique_ids_list:\n",
    "#                 unique_ids_list.append(qa_id)\n",
    "#                 unique_entry_list.append(entry)\n",
    "\n",
    "#         # Closing file \n",
    "#         f.close() \n",
    "\n",
    "#         new_dict = {}\n",
    "#         new_dict['data'] = unique_entry_list\n",
    "\n",
    "#         file_name = 'pt_' + str(file)\n",
    "#         with open(file_name, 'w') as json_file:\n",
    "#             json.dump(new_dict, json_file)\n",
    "            \n",
    "# %cd {root}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1-9jepM3l-W"
   },
   "source": [
    "You can replace the dataset above with any dataset hosted on [the hub](https://huggingface.co/datasets) or use your own files. Just uncomment the following cell and replace the paths with values that will lead to your files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uxSaGa_l3l-W"
   },
   "outputs": [],
   "source": [
    "# datasets = load_dataset(\"text\", data_files={\"train\": path_to_train.txt, \"validation\": path_to_validation.txt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY1SwIrY3l-a"
   },
   "source": [
    "You can also load datasets from a csv or a JSON file, see the [full documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "if dataset_name == \"squad11pt\":\n",
    "    \n",
    "    # dataset folder \n",
    "    path_to_dataset = root/'data'/dataset_name\n",
    "    \n",
    "    # paths to files\n",
    "    train_file = str(path_to_dataset/'pt_squad-train-v1.1.json')\n",
    "    validation_file = str(path_to_dataset/'pt_squad-dev-v1.1.json')\n",
    "    \n",
    "    datasets = load_dataset('json', \n",
    "                            data_files={'train': train_file, \\\n",
    "                                        'validation': validation_file, \\\n",
    "                                       }, \n",
    "                            field='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5735c47ae853931400426b64',\n",
       " 'title': 'Kathmandu',\n",
       " 'context': 'A maioria das cozinhas encontradas em Katmandu n√£o √© vegetariana. No entanto, a pr√°tica do vegetarianismo n√£o √© incomum, e a culin√°ria vegetariana pode ser encontrada em toda a cidade. O consumo de carne bovina √© muito incomum e considerado tabu em muitos lugares. Buff (carne de b√∫falo Marinho) √© muito comum. H√° uma forte tradi√ß√£o de consumo de buffs em Katmandu, especialmente entre Newars, que n√£o √© encontrado em outras partes do Nepal. O consumo de carne de porco era considerado tabu at√© algumas d√©cadas atr√°s. Devido √† mistura com a cozinha Kirat do leste do Nepal, a carne de porco encontrou um lugar nos pratos de Katmandu. Uma popula√ß√£o marginal de hindus e mu√ßulmanos devotos o considera tabu. Os mu√ßulmanos pro√≠bem comer buff a partir do Alcor√£o, enquanto os hindus comem todas as variedades, exceto a carne de vaca, pois consideram a vaca uma deusa e s√≠mbolo da pureza. O caf√© da manh√£ principal para moradores e visitantes √© principalmente Momo ou Chowmein.',\n",
       " 'question': 'De que animal vem o lustre?',\n",
       " 'answers': {'answer_start': [280], 'text': ['b√∫falo Marinho']}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ur5sNUcZ3l-g"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1Uk8NROQ3l-k",
    "outputId": "a822dcec-51e3-4dba-c73c-dba9e0301726"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>572813372ca10214002d9d58</td>\n",
       "      <td>Strasbourg</td>\n",
       "      <td>Al√©m da catedral, Estrasburgo abriga v√°rias outras igrejas medievais que sobreviveram √†s muitas guerras e destrui√ß√µes que assolaram a cidade: a rom√¢nica √âglise Saint-√âtienne, parcialmente destru√≠da em 1944 por bombardeios aliados, parte rom√¢nica, parte g√≥tica, muito a grande Igreja Saint-Thomas, com seu √≥rg√£o Silbermann, no qual Wolfgang Amadeus Mozart e Albert Schweitzer tocavam, o protestante da igreja g√≥tica Saint-Pierre-le-Jeune com sua cripta que remonta ao s√©culo VII e seu claustro parcialmente do s√©culo XI, o g√≥tico √âglise Saint-Guillaume, com seus belos vitrais e m√≥veis do in√≠cio da Renascen√ßa, a g√≥tica √âglise Saint-Jean, a parte g√≥tica, a parte Art Nouveau √âglise Sainte-Madeleine etc. A igreja neog√≥tica Saint-Pierre-le-Vieux Catholique ( h√° tamb√©m uma igreja adjacente (Saint-Pierre-le-Vieux protestante) serve de santu√°rio para v√°rios altares trabalhados e pintados em madeira do s√©culo XV, vindos de outras igrejas agora destru√≠das e instaladas l√° para exibi√ß√£o p√∫blica. Entre os numerosos edif√≠cios medievais seculares, destaca-se o monumental Ancienne Douane (antiga alf√¢ndega).</td>\n",
       "      <td>Onde est√° localizado o √≥rg√£o Silbermann?</td>\n",
       "      <td>{'answer_start': [276], 'text': ['Igreja Saint-Thomas']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57310cad05b4da19006bcd2b</td>\n",
       "      <td>Immaculate_Conception</td>\n",
       "      <td>A popularidade dessa representa√ß√£o particular da Imaculada Concei√ß√£o se espalhou por todo o resto da Europa e desde ent√£o continua sendo a representa√ß√£o art√≠stica mais conhecida do conceito: em um reino celestial, momentos ap√≥s sua cria√ß√£o, o esp√≠rito de Maria (na forma de um jovem) olha com rever√™ncia (ou inclina a cabe√ßa para) Deus. A lua est√° sob seus p√©s e um halo de doze estrelas envolve sua cabe√ßa, possivelmente uma refer√™ncia a \"uma mulher vestida de sol\" em Apocalipse 12: 1-2. Imagens adicionais podem incluir nuvens, uma luz dourada e querubins. Em algumas pinturas, os querubins est√£o segurando l√≠rios e rosas, flores frequentemente associadas a Maria.</td>\n",
       "      <td>Sobre o que Maria se ap√≥ia neste s√≠mbolo?</td>\n",
       "      <td>{'answer_start': [339], 'text': ['lua est√° sob seus p√©s']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>572f79b4a23a5019007fc66a</td>\n",
       "      <td>Han_dynasty</td>\n",
       "      <td>As culturas b√°sicas mais comuns consumidas durante o Han foram trigo, cevada, milheto, milho mo√≠do, arroz e feij√£o. Frutas e legumes comumente consumidos incluem castanhas, peras, ameixas, p√™ssegos, mel√µes, damascos, morangos, morangos vermelhos, jujubas, caba√ßa, brotos de bambu, mostarda e taro. Os animais domesticados que tamb√©m foram comidos inclu√≠am galinhas, patos mandarim, gansos, vacas, ovelhas, porcos, camelos e c√£es (v√°rios tipos foram criados especificamente para alimenta√ß√£o, enquanto a maioria foi usada como animais de estima√ß√£o). Tartarugas e peixes foram retirados de riachos e lagos. Foram consumidos ca√ßa comum, como coruja, fais√£o, pega, veado-sika e perdiz de bambu chinesa. Os temperos inclu√≠am a√ß√∫car, mel, sal e molho de soja. Cerveja e vinho eram consumidos regularmente.</td>\n",
       "      <td>Quais eram os c√£es nessa √©poca com maior probabilidade de serem considerados?</td>\n",
       "      <td>{'answer_start': [525], 'text': ['animais de estima√ß√£o']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5731bfcc0fdd8d15006c64fc</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>As condi√ß√µes econ√¥micas come√ßaram a melhorar consideravelmente, ap√≥s um per√≠odo de estagna√ß√£o, devido √† ado√ß√£o de pol√≠ticas econ√¥micas mais liberais pelo governo, bem como ao aumento das receitas do turismo e do mercado de a√ß√µes em expans√£o. Em seu relat√≥rio anual, o Fundo Monet√°rio Internacional (FMI) classificou o Egito como um dos principais pa√≠ses do mundo que est√° realizando reformas econ√¥micas. Algumas grandes reformas econ√¥micas empreendidas pelo governo desde 2003 incluem uma redu√ß√£o dr√°stica de costumes e tarifas. Uma nova lei tribut√°ria implementada em 2005 reduziu os impostos corporativos de 40% para os atuais 20%, resultando em um aumento declarado de 100% na receita tribut√°ria at√© o ano de 2006.</td>\n",
       "      <td>Qual √°rea de neg√≥cios cresceu ultimamente no Egito?</td>\n",
       "      <td>{'answer_start': [212], 'text': ['mercado de a√ß√µes']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5707066b9e06ca38007e92b3</td>\n",
       "      <td>Letter_case</td>\n",
       "      <td>Em latim, foram encontrados papiros de Herculano, datados de 79 dC (quando foram destru√≠dos), que foram escritos em cursiva romana antiga, onde as primeiras formas de letras min√∫sculas \"d\", \"h\" e \"r\", por exemplo, j√° pode ser reconhecido. Segundo o papirologista Knut Kleve, \"a teoria, portanto, de que as letras min√∫sculas foram desenvolvidas a partir dos unciais do s√©culo V e dos min√∫sculos carol√≠ngios do s√©culo IX parece estar errada\". Havia letras mai√∫sculas e min√∫sculas, mas a diferen√ßa entre as duas variantes era inicialmente estil√≠stica, e n√£o ortogr√°fica, e o sistema de escrita ainda era basicamente unicameral: um determinado documento manuscrito podia usar um estilo ou outro, mas esses n√£o eram mistos. As l√≠nguas europ√©ias, exceto o grego antigo e o latim, n√£o fizeram a distin√ß√£o entre os casos antes de 1300. [cita√ß√£o necess√°rio]</td>\n",
       "      <td>Quais idiomas de continentes com poucas exce√ß√µes geralmente n√£o utilizavam distin√ß√£o entre mai√∫sculas e min√∫sculas at√© cerca de 1300?</td>\n",
       "      <td>{'answer_start': [730], 'text': ['europ√©ias']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>572783acdd62a815002e9f4a</td>\n",
       "      <td>Child_labour</td>\n",
       "      <td>No Brasil, a idade m√≠nima para o trabalho foi identificada como catorze devido a cont√≠nuas emendas constitucionais ocorridas em 1934, 1937 e 1946. No entanto, devido a uma mudan√ßa na ditadura militar pelos anos 80, a restri√ß√£o de idade m√≠nima foi reduzida para doze anos de idade, mas foi revisada devido a relatos de condi√ß√µes perigosas e perigosas de trabalho em 1988. Isso levou √† idade m√≠nima sendo novamente aumentada para 14. Outro conjunto de restri√ß√µes foi aprovado em 1998 que restringia os tipos de trabalho que os jovens podiam participar, como trabalhos considerados perigosos, como a opera√ß√£o de equipamentos de constru√ß√£o ou certos tipos de trabalho na f√°brica. Embora tenham sido tomadas muitas medidas para reduzir o risco e a ocorr√™ncia de trabalho infantil, ainda h√° um n√∫mero elevado de crian√ßas e adolescentes trabalhando com menos de quatorze anos no Brasil. Somente nos anos 80 foi descoberto que quase nove milh√µes de crian√ßas no Brasil trabalhavam ilegalmente e n√£o participavam de atividades infantis tradicionais que ajudam a desenvolver importantes experi√™ncias de vida.</td>\n",
       "      <td>Qual a idade m√≠nima para trabalhar no Brasil?</td>\n",
       "      <td>{'answer_start': [855], 'text': ['quatorze']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5726f391dd62a815002e9609</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>A presta√ß√£o de servi√ßos de sa√∫de na Nig√©ria √© uma responsabilidade simult√¢nea dos tr√™s n√≠veis de governo no pa√≠s e do setor privado. A Nig√©ria vem reorganizando seu sistema de sa√∫de desde a Iniciativa Bamako de 1987, que promoveu formalmente m√©todos baseados na comunidade para aumentar a acessibilidade de medicamentos e servi√ßos de sa√∫de √† popula√ß√£o, em parte implementando taxas de usu√°rio. A nova estrat√©gia aumentou drasticamente a acessibilidade por meio da reforma da sa√∫de baseada na comunidade, resultando em uma presta√ß√£o de servi√ßos mais eficiente e equitativa. Uma estrat√©gia abrangente de abordagem foi estendida a todas as √°reas da assist√™ncia m√©dica, com subsequente melhoria nos indicadores de assist√™ncia m√©dica e melhoria na efici√™ncia e custo dos servi√ßos de sa√∫de.</td>\n",
       "      <td>A Nig√©ria est√° adicionando que tipo de custos ao seu sistema de sa√∫de?</td>\n",
       "      <td>{'answer_start': [376], 'text': ['taxas de usu√°rio']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56d130f217492d1400aabbc3</td>\n",
       "      <td>Kanye_West</td>\n",
       "      <td>West foi preso novamente em 14 de novembro de 2008 no hotel Hilton, perto de Gateshead, ap√≥s outra briga envolvendo um fot√≥grafo do lado de fora da famosa boate Tup Tup Palace em Newcastle upon Tyne. Mais tarde, ele foi libertado \"sem mais a√ß√µes\", segundo um porta-voz da pol√≠cia.</td>\n",
       "      <td>Onde Kanye foi preso pela segunda vez?</td>\n",
       "      <td>{'answer_start': [54], 'text': ['hotel Hilton, perto de Gateshead']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56dd2d7d9a695914005b9530</td>\n",
       "      <td>Prime_minister</td>\n",
       "      <td>Walpole sempre negou que ele era \"primeiro ministro\" e, durante todo o s√©culo 18, parlamentares e juristas continuaram negando que qualquer posi√ß√£o desse tipo fosse conhecida pela Constitui√ß√£o. George II e George III fizeram esfor√ßos √°rduos para recuperar o poder pessoal do monarca, mas a complexidade e as despesas crescentes do governo fizeram com que um ministro que pudesse comandar a lealdade dos Comuns fosse cada vez mais necess√°rio. A longa perman√™ncia do primeiro ministro da guerra William Pitt, o Jovem (1783-1801), combinada com a doen√ßa mental de George III, consolidou o poder do posto. O t√≠tulo foi mencionado pela primeira vez em documentos do governo durante a administra√ß√£o de Benjamin Disraeli, mas n√£o apareceu na Ordem Brit√¢nica formal de preced√™ncia at√© 1905.</td>\n",
       "      <td>Al√©m de Walpole, quem mais negou que o primeiro ministro n√£o existisse?</td>\n",
       "      <td>{'answer_start': [82], 'text': ['parlamentares e juristas']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>570c3f1dec8fbc190045be1d</td>\n",
       "      <td>Mary_(mother_of_Jesus)</td>\n",
       "      <td>As doutrinas da Assun√ß√£o ou Dormi√ß√£o de Maria est√£o relacionadas √† sua morte e suposi√ß√£o corporal ao c√©u. A Igreja Cat√≥lica Romana definiu dogmaticamente a doutrina da Assun√ß√£o, que foi feita em 1950 pelo Papa Pio XII no Munificentissimus Deus. Se a Virgem Maria morreu ou n√£o, n√£o √© definido dogmaticamente, embora uma refer√™ncia √† morte de Maria seja feita em Munificentissimus Deus. Na Igreja Ortodoxa Oriental, acredita-se na Assun√ß√£o da Virgem Maria e √© celebrada com sua Dormi√ß√£o, onde eles acreditam que ela morreu.</td>\n",
       "      <td>Em qual documento papal foi definido o dogma da Assun√ß√£o?</td>\n",
       "      <td>{'answer_start': [221], 'text': ['Munificentissimus Deus']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training + Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup environment variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magic command `%env` corresponds to `export` in linux. It allows to setup the values of all arguments of the script `run_qa_adapter.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = {\n",
    "'n_gpu':n_gpu,\n",
    "'gpu':gpu,\n",
    "'CUDA_VISIBLE_DEVICES':gpu,\n",
    "'model_name_or_path':model_checkpoint,\n",
    "'dataset_name':dataset_name,\n",
    "'train_file':train_file,\n",
    "'validation_file':validation_file,\n",
    "'do_train':do_train,\n",
    "'do_eval':do_eval,\n",
    "'max_train_samples':max_train_samples,\n",
    "'max_val_samples':max_train_samples,\n",
    "'output_dir':output_dir,\n",
    "'overwrite_output_dir':overwrite_output_dir,\n",
    "'max_seq_length':max_seq_length,\n",
    "'pad_to_max_length':pad_to_max_length,\n",
    "'null_score_diff_threshold':null_score_diff_threshold,\n",
    "'doc_stride':doc_stride,\n",
    "'n_best_size':n_best_size,\n",
    "'max_answer_length':max_answer_length,\n",
    "'evaluation_strategy':evaluation_strategy,\n",
    "'per_device_train_batch_size':batch_size,\n",
    "'per_device_eval_batch_size':batch_size,\n",
    "'gradient_accumulation_steps':gradient_accumulation_steps,\n",
    "'learning_rate':learning_rate,\n",
    "'weight_decay':weight_decay,\n",
    "'adam_beta1':adam_beta1,\n",
    "'adam_beta2':adam_beta2,\n",
    "'adam_epsilon':adam_epsilon,\n",
    "'num_train_epochs':num_train_epochs,\n",
    "'warmup_ratio':warmup_ratio,\n",
    "'warmup_steps':warmup_steps,\n",
    "'logging_dir':logging_dir,\n",
    "'logging_strategy':logging_strategy,\n",
    "'logging_first_step':logging_first_step,\n",
    "'logging_steps':logging_steps,\n",
    "'eval_steps':eval_steps,\n",
    "'save_strategy':save_strategy,\n",
    "'save_steps':save_steps,\n",
    "'save_total_limit':save_total_limit,\n",
    "'no_cuda':no_cuda,\n",
    "'seed':seed,\n",
    "'fp16':fp16,\n",
    "'fp16_opt_level':fp16_opt_level,\n",
    "'fp16_backend':fp16_backend,\n",
    "'fp16_full_eval':fp16_full_eval,\n",
    "'disable_tqdm':disable_tqdm,\n",
    "'remove_unused_columns':remove_unused_columns,\n",
    "'load_best_model_at_end':load_best_model_at_end,\n",
    "'metric_for_best_model':metric_for_best_model,\n",
    "'greater_is_better':greater_is_better,\n",
    "'early_stopping_patience':early_stopping_patience,\n",
    "'madx2':madx2,\n",
    "'train_adapter':train_adapter,\n",
    "'adapter_config':adapter_config,\n",
    "'adapter_non_linearity':adapter_non_linearity,\n",
    "'adapter_reduction_factor':adapter_reduction_factor,\n",
    "'language':language,\n",
    "'adapter_composition':adapter_composition\n",
    "}\n",
    "\n",
    "if with_adapters_mlm:\n",
    "    envs['load_lang_adapter']=load_lang_adapter\n",
    "    envs['lang_adapter_config']=lang_adapter_config\n",
    "    envs['lang_adapter_non_linearity']=lang_adapter_non_linearity\n",
    "    envs['lang_adapter_reduction_factor']=lang_adapter_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in envs.items():\n",
    "    %env {k}={v}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the output_dir (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can launch the training :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy/paste/uncomment the 2 following lines in the following cell if you want to limit the number of data (useful for testing)\n",
    "# --max_train_samples $max_train_samples \\\n",
    "# --max_val_samples $max_val_samples \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if with_adapters_mlm:\n",
    "    # with lang adapter\n",
    "    !python -m torch.distributed.launch --nproc_per_node=$n_gpu run_qa_adapter.py \\\n",
    "    --model_name_or_path $model_checkpoint \\\n",
    "    --train_file $train_file \\\n",
    "    --validation_file $validation_file \\\n",
    "    --do_train $do_train \\\n",
    "    --do_eval $do_eval \\\n",
    "    --output_dir $output_dir \\\n",
    "    --overwrite_output_dir $overwrite_output_dir \\\n",
    "    --max_seq_length $max_seq_length \\\n",
    "    --pad_to_max_length $pad_to_max_length \\\n",
    "    --null_score_diff_threshold $null_score_diff_threshold \\\n",
    "    --doc_stride $doc_stride \\\n",
    "    --n_best_size $n_best_size \\\n",
    "    --max_answer_length $max_answer_length \\\n",
    "    --evaluation_strategy $evaluation_strategy \\\n",
    "    --per_device_train_batch_size $batch_size \\\n",
    "    --per_device_eval_batch_size $batch_size \\\n",
    "    --gradient_accumulation_steps $gradient_accumulation_steps \\\n",
    "    --learning_rate $learning_rate \\\n",
    "    --weight_decay $weight_decay \\\n",
    "    --adam_beta1 $adam_beta1 \\\n",
    "    --adam_beta2 $adam_beta2 \\\n",
    "    --adam_epsilon $adam_epsilon \\\n",
    "    --num_train_epochs $num_train_epochs \\\n",
    "    --warmup_ratio $warmup_ratio \\\n",
    "    --warmup_steps $warmup_steps \\\n",
    "    --logging_dir $logging_dir \\\n",
    "    --logging_strategy $logging_strategy \\\n",
    "    --logging_first_step $logging_first_step \\\n",
    "    --logging_steps $logging_steps \\\n",
    "    --eval_steps $eval_steps \\\n",
    "    --save_strategy $save_strategy \\\n",
    "    --save_steps $save_steps \\\n",
    "    --save_total_limit $save_total_limit \\\n",
    "    --no_cuda $no_cuda \\\n",
    "    --seed $seed \\\n",
    "    --fp16 $fp16 \\\n",
    "    --fp16_opt_level $fp16_opt_level \\\n",
    "    --fp16_backend $fp16_backend \\\n",
    "    --fp16_full_eval $fp16_full_eval \\\n",
    "    --disable_tqdm $disable_tqdm \\\n",
    "    --remove_unused_columns $remove_unused_columns \\\n",
    "    --load_best_model_at_end $load_best_model_at_end \\\n",
    "    --metric_for_best_model $metric_for_best_model \\\n",
    "    --greater_is_better $greater_is_better \\\n",
    "    --early_stopping_patience $early_stopping_patience \\\n",
    "    --madx2 $madx2 \\\n",
    "    --train_adapter $train_adapter \\\n",
    "    --adapter_config $adapter_config \\\n",
    "    --adapter_non_linearity $adapter_non_linearity \\\n",
    "    --adapter_reduction_factor $adapter_reduction_factor \\\n",
    "    --language $language \\\n",
    "    --adapter_composition $adapter_composition \\\n",
    "    --load_lang_adapter $load_lang_adapter \\\n",
    "    --lang_adapter_config $lang_adapter_config \\\n",
    "    --lang_adapter_non_linearity $lang_adapter_non_linearity \\\n",
    "    --lang_adapter_reduction_factor $lang_adapter_reduction_factor\n",
    "else:\n",
    "    # without lang adapter\n",
    "    !python -m torch.distributed.launch --nproc_per_node=$n_gpu run_qa_adapter.py \\\n",
    "    --model_name_or_path $model_checkpoint \\\n",
    "    --train_file $train_file \\\n",
    "    --validation_file $validation_file \\\n",
    "    --do_train $do_train \\\n",
    "    --do_eval $do_eval \\\n",
    "    --output_dir $output_dir \\\n",
    "    --overwrite_output_dir $overwrite_output_dir \\\n",
    "    --max_seq_length $max_seq_length \\\n",
    "    --pad_to_max_length $pad_to_max_length \\\n",
    "    --null_score_diff_threshold $null_score_diff_threshold \\\n",
    "    --doc_stride $doc_stride \\\n",
    "    --n_best_size $n_best_size \\\n",
    "    --max_answer_length $max_answer_length \\\n",
    "    --evaluation_strategy $evaluation_strategy \\\n",
    "    --per_device_train_batch_size $batch_size \\\n",
    "    --per_device_eval_batch_size $batch_size \\\n",
    "    --gradient_accumulation_steps $gradient_accumulation_steps \\\n",
    "    --learning_rate $learning_rate \\\n",
    "    --weight_decay $weight_decay \\\n",
    "    --adam_beta1 $adam_beta1 \\\n",
    "    --adam_beta2 $adam_beta2 \\\n",
    "    --adam_epsilon $adam_epsilon \\\n",
    "    --num_train_epochs $num_train_epochs \\\n",
    "    --warmup_ratio $warmup_ratio \\\n",
    "    --warmup_steps $warmup_steps \\\n",
    "    --logging_dir $logging_dir \\\n",
    "    --logging_strategy $logging_strategy \\\n",
    "    --logging_first_step $logging_first_step \\\n",
    "    --logging_steps $logging_steps \\\n",
    "    --eval_steps $eval_steps \\\n",
    "    --save_strategy $save_strategy \\\n",
    "    --save_steps $save_steps \\\n",
    "    --save_total_limit $save_total_limit \\\n",
    "    --no_cuda $no_cuda \\\n",
    "    --seed $seed \\\n",
    "    --fp16 $fp16 \\\n",
    "    --fp16_opt_level $fp16_opt_level \\\n",
    "    --fp16_backend $fp16_backend \\\n",
    "    --fp16_full_eval $fp16_full_eval \\\n",
    "    --disable_tqdm $disable_tqdm \\\n",
    "    --remove_unused_columns $remove_unused_columns \\\n",
    "    --load_best_model_at_end $load_best_model_at_end \\\n",
    "    --metric_for_best_model $metric_for_best_model \\\n",
    "    --greater_is_better $greater_is_better \\\n",
    "    --early_stopping_patience $early_stopping_patience \\\n",
    "    --madx2 $madx2 \\\n",
    "    --train_adapter $train_adapter \\\n",
    "    --adapter_config $adapter_config \\\n",
    "    --adapter_non_linearity $adapter_non_linearity \\\n",
    "    --adapter_reduction_factor $adapter_reduction_factor \\\n",
    "    --language $language \\\n",
    "    --adapter_composition $adapter_composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "Saving nbest_preds to (...)/mnt/home/pierre/course-v4/nbs/MLM/language-modeling/models_outputs/neuralmind-bert-base-portuguese-cased_squad11pt/qa_lr0.0001_bs16_eps1e-06_epochs10.0_wamlmFalse_madx2True_dsFalse_fp16True_bestTrue_metricf1/output_dir/eval_nbest_predictions.json.\n",
    "\n",
    "[INFO|trainer_pt_utils.py:722] 2021-07-01 19:40:37,300 >> ***** eval metrics *****\n",
    "[INFO|trainer_pt_utils.py:727] 2021-07-01 19:40:37,300 >>   epoch        =    10.0\n",
    "[INFO|trainer_pt_utils.py:727] 2021-07-01 19:40:37,300 >>   eval_f1      = 82.0608\n",
    "[INFO|trainer_pt_utils.py:727] 2021-07-01 19:40:37,300 >>   eval_samples =   10917\n",
    "[INFO|trainer_pt_utils.py:727] 2021-07-01 19:40:37,300 >>   exact_match  = 69.9054\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3244\r\n",
      "drwxrwxr-x 2 pierre pierre    4096 Jul  1 19:35 .\r\n",
      "drwxrwxr-x 4 pierre pierre    4096 Jul  1 19:37 ..\r\n",
      "-rw-rw-r-- 1 pierre pierre     642 Jul  1 19:35 adapter_config.json\r\n",
      "-rw-rw-r-- 1 pierre pierre     240 Jul  1 19:35 head_config.json\r\n",
      "-rw-rw-r-- 1 pierre pierre 3294305 Jul  1 19:35 pytorch_adapter.bin\r\n",
      "-rw-rw-r-- 1 pierre pierre    7143 Jul  1 19:35 pytorch_model_head.bin\r\n"
     ]
    }
   ],
   "source": [
    "# folder of the saved task adapter\n",
    "!ls -al {output_dir/task}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can push the saved adapter + head to the [AdapterHub](https://adapterhub.ml/) (follow instructions at [Contributing to Adapter Hub](https://docs.adapterhub.ml/contributing.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = os.getenv('PATH')\n",
    "# replace xxxx by your username on your server (ex: paulo)\n",
    "# replace yyyy by the name of the virtual environment of this notebook (ex: adapter-transformers)\n",
    "%env PATH=/mnt/home/xxxx/anaconda3/envs/yyyy/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6067 (pid 1172553), started 6:53:10 ago. (Use '!kill 1172553' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-de5dcd628dee24b9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-de5dcd628dee24b9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6067;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "%tensorboard --logdir {logging_dir} --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Application QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true,
    "id": "Ckl2Fzn3is0F"
   },
   "outputs": [],
   "source": [
    "### import transformers\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true,
    "id": "IJA9CgOBis0F",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "model_qa = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "tokenizer_qa = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true,
    "id": "NiuPQTxuzRrm"
   },
   "outputs": [],
   "source": [
    "# load the language adapter\n",
    "if with_adapters_mlm:\n",
    "    task_mlm_load_as = 'mlm'\n",
    "    lang_adapter_name = model_qa.load_adapter(\n",
    "        load_lang_adapter,\n",
    "        config=lang_adapter_config,\n",
    "        load_as=task_mlm_load_as,\n",
    "        with_head=False\n",
    "    )\n",
    "else:\n",
    "    lang_adapter_name = None\n",
    "\n",
    "# load adapter\n",
    "if train_adapter:\n",
    "    task_name = 'qa'\n",
    "    load_adapter_qa = str(output_dir/task_name)\n",
    "    adapter_name = model_qa.load_adapter(\n",
    "        load_adapter_qa,\n",
    "        config=adapter_config,\n",
    "        load_as=task_name,\n",
    "        with_head=True\n",
    "    )\n",
    "else:\n",
    "    adapter_name = None\n",
    "    \n",
    "if train_adapter:\n",
    "    # Set the adapters to be used in every forward pass\n",
    "    if lang_adapter_name:\n",
    "        model_qa.set_active_adapters([lang_adapter_name, adapter_name])\n",
    "    else:\n",
    "        model_qa.set_active_adapters([adapter_name])\n",
    "else:\n",
    "    # Set the adapters to be used in every forward pass\n",
    "    if lang_adapter_name:\n",
    "        model_qa.set_active_adapters([lang_adapter_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"question-answering\", model=model_qa, tokenizer=tokenizer_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://pt.wikipedia.org/wiki/Pandemia_de_COVID-19\n",
    "context = r\"\"\"A pandemia de COVID-19, tamb√©m conhecida como pandemia de coronav√≠rus, √© uma pandemia em curso de COVID-19, \n",
    "uma doen√ßa respirat√≥ria causada pelo coronav√≠rus da s√≠ndrome respirat√≥ria aguda grave 2 (SARS-CoV-2). \n",
    "O v√≠rus tem origem zoon√≥tica e o primeiro caso conhecido da doen√ßa remonta a dezembro de 2019 em Wuhan, na China. \n",
    "Em 20 de janeiro de 2020, a Organiza√ß√£o Mundial da Sa√∫de (OMS) classificou o surto \n",
    "como Emerg√™ncia de Sa√∫de P√∫blica de √Çmbito Internacional e, em 11 de mar√ßo de 2020, como pandemia. \n",
    "Em 18 de junho de 2021, 177 349 274 casos foram confirmados em 192 pa√≠ses e territ√≥rios, \n",
    "com 3 840 181 mortes atribu√≠das √† doen√ßa, tornando-se uma das pandemias mais mortais da hist√≥ria.\n",
    "Os sintomas de COVID-19 s√£o altamente vari√°veis, variando de nenhum a doen√ßas com risco de morte. \n",
    "O v√≠rus se espalha principalmente pelo ar quando as pessoas est√£o perto umas das outras. \n",
    "Ele deixa uma pessoa infectada quando ela respira, tosse, espirra ou fala e entra em outra pessoa pela boca, nariz ou olhos.\n",
    "Ele tamb√©m pode se espalhar atrav√©s de superf√≠cies contaminadas. \n",
    "As pessoas permanecem contagiosas por at√© duas semanas e podem espalhar o v√≠rus mesmo se forem assintom√°ticas.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true,
    "id": "t4_ezTchwKZl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'dezembro de 2019', score: 0.2859, start: 289, end: 305\n",
      "CPU times: user 20.3 s, sys: 3.14 s, total: 23.5 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Quando come√ßou a pandemia de Covid-19 no mundo?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true,
    "id": "a_JgLD8fxdwn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'dezembro de 2019', score: 0.2018, start: 289, end: 305\n",
      "CPU times: user 20 s, sys: 4.73 s, total: 24.7 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Qual √© a data de in√≠cio da pandemia Covid-19 em todo o mundo?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true,
    "id": "NecR00-wzRrn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'origem zoon√≥tica', score: 0.4086, start: 224, end: 240\n",
      "CPU times: user 18.3 s, sys: 4.79 s, total: 23.1 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"A Covid-19 tem algo a ver com animais?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true,
    "id": "RcK4pn1hbLhL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'Wuhan, na China', score: 0.701, start: 309, end: 324\n",
      "CPU times: user 23.6 s, sys: 6.27 s, total: 29.9 s\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Onde foi descoberta a Covid-19?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true,
    "id": "7rFEmmsjzRrn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: '177 349 274', score: 0.6846, start: 535, end: 546\n",
      "CPU times: user 13.4 s, sys: 2.49 s, total: 15.9 s\n",
      "Wall time: 678 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Quantos casos houve?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true,
    "id": "0v1TTQXDzRrn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: '3 840 181', score: 0.8342, start: 605, end: 614\n",
      "CPU times: user 40 s, sys: 9.41 s, total: 49.4 s\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Quantos mortes?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true,
    "id": "f78AggHLzRro"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: '192', score: 0.5195, start: 574, end: 577\n",
      "CPU times: user 48.2 s, sys: 12.7 s, total: 1min\n",
      "Wall time: 3.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Quantos paises tiveram casos?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true,
    "id": "J0AGoVc_xhdo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'nenhum a doen√ßas com risco de morte', score: 0.6356, start: 760, end: 795\n",
      "CPU times: user 40.2 s, sys: 10.4 s, total: 50.5 s\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Quais s√£o sintomas de COVID-19\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true,
    "id": "YSQnntVgcHHq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'pelo ar', score: 0.1721, start: 832, end: 839\n",
      "CPU times: user 43.3 s, sys: 11.4 s, total: 54.7 s\n",
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Como se espalha o v√≠rus?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Language Modeling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adapter-transformers",
   "language": "python",
   "name": "adapter-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
