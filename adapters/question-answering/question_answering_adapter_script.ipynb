{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning BERT (base or large) on a Question-Answering task by using the library adapter-transformers (script version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Credit**: [Hugging Face](https://huggingface.co/) and [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)\n",
    "- **Author**: [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/)\n",
    "- **Date**: 08/02/2021\n",
    "- **Blog post**: [NLP nas empresas | Como ajustar um modelo de linguagem natural como BERT para a tarefa de Question-Answering (QA) com um¬†Adapter?]()\n",
    "- **Link to the folder in github with this notebook and all necessary scripts**: [question-answering with adapters](https://github.com/piegu/language-models/tree/master/adapters/question-answering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective here is to **fine-tune a Masked Language Model (MLM) like BERT (base or large) for a QA task by training adapters (library [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)), not the embeddings and transformers layers of the MLM model**, and to compare results with BERT model fully fine-tune for the same task.\n",
    "\n",
    "The interest is obvious: if you need models for different NLP tasks, instead of fine-tuning and storing one model by NLP task, **you store only one MLM model and the trained tasks adapters which sizes are about 1% (QA adapter) and 8% (Lang+QA adapters) of the MLM model one** (it depends of the choosen adapter configuration). More, the loading of these adapters in production is very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to fine-tune one of the [ü§ó Transformers](https://github.com/huggingface/transformers) model to a question answering task, which is the task of extracting the answer to a question from a given context. We will use the library [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers) and see how to easily load a dataset for these kinds of tasks and use the `Trainer` API to fine-tune a model on it.\n",
    "\n",
    "![Widget inference representing the QA task](images/question_answering_adapter.png)\n",
    "\n",
    "**Note:** This notebook finetunes models that answer question by taking a substring of a context, not by generating new text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook is built to run on any question answering task with the same format as SQUAD (version 1 or 2), with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a token classification head and a fast tokenizer (check on [this table](https://huggingface.co/transformers/index.html#bigtable) if this is the case). It might just need some small adjustments if you decide to use a different dataset than the one used here. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History and Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an adaptation of the following notebooks and scripts for **fine-tuning a (transformer) Masked Language Model (MLM) like BERT (base or large) on the QA task with any QA dataset** (we use here the [Portuguese Squad 1.1 dataset](https://forum.ailab.unb.br/t/datasets-em-portugues/251/4)):\n",
    "- **from [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)** | notebook [04_Cross_Lingual_Transfer.ipynb](https://github.com/Adapter-Hub/adapter-transformers/blob/master/notebooks/04_Cross_Lingual_Transfer.ipynb) and script [run_qa.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/question-answering/run_qa.py) (this script was adapted from the script [run_qa.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py) of HF)\n",
    "- **from [transformers](https://github.com/huggingface/transformers) of Hugging Face** | notebook [question_answering.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb) and script [run_qa.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py) \n",
    "\n",
    "In order to speed up the fine-tuning of the model on only one GPU, the library [DeepSpeed](https://www.deepspeed.ai/) could be used by applying the configuration provided by HF in the notebook [transformers + deepspeed CLI](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb) but as the library adapter-transformers is not synchronized with the last version of the library transformers of HF, we keep that option for the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major changes from original notebooks and scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script [run_qa.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/question-answering/run_qa.py) allows to evaluate the model performance against f1 metric at the end of each epoch, and not against validation loss as done in the HF notebook [question_answering.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb). This is very important as we consider the metric when selecting a model, not the loss. Therefore, we decided to launch this script inside this notebook ([question_answering_adapter_script.ipynb](https://github.com/piegu/language-models/blob/master/adapters/question-answering/question_answering_adapter_script.ipynb)) (by simulating terminal command line) instead of running code in cells as done in the HF notebook. \n",
    "\n",
    "However, to provide an HF-like notebook, we also updated the HF notebook [question_answering.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb) on the notebook [question_answering_adapter.ipynb](https://github.com/piegu/language-models/blob/master/adapters/question-answering/question_answering_adapter.ipynb), but without the f1 metric as the evaluation metric during model training (ie, the f1 metric is used only after the model training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More, we updated the script [run_qa.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/question-answering/run_qa.py) to [run_qa_adapter.py](https://github.com/piegu/language-models/blob/master/adapters/question-answering/run_qa_adapter.py) with the following changes:\n",
    "- **EarlyStopping** by selecting the model with the highest eval f1 (patience of 3 before ending the training)\n",
    "- **MAD-X 2.0** that allows not to train adapters in the last transformer layer (read page 6 of [UNKs Everywhere: Adapting Multilingual Language Models to New Scripts](https://arxiv.org/pdf/2012.15562.pdf))\n",
    "- **Stack method** for the lang and task adapters when a lang adapter is loaded ([doc](https://docs.adapterhub.ml/adapter_composition.html?highlight=stack#stack))\n",
    "- **Houlsby MHA last layer** that allows no to train the task adapter after the Feed Fordward but only after the MHA (Multi-Head Attention) in the last layer for the Houlsby configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "#root path\n",
    "root = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "Pytorch: 1.9.0\n",
      "adapter-transformers: 2.1.1\n",
      "HF transformers: 4.8.2\n",
      "tokenizers: 0.10.3\n",
      "datasets: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys; print('python:',sys.version)\n",
    "\n",
    "import torch; print('Pytorch:',torch.__version__)\n",
    "\n",
    "import transformers; print('adapter-transformers:',transformers.__version__)\n",
    "import transformers; print('HF transformers:',transformers.__hf_version__)\n",
    "import tokenizers; print('tokenizers:',tokenizers.__version__)\n",
    "import datasets; print('datasets:',datasets.__version__)\n",
    "\n",
    "# import deepspeed; print('deepspeed:',deepspeed.__version__)\n",
    "\n",
    "# Versions used in the virtuel environment of this notebook:\n",
    "\n",
    "# python: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
    "# [GCC 7.5.0]\n",
    "# Pytorch: 1.9.0\n",
    "# adapter-transformers: 2.1.1\n",
    "# HF transformers: 4.8.2\n",
    "# tokenizers: 0.10.3\n",
    "# datasets: 1.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create symbolic links to the folder with the scripts to run or download them in the same folder of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ln -s ~/adapter-transformers/examples/question-answering/run_qa_adapter.py\n",
    "# ln -s ~/adapter-transformers/examples/question-answering/trainer_qa.py\n",
    "# ln -s ~/adapter-transformers/examples/question-answering/utils_qa.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a MLM BERT base or large in the dataset language\n",
    "# model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "model_checkpoint = \"neuralmind/bert-large-portuguese-cased\"\n",
    "\n",
    "# We can upload the model from its local archive, too\n",
    "model_checkpoint_original = \"neuralmind/bert-large-portuguese-cased\"\n",
    "model_checkpoint_original_local = root.parent/'neuralmind-bert-large-portuguese-cased-lenerbr/mlm_base'\n",
    "\n",
    "# SQuAD 1.1 in Portuguese\n",
    "dataset_name = \"squad11pt\"\n",
    "\n",
    "# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n",
    "# answers are allowed or not).\n",
    "version_2_with_negative = False # If true, some of the examples do not have an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"qa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments\n",
    "batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_train_epochs = 15.\n",
    "early_stopping_patience = 5\n",
    "\n",
    "adam_epsilon = 1e-7\n",
    "\n",
    "fp16 = True\n",
    "ds = False # If True, we use DeepSpeed\n",
    "\n",
    "# best model\n",
    "load_best_model_at_end = True \n",
    "metric_for_best_model = \"f1\"\n",
    "greater_is_better = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train adapter\n",
    "train_adapter = True # we want to train an adapter\n",
    "load_adapter = None # we do not upload an existing adapter \n",
    "\n",
    "# lang adapter\n",
    "with_adapters_mlm = False # if False, we do not upload an existing lang adapter\n",
    "\n",
    "if with_adapters_mlm:\n",
    "    adapter_composition = \"stack\" # we will stack the lang and task adapters\n",
    "else:\n",
    "    adapter_composition = None\n",
    "\n",
    "# if True, do not put adapter in the last transformer layer (Pfeiffer configuration)\n",
    "madx2 = False\n",
    "\n",
    "# if True, put only an adapter after the MHA but not after the FF in the last layer (Houlsby configuration)\n",
    "houlsby_MHA_lastlayer = True\n",
    "if madx2:\n",
    "    houlsby_MHA_lastlayer = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "n_gpu = 1 # train on just one GPU\n",
    "gpu = 0 # select the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select GPU 0\n",
    "import os\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "if gpu == 0:\n",
    "    os.environ['MASTER_PORT'] = '9996' # modify if RuntimeError: Address already in use # GPU 0\n",
    "elif gpu == 1:\n",
    "    os.environ['MASTER_PORT'] = '9995'\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = str(gpu)\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapters config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task adapter config\n",
    "adapter_config_name = \"pfeiffer\" # houlsby is possible, too\n",
    "if adapter_config_name == \"pfeiffer\":\n",
    "    adapter_non_linearity = 'gelu' # relu is possible, too\n",
    "elif adapter_config_name == \"houlsby\":\n",
    "    adapter_non_linearity = 'swish'\n",
    "adapter_reduction_factor = 16\n",
    "language = 'pt' # pt = Portuguese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lang adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_adapters_mlm:\n",
    "    \n",
    "    # hyperparameters used for fine-tuning the MLM with lang adapter\n",
    "    learning_rate_mlm = 1e-4\n",
    "    batch_size_mlm = 16\n",
    "    gradient_accumulation_steps_mlm = 1\n",
    "    adam_epsilon_mlm = 1e-4\n",
    "    num_train_epoch_mlm = 100.\n",
    "    early_stopping_patience_mlm = 10\n",
    "    madx2_mlm = madx2\n",
    "    houlsby_MHA_lastlayer_mlm = houlsby_MHA_lastlayer\n",
    "    ds_mlm = False\n",
    "    fp16_mlm = True\n",
    "    load_best_model_at_end_mlm = True\n",
    "    metric_for_best_model_mlm = \"loss\"\n",
    "    adapter_config_mlm = adapter_config_name + '+inv'\n",
    "\n",
    "    # path to lang adapter\n",
    "    outputs_mlm = model_checkpoint.replace('/','-') + '_' + dataset_name + '/' + 'mlm' + '/' \\\n",
    "    + 'lr' + str(learning_rate_mlm) \\\n",
    "    + '_bs' + str(batch_size_mlm) \\\n",
    "    + '_GAS' + str(gradient_accumulation_steps_mlm) \\\n",
    "    + '_eps' + str(adam_epsilon_mlm) \\\n",
    "    + '_epochs' + str(num_train_epoch_mlm) \\\n",
    "    + '_patience' + str(early_stopping_patience_mlm) \\\n",
    "    + '_madx2' + str(madx2_mlm) \\\n",
    "    + '_houlsby_MHA_lastlayer' + str(houlsby_MHA_lastlayer_mlm) \\\n",
    "    + '_ds' + str(ds_mlm) \\\n",
    "    + '_fp16' + str(fp16_mlm) \\\n",
    "    + '_best' + str(load_best_model_at_end_mlm) \\\n",
    "    + '_metric' + str(metric_for_best_model_mlm) \\\n",
    "    + '_adapterconfig' + str(adapter_config_mlm)\n",
    "\n",
    "    path_to_outputs_mlm = root/'outputs'/outputs_mlm\n",
    "    \n",
    "    # Config of the lang adapter\n",
    "    lang_adapter_path = path_to_outputs_mlm/'adapters-mlm/'\n",
    "    \n",
    "    load_lang_adapter = str(lang_adapter_path)\n",
    "    lang_adapter_config = str(lang_adapter_path) + \"/adapter_config.json\"\n",
    "    if adapter_config_mlm == \"pfeiffer+inv\":\n",
    "        lang_adapter_non_linearity = 'gelu' # relu is possible, too\n",
    "    elif adapter_config_mlm == \"houlsby+inv\":\n",
    "        lang_adapter_non_linearity = 'swish'\n",
    "    lang_adapter_reduction_factor = 2\n",
    "    language_mlm = language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training arguments of the HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the training argument\n",
    "do_train = True \n",
    "do_eval = True \n",
    "\n",
    "# if you want to test the trainer, set up the following variables\n",
    "max_train_samples = 200 # None\n",
    "max_val_samples = 50 # None\n",
    "\n",
    "# epochs, bs, GA\n",
    "evaluation_strategy = \"epoch\" \n",
    "\n",
    "# fp16\n",
    "fp16_opt_level = 'O1'\n",
    "fp16_backend = \"auto\"\n",
    "fp16_full_eval = False\n",
    "\n",
    "# optimizer (AdamW)\n",
    "weight_decay = 0.01 # 0.0\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "\n",
    "# scheduler\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.0\n",
    "warmup_steps = 0\n",
    "\n",
    "# logs\n",
    "logging_strategy = \"steps\"\n",
    "logging_first_step = True # False\n",
    "logging_steps = 500     # if strategy = \"steps\"\n",
    "eval_steps = logging_steps # logging_steps\n",
    "\n",
    "# checkpoints\n",
    "save_strategy = \"epoch\" # steps\n",
    "save_steps = 500 # if save_strategy = \"steps\"\n",
    "save_total_limit = 1 # None\n",
    "\n",
    "# no cuda, seed\n",
    "no_cuda = False\n",
    "seed = 42\n",
    "\n",
    "# bar\n",
    "disable_tqdm = False # True\n",
    "remove_unused_columns = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder for training outputs\n",
    "\n",
    "outputs = model_checkpoint.replace('/','-') + '_' + dataset_name\n",
    "\n",
    "if with_adapters_mlm:\n",
    "    outputs = outputs + '/' + 'mlm_' + str(task) + '_AdCompo' + str(adapter_composition) + '/'\n",
    "else:\n",
    "    outputs = outputs + '/' + str(task) + '/'\n",
    "\n",
    "outputs = outputs \\\n",
    "+ 'lr' + str(learning_rate) \\\n",
    "+ '_bs' + str(batch_size) \\\n",
    "+ '_GAS' + str(gradient_accumulation_steps) \\\n",
    "+ '_eps' + str(adam_epsilon) \\\n",
    "+ '_epochs' + str(num_train_epochs) \\\n",
    "+ '_patience' + str(early_stopping_patience) \\\n",
    "+ '_wamlm' + str(with_adapters_mlm) \\\n",
    "+ '_madx2' + str(madx2) \\\n",
    "+ '_houlsby_MHA_lastlayer' + str(houlsby_MHA_lastlayer) \\\n",
    "+ '_ds' + str(ds) \\\n",
    "+ '_fp16' + str(fp16) \\\n",
    "+ '_best' + str(load_best_model_at_end) \\\n",
    "+ '_metric' + str(metric_for_best_model) \\\n",
    "+ '_adapterconfig' + str(adapter_config_name)\n",
    "\n",
    "# path to outputs\n",
    "path_to_outputs = root/'outputs'/outputs\n",
    "\n",
    "# subfolder for model outputs\n",
    "output_dir = path_to_outputs/'output_dir' \n",
    "overwrite_output_dir = True # False\n",
    "\n",
    "# logs\n",
    "logging_dir = path_to_outputs/'logging_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum total input sequence length after tokenization. Sequences longer\n",
    "# than this will be truncated, sequences shorter will be padded.\n",
    "max_seq_length = 384\n",
    "\n",
    "# Whether to pad all samples to `max_seq_length`.\n",
    "# If False, will pad the samples dynamically when batching to the maximum length in the batch (which can\n",
    "# be faster on GPU but will be slower on TPU).\n",
    "pad_to_max_length = True\n",
    "    \n",
    "# The threshold used to select the null answer: if the best answer has a score that is less than\n",
    "# the score of the null answer minus this threshold, the null answer is selected for this example.\n",
    "# Only useful when `version_2_with_negative=True`.\n",
    "null_score_diff_threshold = 0.0\n",
    "\n",
    "# When splitting up a long document into chunks, how much stride to take between chunks\n",
    "doc_stride = 128\n",
    "    \n",
    "# The total number of n-best predictions to generate when looking for an answer.\n",
    "n_best_size = 20\n",
    " \n",
    "# The maximum length of an answer that can be generated. This is needed because the start\n",
    "# and end predictions are not conditioned on one another.\n",
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r_n9OWV3l-Q"
   },
   "source": [
    "## 6. Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if dataset_name == \"squad11pt\":\n",
    "    \n",
    "#     # create dataset folder \n",
    "#     path_to_dataset = root/'data'/dataset_name\n",
    "#     path_to_dataset.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "#     # Get dataset SQUAD in Portuguese\n",
    "#     %cd {path_to_dataset}\n",
    "#     !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn\" -O squad-pt.tar.gz && rm -rf /tmp/cookies.txt\n",
    "\n",
    "#     # unzip \n",
    "#     !tar -xvf squad-pt.tar.gz\n",
    "\n",
    "#     # Get the train and validation json file in the HF script format \n",
    "#     # inspiration: file squad.py at https://github.com/huggingface/datasets/tree/master/datasets/squad\n",
    "\n",
    "#     import json \n",
    "#     files = ['squad-train-v1.1.json','squad-dev-v1.1.json']\n",
    "\n",
    "#     for file in files:\n",
    "\n",
    "#         # Opening JSON file & returns JSON object as a dictionary \n",
    "#         f = open(file, encoding=\"utf-8\") \n",
    "#         data = json.load(f) \n",
    "\n",
    "#         # Iterating through the json list \n",
    "#         entry_list = list()\n",
    "#         id_list = list()\n",
    "\n",
    "#         for row in data['data']: \n",
    "#             title = row['title']\n",
    "\n",
    "#             for paragraph in row['paragraphs']:\n",
    "#                 context = paragraph['context']\n",
    "\n",
    "#                 for qa in paragraph['qas']:\n",
    "#                     entry = {}\n",
    "\n",
    "#                     qa_id = qa['id']\n",
    "#                     question = qa['question']\n",
    "#                     answers = qa['answers']\n",
    "\n",
    "#                     entry['id'] = qa_id\n",
    "#                     entry['title'] = title.strip()\n",
    "#                     entry['context'] = context.strip()\n",
    "#                     entry['question'] = question.strip()\n",
    "\n",
    "#                     answer_starts = [answer[\"answer_start\"] for answer in answers]\n",
    "#                     answer_texts = [answer[\"text\"].strip() for answer in answers]\n",
    "#                     entry['answers'] = {}\n",
    "#                     entry['answers']['answer_start'] = answer_starts\n",
    "#                     entry['answers']['text'] = answer_texts\n",
    "\n",
    "#                     entry_list.append(entry)\n",
    "\n",
    "#         reverse_entry_list = entry_list[::-1]\n",
    "\n",
    "#         # for entries with same id, keep only last one (corrected texts by the group Deep Learning Brasil)\n",
    "#         unique_ids_list = list()\n",
    "#         unique_entry_list = list()\n",
    "#         for entry in reverse_entry_list:\n",
    "#             qa_id = entry['id']\n",
    "#             if qa_id not in unique_ids_list:\n",
    "#                 unique_ids_list.append(qa_id)\n",
    "#                 unique_entry_list.append(entry)\n",
    "\n",
    "#         # Closing file \n",
    "#         f.close() \n",
    "\n",
    "#         new_dict = {}\n",
    "#         new_dict['data'] = unique_entry_list\n",
    "\n",
    "#         file_name = 'pt_' + str(file)\n",
    "#         with open(file_name, 'w') as json_file:\n",
    "#             json.dump(new_dict, json_file)\n",
    "            \n",
    "# %cd {root}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1-9jepM3l-W"
   },
   "source": [
    "You can replace the dataset above with any dataset hosted on [the hub](https://huggingface.co/datasets) or use your own files. Just uncomment the following cell and replace the paths with values that will lead to your files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uxSaGa_l3l-W"
   },
   "outputs": [],
   "source": [
    "# datasets = load_dataset(\"text\", data_files={\"train\": path_to_train.txt, \"validation\": path_to_validation.txt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY1SwIrY3l-a"
   },
   "source": [
    "You can also load datasets from a csv or a JSON file, see the [full documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "if dataset_name == \"squad11pt\":\n",
    "    \n",
    "    # dataset folder \n",
    "    path_to_dataset = root/'data'/dataset_name\n",
    "    \n",
    "    # paths to files\n",
    "    train_file = str(path_to_dataset/'pt_squad-train-v1.1.json')\n",
    "    validation_file = str(path_to_dataset/'pt_squad-dev-v1.1.json')\n",
    "    \n",
    "    datasets = load_dataset('json', \n",
    "                            data_files={'train': train_file, \\\n",
    "                                        'validation': validation_file, \\\n",
    "                                       }, \n",
    "                            field='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5735c47ae853931400426b64',\n",
       " 'title': 'Kathmandu',\n",
       " 'context': 'A maioria das cozinhas encontradas em Katmandu n√£o √© vegetariana. No entanto, a pr√°tica do vegetarianismo n√£o √© incomum, e a culin√°ria vegetariana pode ser encontrada em toda a cidade. O consumo de carne bovina √© muito incomum e considerado tabu em muitos lugares. Buff (carne de b√∫falo Marinho) √© muito comum. H√° uma forte tradi√ß√£o de consumo de buffs em Katmandu, especialmente entre Newars, que n√£o √© encontrado em outras partes do Nepal. O consumo de carne de porco era considerado tabu at√© algumas d√©cadas atr√°s. Devido √† mistura com a cozinha Kirat do leste do Nepal, a carne de porco encontrou um lugar nos pratos de Katmandu. Uma popula√ß√£o marginal de hindus e mu√ßulmanos devotos o considera tabu. Os mu√ßulmanos pro√≠bem comer buff a partir do Alcor√£o, enquanto os hindus comem todas as variedades, exceto a carne de vaca, pois consideram a vaca uma deusa e s√≠mbolo da pureza. O caf√© da manh√£ principal para moradores e visitantes √© principalmente Momo ou Chowmein.',\n",
       " 'question': 'De que animal vem o lustre?',\n",
       " 'answers': {'answer_start': [280], 'text': ['b√∫falo Marinho']}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ur5sNUcZ3l-g"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1Uk8NROQ3l-k",
    "outputId": "a822dcec-51e3-4dba-c73c-dba9e0301726"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5731d9f3e17f3d1400422495</td>\n",
       "      <td>Bras%C3%ADlia</td>\n",
       "      <td>Bras√≠lia (pron√∫ncia em portugu√™s: [b…æaÀàzilj…ê]) √© a capital federal do Brasil e sede do governo do Distrito Federal. A cidade est√° localizada no topo do planalto brasileiro na regi√£o centro-oeste do pa√≠s. Foi fundada em 21 de abril de 1960, para servir como a nova capital nacional. Bras√≠lia e seu metr√¥ (abrangendo todo o Distrito Federal) tinham uma popula√ß√£o de 2.556.149 em 2011, sendo a quarta cidade mais populosa do Brasil. Entre as principais cidades latino-americanas, Bras√≠lia possui o maior PIB per capita em R $ 61.915 (US $ 36.175).</td>\n",
       "      <td>Qual √© a capital do Brasil?</td>\n",
       "      <td>{'answer_start': [0], 'text': ['Bras√≠lia']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57278d02f1498d1400e8fbc9</td>\n",
       "      <td>Carnival</td>\n",
       "      <td>Tarragona tem uma das sequ√™ncias rituais mais completas da regi√£o. Os eventos come√ßam com a constru√ß√£o de um enorme barril e terminam com a queima das ef√≠gies do rei e da rainha. No s√°bado, o desfile principal acontece com grupos mascarados, figuras zoom√≥rficas, bandas de m√∫sica e percuss√£o e grupos com fogos de artif√≠cio (os dem√¥nios, o drag√£o, o boi, a f√™mea). Grupos de carnaval se destacam por suas roupas cheias de eleg√¢ncia, mostrando exemplos brilhantes de artesanato em tecido, nos desfiles de s√°bado e domingo. Cerca de 5.000 pessoas s√£o membros dos grupos de desfiles.</td>\n",
       "      <td>De que est√£o cheias as roupas dos grupos de carnaval?</td>\n",
       "      <td>{'answer_start': [422], 'text': ['eleg√¢ncia']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5735190cacc1501500bac403</td>\n",
       "      <td>Hunting</td>\n",
       "      <td>Desde 1934, a venda do Federal Duck Stamps gerou US $ 670 milh√µes e ajudou a comprar ou arrendar 5.200.000 acres (8.100 sq mi; 21.000 km2) de habitat. Os selos servem como uma licen√ßa para ca√ßar aves migrat√≥rias, um passe de entrada para todas as √°reas do Ref√∫gio Nacional da Vida Selvagem, e tamb√©m s√£o considerados itens de colecionador, muitas vezes comprados por raz√µes est√©ticas fora das comunidades de ca√ßa e observa√ß√£o de p√°ssaros. Embora os n√£o ca√ßadores comprem um n√∫mero significativo de selos de patos, oitenta e sete por cento de suas vendas s√£o contribu√≠dos por ca√ßadores, o que √© l√≥gico, pois os ca√ßadores s√£o obrigados a compr√°-los. A distribui√ß√£o de fundos √© gerenciada pela Comiss√£o de Conserva√ß√£o de Aves Migrat√≥rias (MBCC).</td>\n",
       "      <td>O que os selos concedem ao comprador uma licen√ßa para fazer?</td>\n",
       "      <td>{'answer_start': [189], 'text': ['ca√ßar aves migrat√≥rias']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57278f77f1498d1400e8fc20</td>\n",
       "      <td>FA_Cup</td>\n",
       "      <td>A possibilidade de vit√≥rias improv√°veis nas rodadas anteriores da competi√ß√£o, em que equipes de classifica√ß√£o mais baixa vencem oposi√ß√£o mais alta, conhecida como \"matan√ßas gigantes\", √© muito esperada pelo p√∫blico e √© considerada parte integrante da tradi√ß√£o e prest√≠gio da competi√ß√£o. , ao lado do ganho pelas equipes vencedoras da competi√ß√£o. Quase todos os clubes da Pir√¢mide da Liga t√™m um ato de matan√ßa de gigantes lembrado com carinho em sua hist√≥ria. √â considerado particularmente digno de nota quando um time de primeira linha da Premier League sofre uma derrota chateada ou quando o assassino gigante √© um clube que n√£o pertence √† liga, ou seja, de fora dos n√≠veis profissionais da Liga de Futebol Americano.</td>\n",
       "      <td>O que √© um assassino gigante?</td>\n",
       "      <td>{'answer_start': [19], 'text': ['vit√≥rias improv√°veis nas rodadas anteriores da competi√ß√£o, em que equipes de classifica√ß√£o mais baixa vencem oposi√ß√£o mais alta']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>573254560fdd8d15006c69bd</td>\n",
       "      <td>Jehovah%27s_Witnesses</td>\n",
       "      <td>Em agosto de 2015, as Testemunhas de Jeov√° relatam uma m√©dia de 8,2 milh√µes de editores - o termo que eles usam para os membros envolvidos ativamente na prega√ß√£o - em 118.016 congrega√ß√µes. Em 2015, esses relat√≥rios indicaram mais de 1,93 bilh√µes de horas gastas em atividades de prega√ß√£o e \"estudo da B√≠blia\". Desde meados dos anos 90, o n√∫mero de publicadores de pico aumentou de 4,5 milh√µes para 8,2 milh√µes. No mesmo ano, eles conduziram \"estudos b√≠blicos\" com mais de 9,7 milh√µes de indiv√≠duos, incluindo aqueles realizados pelos pais das Testemunhas de Jeov√° com seus filhos. As Testemunhas de Jeov√° estimam que a atual taxa de crescimento mundial √© de 1,5% ao ano.</td>\n",
       "      <td>Que termo as Testemunhas de Jeov√° usam para os membros ativamente envolvidos na prega√ß√£o?</td>\n",
       "      <td>{'answer_start': [79], 'text': ['editores']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>572f72aeb2c2fd1400568138</td>\n",
       "      <td>Database</td>\n",
       "      <td>O trabalho de Codd foi recolhido por duas pessoas em Berkeley, Eugene Wong e Michael Stonebraker. Eles iniciaram um projeto conhecido como INGRES usando financiamento que j√° havia sido alocado para um projeto de banco de dados geogr√°ficos e programadores de estudantes para produzir c√≥digo. A partir de 1973, o INGRES entregou seus primeiros produtos de teste que estavam geralmente prontos para uso generalizado em 1979. O INGRES era semelhante ao Sistema R de v√°rias maneiras, incluindo o uso de uma \"linguagem\" para acesso a dados, conhecida como QUEL. Com o tempo, o INGRES passou para o padr√£o SQL emergente.</td>\n",
       "      <td>Qual era o nome do projeto para criar um banco de dados geogr√°ficos?</td>\n",
       "      <td>{'answer_start': [311], 'text': ['INGRES']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5726f7dbf1498d1400e8f148</td>\n",
       "      <td>Yale_University</td>\n",
       "      <td>Yale produziu ex-alunos destacados em seus respectivos campos. Entre os mais conhecidos est√£o os presidentes dos EUA William Howard Taft, Gerald Ford, George H.W. Bush, Bill Clinton e George W. Bush; membros da realeza Princesa Victoria Victoria Bernadotte, Pr√≠ncipe Rostislav Romanov e Pr√≠ncipe Akiiki Hosea Nyabongo; chefes de estado, incluindo o primeiro-ministro italiano Mario Monti, o primeiro-ministro turco Tansu √áiller, o presidente mexicano Ernesto Zedillo, o presidente alem√£o Karl Carstens e o presidente das Filipinas, Jos√© Paciano Laurel; Ju√≠zes da Suprema Corte dos EUA Sonia Sotomayor, Samuel Alito e Clarence Thomas; Secret√°rios de Estado dos EUA John Kerry, Hillary Clinton, Cyrus Vance e Dean Acheson; os autores Sinclair Lewis, Stephen Vincent Ben√©t e Tom Wolfe; lexic√≥grafo Noah Webster; inventores Samuel F. B. Morse e Eli Whitney; patriota e \"primeiro espi√£o\" Nathan Hale; te√≥logo Jonathan Edwards; atores, diretores e produtores Paul Newman, Henry Winkler, Vincent Price, Meryl Streep, Sigourney Weaver, Jodie Foster, Angela Bassett, Patricia Clarkson, Courtney Vance, Frances McDormand, Elia Kazan, George Roy Hill, Edward Norton, Lupita Nyong'o, Allison Williams, Oliver Stone, Sam Waterston e Michael Cimino; \"Pai do futebol americano\" Walter Camp, James Franco, \"O remador perfeito\" Rusty Wailes; jogadores de beisebol Ron Darling, Bill Hutchinson e Craig Breslow; jogador de basquete Chris Dudley; jogadores de futebol Gary Fencik e Calvin Hill; jogadores de h√≥quei Chris Higgins e Mike Richter; patinadora art√≠stica Sarah Hughes; nadador Don Schollander; esquiador Ryan Max Riley; corredor Frank Shorter; compositores Charles Ives, Douglas Moore e Cole Porter; Sargent Shriver, fundador do Peace Corps; psic√≥logo infantil Benjamin Spock; arquitetos Eero Saarinen e Norman Foster; escultor Richard Serra; cr√≠tico de cinema Gene Siskel; comentaristas de televis√£o Dick Cavett e Anderson Cooper; O jornalista do New York Times David Gonzalez; especialistas William F. Buckley, Jr. e Fareed Zakaria; os economistas Irving Fischer, Mahbub ul Haq e Paul Krugman; inventor do ciclotr√£o e ganhador do Nobel de F√≠sica, Ernest Lawrence; Diretor do Projeto Genoma Humano Francis S. Collins; matem√°tico e qu√≠mico Josiah Willard Gibbs; e empres√°rios, incluindo o co-fundador da Time Magazine Henry Luce, o fundador do Morgan Stanley Harold Stanley, o CEO da Boeing James McNerney, o fundador da FedEx Frederick W. Smith, o presidente da Time Warner Jeffrey Bewkes, o co-fundador da Electronic Arts Bing Gordon e o investidor / filantropo Sir John Templeton; pioneira em aplica√ß√µes el√©tricas Austin Cornelius Dunham.</td>\n",
       "      <td>Qual realeza participou de Yale?</td>\n",
       "      <td>{'answer_start': [219], 'text': ['Princesa Victoria Victoria Bernadotte, Pr√≠ncipe Rostislav Romanov e Pr√≠ncipe Akiiki Hosea Nyabongo']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>572faad8b2c2fd14005682eb</td>\n",
       "      <td>Pain</td>\n",
       "      <td>A presen√ßa de dor em um animal n√£o pode ser conhecida com certeza, mas pode ser inferida por meio de rea√ß√µes f√≠sicas e comportamentais. Atualmente, os especialistas acreditam que todos os vertebrados podem sentir dor e que certos invertebrados, como o polvo, tamb√©m podem sentir. Quanto a outros animais, plantas ou outras entidades, sua capacidade de sentir dor f√≠sica √© atualmente uma quest√£o fora do alcance cient√≠fico, uma vez que nenhum mecanismo √© conhecido pelo qual eles possam ter esse sentimento. Em particular, n√£o h√° nociceptores conhecidos em grupos como plantas, fungos e a maioria dos insetos, exceto, por exemplo, nas moscas da fruta.</td>\n",
       "      <td>O que os fungos e as moscas da fruta parecem n√£o ter?</td>\n",
       "      <td>{'answer_start': [529], 'text': ['nociceptores']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>572b3ab4111d821400f38de5</td>\n",
       "      <td>Empiricism</td>\n",
       "      <td>A maioria dos seguidores de Hume discordou de sua conclus√£o de que a cren√ßa em um mundo externo √© racionalmente injustific√°vel, sustentando que os pr√≥prios princ√≠pios de Hume continham implicitamente a justificativa racional para tal cren√ßa, ou seja, al√©m de se contentar em deixar a quest√£o repousar no instinto humano, costume e h√°bito. De acordo com uma teoria empirista extrema conhecida como fenomenalismo, antecipada pelos argumentos de Hume e George Berkeley, um objeto f√≠sico √© um tipo de constru√ß√£o a partir de nossas experi√™ncias. Fenomenalismo √© a vis√£o de que objetos f√≠sicos, propriedades, eventos (o que quer que seja f√≠sico) s√£o redut√≠veis a objetos mentais, propriedades, eventos. Por fim, apenas objetos mentais, propriedades, eventos existem - da√≠ o termo intimamente relacionado idealismo subjetivo. Pela linha de pensamento fenomenalista, ter uma experi√™ncia visual de uma coisa f√≠sica real √© ter uma experi√™ncia de um certo tipo de grupo de experi√™ncias. Esse tipo de conjunto de experi√™ncias possui uma const√¢ncia e coer√™ncia que faltam no conjunto de experi√™ncias das quais alucina√ß√µes, por exemplo, fazem parte. Como John Stuart Mill colocou em meados do s√©culo XIX, a mat√©ria √© a \"possibilidade permanente de sensa√ß√£o\". O empirismo de Mill deu um passo significativo al√©m de Hume em outro aspecto: ao sustentar que a indu√ß√£o √© necess√°ria para todo conhecimento significativo, incluindo a matem√°tica. Como resumido por D.W. Hamlin:</td>\n",
       "      <td>O que Hume disse que n√£o pode ser racionalmente justificado?</td>\n",
       "      <td>{'answer_start': [69], 'text': ['cren√ßa em um mundo externo']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>572e9d48cb0c0d14000f136f</td>\n",
       "      <td>Vacuum</td>\n",
       "      <td>A descompress√£o r√°pida pode ser muito mais perigosa do que a pr√≥pria exposi√ß√£o ao v√°cuo. Mesmo que a v√≠tima n√£o prenda a respira√ß√£o, a ventila√ß√£o pela traqu√©ia pode ser muito lenta para evitar a ruptura fatal dos delicados alv√©olos dos pulm√µes. Os terremotos e seios nasais podem ser rompidos por descompress√£o r√°pida, os tecidos moles podem machucar e infiltrar sangue, e o estresse do choque acelerar√° o consumo de oxig√™nio, levando √† hip√≥xia. Les√µes causadas por descompress√£o r√°pida s√£o chamadas barotrauma. Uma queda de press√£o de 13 kPa (100 Torr), que n√£o produz sintomas se for gradual, pode ser fatal se ocorrer repentinamente.</td>\n",
       "      <td>O que a acelera√ß√£o do consumo de oxig√™nio faz?</td>\n",
       "      <td>{'answer_start': [437], 'text': ['hip√≥xia']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training + Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup environment variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magic command `%env` corresponds to `export` in linux. It allows to setup the values of all arguments of the script `run_qa_adapter.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = {\n",
    "'n_gpu':n_gpu,\n",
    "'gpu':gpu,\n",
    "'CUDA_VISIBLE_DEVICES':gpu,\n",
    "'model_name_or_path':model_checkpoint,\n",
    "'dataset_name':dataset_name,\n",
    "'train_file':train_file,\n",
    "'validation_file':validation_file,\n",
    "'do_train':do_train,\n",
    "'do_eval':do_eval,\n",
    "'max_train_samples':max_train_samples,\n",
    "'max_val_samples':max_train_samples,\n",
    "'output_dir':output_dir,\n",
    "'overwrite_output_dir':overwrite_output_dir,\n",
    "'max_seq_length':max_seq_length,\n",
    "'pad_to_max_length':pad_to_max_length,\n",
    "'null_score_diff_threshold':null_score_diff_threshold,\n",
    "'doc_stride':doc_stride,\n",
    "'n_best_size':n_best_size,\n",
    "'max_answer_length':max_answer_length,\n",
    "'evaluation_strategy':evaluation_strategy,\n",
    "'per_device_train_batch_size':batch_size,\n",
    "'per_device_eval_batch_size':batch_size,\n",
    "'gradient_accumulation_steps':gradient_accumulation_steps,\n",
    "'learning_rate':learning_rate,\n",
    "'weight_decay':weight_decay,\n",
    "'adam_beta1':adam_beta1,\n",
    "'adam_beta2':adam_beta2,\n",
    "'adam_epsilon':adam_epsilon,\n",
    "'num_train_epochs':num_train_epochs,\n",
    "'warmup_ratio':warmup_ratio,\n",
    "'warmup_steps':warmup_steps,\n",
    "'logging_dir':logging_dir,\n",
    "'logging_strategy':logging_strategy,\n",
    "'logging_first_step':logging_first_step,\n",
    "'logging_steps':logging_steps,\n",
    "'eval_steps':eval_steps,\n",
    "'save_strategy':save_strategy,\n",
    "'save_steps':save_steps,\n",
    "'save_total_limit':save_total_limit,\n",
    "'no_cuda':no_cuda,\n",
    "'seed':seed,\n",
    "'fp16':fp16,\n",
    "'fp16_opt_level':fp16_opt_level,\n",
    "'fp16_backend':fp16_backend,\n",
    "'fp16_full_eval':fp16_full_eval,\n",
    "'disable_tqdm':disable_tqdm,\n",
    "'remove_unused_columns':remove_unused_columns,\n",
    "'load_best_model_at_end':load_best_model_at_end,\n",
    "'metric_for_best_model':metric_for_best_model,\n",
    "'greater_is_better':greater_is_better,\n",
    "'early_stopping_patience':early_stopping_patience,\n",
    "'madx2':madx2,\n",
    "'houlsby_MHA_lastlayer':houlsby_MHA_lastlayer,\n",
    "'train_adapter':train_adapter,\n",
    "'adapter_config_name':adapter_config_name,\n",
    "'adapter_non_linearity':adapter_non_linearity,\n",
    "'adapter_reduction_factor':adapter_reduction_factor,\n",
    "'language':language,\n",
    "'adapter_composition':adapter_composition\n",
    "}\n",
    "\n",
    "if with_adapters_mlm:\n",
    "    envs['load_lang_adapter']=load_lang_adapter\n",
    "    envs['lang_adapter_config']=lang_adapter_config\n",
    "    envs['lang_adapter_non_linearity']=lang_adapter_non_linearity\n",
    "    envs['lang_adapter_reduction_factor']=lang_adapter_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: n_gpu=1\n",
      "env: gpu=0\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: model_name_or_path=neuralmind/bert-base-portuguese-cased\n",
      "env: dataset_name=squad11pt\n",
      "env: train_file=/mnt/home/pierre/course-v4/nbs/MLM/language-modeling/data/squad11pt/pt_squad-train-v1.1.json\n",
      "env: validation_file=/mnt/home/pierre/course-v4/nbs/MLM/language-modeling/data/squad11pt/pt_squad-dev-v1.1.json\n",
      "env: do_train=True\n",
      "env: do_eval=True\n",
      "env: max_train_samples=200\n",
      "env: max_val_samples=200\n",
      "env: output_dir=/mnt/home/pierre/course-v4/nbs/MLM/language-modeling/models_outputs/neuralmind-bert-base-portuguese-cased_squad11pt/qa_lr0.0001_bs16_eps1e-06_epochs15.0_patience5_wamlmFalse_madx2False_houlsby_MHA_lastlayerTrue_dsFalse_fp16True_bestTrue_metricf1_adapterconfighoulsby/output_dir\n",
      "env: overwrite_output_dir=True\n",
      "env: max_seq_length=384\n",
      "env: pad_to_max_length=True\n",
      "env: null_score_diff_threshold=0.0\n",
      "env: doc_stride=128\n",
      "env: n_best_size=20\n",
      "env: max_answer_length=30\n",
      "env: evaluation_strategy=epoch\n",
      "env: per_device_train_batch_size=16\n",
      "env: per_device_eval_batch_size=16\n",
      "env: gradient_accumulation_steps=1\n",
      "env: learning_rate=0.0001\n",
      "env: weight_decay=0.01\n",
      "env: adam_beta1=0.9\n",
      "env: adam_beta2=0.999\n",
      "env: adam_epsilon=1e-06\n",
      "env: num_train_epochs=15.0\n",
      "env: warmup_ratio=0.0\n",
      "env: warmup_steps=0\n",
      "env: logging_dir=/mnt/home/pierre/course-v4/nbs/MLM/language-modeling/models_outputs/neuralmind-bert-base-portuguese-cased_squad11pt/qa_lr0.0001_bs16_eps1e-06_epochs15.0_patience5_wamlmFalse_madx2False_houlsby_MHA_lastlayerTrue_dsFalse_fp16True_bestTrue_metricf1_adapterconfighoulsby/logging_dir\n",
      "env: logging_strategy=steps\n",
      "env: logging_first_step=True\n",
      "env: logging_steps=500\n",
      "env: eval_steps=500\n",
      "env: save_strategy=epoch\n",
      "env: save_steps=500\n",
      "env: save_total_limit=1\n",
      "env: no_cuda=False\n",
      "env: seed=42\n",
      "env: fp16=True\n",
      "env: fp16_opt_level=O1\n",
      "env: fp16_backend=auto\n",
      "env: fp16_full_eval=False\n",
      "env: disable_tqdm=False\n",
      "env: remove_unused_columns=True\n",
      "env: load_best_model_at_end=True\n",
      "env: metric_for_best_model=f1\n",
      "env: greater_is_better=True\n",
      "env: early_stopping_patience=5\n",
      "env: madx2=False\n",
      "env: houlsby_MHA_lastlayer=True\n",
      "env: train_adapter=True\n",
      "env: adapter_config_name=houlsby\n",
      "env: adapter_non_linearity=swish\n",
      "env: adapter_reduction_factor=16\n",
      "env: language=pt\n",
      "env: adapter_composition=None\n"
     ]
    }
   ],
   "source": [
    "for k,v in envs.items():\n",
    "    %env {k}={v}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the output_dir (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can launch the training :-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = os.getenv('PATH')\n",
    "# replace xxxx by your username on your server (ex: paulo)\n",
    "# replace yyyy by the name of the virtual environment of this notebook (ex: adapter-transformers)\n",
    "%env PATH=/mnt/home/xxxx/anaconda3/envs/yyyy/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy/paste/uncomment the 2 following lines in the following cell if you want to limit the number of data (useful for testing)\n",
    "# --max_train_samples $max_train_samples \\\n",
    "# --max_val_samples $max_val_samples \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if with_adapters_mlm:\n",
    "    # with lang adapter\n",
    "    !python -m torch.distributed.launch --nproc_per_node=$n_gpu run_qa_adapter.py \\\n",
    "    --model_name_or_path $model_name_or_path \\\n",
    "    --train_file $train_file \\\n",
    "    --validation_file $validation_file \\\n",
    "    --do_train $do_train \\\n",
    "    --do_eval $do_eval \\\n",
    "    --output_dir $output_dir \\\n",
    "    --overwrite_output_dir $overwrite_output_dir \\\n",
    "    --max_seq_length $max_seq_length \\\n",
    "    --pad_to_max_length $pad_to_max_length \\\n",
    "    --null_score_diff_threshold $null_score_diff_threshold \\\n",
    "    --doc_stride $doc_stride \\\n",
    "    --n_best_size $n_best_size \\\n",
    "    --max_answer_length $max_answer_length \\\n",
    "    --evaluation_strategy $evaluation_strategy \\\n",
    "    --per_device_train_batch_size $per_device_train_batch_size \\\n",
    "    --per_device_eval_batch_size $per_device_eval_batch_size \\\n",
    "    --gradient_accumulation_steps $gradient_accumulation_steps \\\n",
    "    --learning_rate $learning_rate \\\n",
    "    --weight_decay $weight_decay \\\n",
    "    --adam_beta1 $adam_beta1 \\\n",
    "    --adam_beta2 $adam_beta2 \\\n",
    "    --adam_epsilon $adam_epsilon \\\n",
    "    --num_train_epochs $num_train_epochs \\\n",
    "    --warmup_ratio $warmup_ratio \\\n",
    "    --warmup_steps $warmup_steps \\\n",
    "    --logging_dir $logging_dir \\\n",
    "    --logging_strategy $logging_strategy \\\n",
    "    --logging_first_step $logging_first_step \\\n",
    "    --logging_steps $logging_steps \\\n",
    "    --eval_steps $eval_steps \\\n",
    "    --save_strategy $save_strategy \\\n",
    "    --save_steps $save_steps \\\n",
    "    --save_total_limit $save_total_limit \\\n",
    "    --no_cuda $no_cuda \\\n",
    "    --seed $seed \\\n",
    "    --fp16 $fp16 \\\n",
    "    --fp16_opt_level $fp16_opt_level \\\n",
    "    --fp16_backend $fp16_backend \\\n",
    "    --fp16_full_eval $fp16_full_eval \\\n",
    "    --disable_tqdm $disable_tqdm \\\n",
    "    --remove_unused_columns $remove_unused_columns \\\n",
    "    --load_best_model_at_end $load_best_model_at_end \\\n",
    "    --metric_for_best_model $metric_for_best_model \\\n",
    "    --greater_is_better $greater_is_better \\\n",
    "    --early_stopping_patience $early_stopping_patience \\\n",
    "    --madx2 $madx2 \\\n",
    "    --houlsby_MHA_lastlayer $houlsby_MHA_lastlayer \\\n",
    "    --train_adapter $train_adapter \\\n",
    "    --adapter_config $adapter_config_name \\\n",
    "    --adapter_non_linearity $adapter_non_linearity \\\n",
    "    --adapter_reduction_factor $adapter_reduction_factor \\\n",
    "    --language $language \\\n",
    "    --adapter_composition $adapter_composition \\\n",
    "    --load_lang_adapter $load_lang_adapter \\\n",
    "    --lang_adapter_config $lang_adapter_config \\\n",
    "    --lang_adapter_non_linearity $lang_adapter_non_linearity \\\n",
    "    --lang_adapter_reduction_factor $lang_adapter_reduction_factor\n",
    "else:\n",
    "    # without lang adapter\n",
    "    !python -m torch.distributed.launch --nproc_per_node=$n_gpu run_qa_adapter.py \\\n",
    "    --model_name_or_path $model_name_or_path \\\n",
    "    --train_file $train_file \\\n",
    "    --validation_file $validation_file \\\n",
    "    --do_train $do_train \\\n",
    "    --do_eval $do_eval \\\n",
    "    --output_dir $output_dir \\\n",
    "    --overwrite_output_dir $overwrite_output_dir \\\n",
    "    --max_seq_length $max_seq_length \\\n",
    "    --pad_to_max_length $pad_to_max_length \\\n",
    "    --null_score_diff_threshold $null_score_diff_threshold \\\n",
    "    --doc_stride $doc_stride \\\n",
    "    --n_best_size $n_best_size \\\n",
    "    --max_answer_length $max_answer_length \\\n",
    "    --evaluation_strategy $evaluation_strategy \\\n",
    "    --per_device_train_batch_size $per_device_train_batch_size \\\n",
    "    --per_device_eval_batch_size $per_device_eval_batch_size \\\n",
    "    --gradient_accumulation_steps $gradient_accumulation_steps \\\n",
    "    --learning_rate $learning_rate \\\n",
    "    --weight_decay $weight_decay \\\n",
    "    --adam_beta1 $adam_beta1 \\\n",
    "    --adam_beta2 $adam_beta2 \\\n",
    "    --adam_epsilon $adam_epsilon \\\n",
    "    --num_train_epochs $num_train_epochs \\\n",
    "    --warmup_ratio $warmup_ratio \\\n",
    "    --warmup_steps $warmup_steps \\\n",
    "    --logging_dir $logging_dir \\\n",
    "    --logging_strategy $logging_strategy \\\n",
    "    --logging_first_step $logging_first_step \\\n",
    "    --logging_steps $logging_steps \\\n",
    "    --eval_steps $eval_steps \\\n",
    "    --save_strategy $save_strategy \\\n",
    "    --save_steps $save_steps \\\n",
    "    --save_total_limit $save_total_limit \\\n",
    "    --no_cuda $no_cuda \\\n",
    "    --seed $seed \\\n",
    "    --fp16 $fp16 \\\n",
    "    --fp16_opt_level $fp16_opt_level \\\n",
    "    --fp16_backend $fp16_backend \\\n",
    "    --fp16_full_eval $fp16_full_eval \\\n",
    "    --disable_tqdm $disable_tqdm \\\n",
    "    --remove_unused_columns $remove_unused_columns \\\n",
    "    --load_best_model_at_end $load_best_model_at_end \\\n",
    "    --metric_for_best_model $metric_for_best_model \\\n",
    "    --greater_is_better $greater_is_better \\\n",
    "    --early_stopping_patience $early_stopping_patience \\\n",
    "    --madx2 $madx2 \\\n",
    "    --houlsby_MHA_lastlayer $houlsby_MHA_lastlayer \\\n",
    "    --train_adapter $train_adapter \\\n",
    "    --adapter_config $adapter_config_name \\\n",
    "    --adapter_non_linearity $adapter_non_linearity \\\n",
    "    --adapter_reduction_factor $adapter_reduction_factor \\\n",
    "    --language $language \\\n",
    "    --adapter_composition $adapter_composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6756\r\n",
      "drwxrwxr-x 2 pierre pierre    4096 Jul  9 23:52 .\r\n",
      "drwxrwxr-x 4 pierre pierre    4096 Jul  9 23:56 ..\r\n",
      "-rw-rw-r-- 1 pierre pierre     629 Jul  9 23:52 adapter_config.json\r\n",
      "-rw-rw-r-- 1 pierre pierre     240 Jul  9 23:52 head_config.json\r\n",
      "-rw-rw-r-- 1 pierre pierre 6889489 Jul  9 23:52 pytorch_adapter.bin\r\n",
      "-rw-rw-r-- 1 pierre pierre    7143 Jul  9 23:52 pytorch_model_head.bin\r\n"
     ]
    }
   ],
   "source": [
    "# folder of the saved task adapter\n",
    "!ls -al {output_dir/task}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir/task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can push the saved adapter + head to the [AdapterHub](https://adapterhub.ml/) (follow instructions at [Contributing to Adapter Hub](https://docs.adapterhub.ml/contributing.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = os.getenv('PATH')\n",
    "# replace xxxx by your username on your server (ex: paulo)\n",
    "# replace yyyy by the name of the virtual environment of this notebook (ex: adapter-transformers)\n",
    "%env PATH=/mnt/home/xxxx/anaconda3/envs/yyyy/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "%tensorboard --logdir {logging_dir} --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Application QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true,
    "id": "Ckl2Fzn3is0F"
   },
   "outputs": [],
   "source": [
    "### import transformers\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true,
    "id": "IJA9CgOBis0F",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "if model_checkpoint_original == \"neuralmind/bert-large-portuguese-cased\":\n",
    "    tokenizer_qa = AutoTokenizer.from_pretrained(model_checkpoint_original_local)\n",
    "    model_qa = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint_original_local)\n",
    "else:\n",
    "    tokenizer_qa = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    model_qa = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lang adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_adapters_mlm:\n",
    "    \n",
    "    # hyperparameters used for fine-tuning the MLM with lang adapter\n",
    "    learning_rate_mlm = 1e-4\n",
    "    batch_size_mlm = 16\n",
    "    gradient_accumulation_steps_mlm = 1\n",
    "    adam_epsilon_mlm = 1e-4\n",
    "    num_train_epoch_mlm = 100.\n",
    "    early_stopping_patience_mlm = 10\n",
    "    madx2_mlm = madx2\n",
    "    houlsby_MHA_lastlayer_mlm = houlsby_MHA_lastlayer\n",
    "    ds_mlm = False\n",
    "    fp16_mlm = True\n",
    "    load_best_model_at_end_mlm = True\n",
    "    metric_for_best_model_mlm = \"loss\"\n",
    "    adapter_config_mlm = adapter_config_name + '+inv'\n",
    "\n",
    "    # path to lang adapter\n",
    "    outputs_mlm = model_checkpoint.replace('/','-') + '_' + dataset_name + '/' + 'mlm' + '/' \\\n",
    "    + 'lr' + str(learning_rate_mlm) \\\n",
    "    + '_bs' + str(batch_size_mlm) \\\n",
    "    + '_GAS' + str(gradient_accumulation_steps_mlm) \\\n",
    "    + '_eps' + str(adam_epsilon_mlm) \\\n",
    "    + '_epochs' + str(num_train_epoch_mlm) \\\n",
    "    + '_patience' + str(early_stopping_patience_mlm) \\\n",
    "    + '_madx2' + str(madx2_mlm) \\\n",
    "    + '_houlsby_MHA_lastlayer' + str(houlsby_MHA_lastlayer_mlm) \\\n",
    "    + '_ds' + str(ds_mlm) \\\n",
    "    + '_fp16' + str(fp16_mlm) \\\n",
    "    + '_best' + str(load_best_model_at_end_mlm) \\\n",
    "    + '_metric' + str(metric_for_best_model_mlm) \\\n",
    "    + '_adapterconfig' + str(adapter_config_mlm)\n",
    "\n",
    "    path_to_outputs_mlm = root/'outputs'/outputs_mlm\n",
    "    \n",
    "    # Config of the lang adapter\n",
    "    lang_adapter_path = path_to_outputs_mlm/'adapters-mlm/'\n",
    "    \n",
    "    load_lang_adapter = str(lang_adapter_path)\n",
    "    lang_adapter_config = str(lang_adapter_path) + \"/adapter_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_adapters_mlm:\n",
    "    # load the language adapter without head\n",
    "    task_mlm_load_as = 'mlm'\n",
    "    lang_adapter_name = model_qa.load_adapter(\n",
    "        load_lang_adapter,\n",
    "        config=lang_adapter_config,\n",
    "        load_as=task_mlm_load_as,\n",
    "        with_head=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder for training outputs\n",
    "\n",
    "outputs = model_checkpoint.replace('/','-') + '_' + dataset_name\n",
    "\n",
    "if with_adapters_mlm:\n",
    "    outputs = outputs + '/' + 'mlm_' + str(task) + '_AdCompo' + str(adapter_composition) + '/'\n",
    "else:\n",
    "    outputs = outputs + '/' + str(task) + '/'\n",
    "\n",
    "outputs = outputs \\\n",
    "+ 'lr' + str(learning_rate) \\\n",
    "+ '_bs' + str(batch_size) \\\n",
    "+ '_GAS' + str(gradient_accumulation_steps) \\\n",
    "+ '_eps' + str(adam_epsilon) \\\n",
    "+ '_epochs' + str(num_train_epochs) \\\n",
    "+ '_patience' + str(early_stopping_patience) \\\n",
    "+ '_wamlm' + str(with_adapters_mlm) \\\n",
    "+ '_madx2' + str(madx2) \\\n",
    "+ '_houlsby_MHA_lastlayer' + str(houlsby_MHA_lastlayer) \\\n",
    "+ '_ds' + str(ds) \\\n",
    "+ '_fp16' + str(fp16) \\\n",
    "+ '_best' + str(load_best_model_at_end) \\\n",
    "+ '_metric' + str(metric_for_best_model) \\\n",
    "+ '_adapterconfig' + str(adapter_config_name)\n",
    "\n",
    "# path to outputs\n",
    "path_to_outputs = root/'outputs'/outputs\n",
    "\n",
    "# path to adapter\n",
    "adapters_folder = 'adapters-' + task\n",
    "path_to_save_adapter = path_to_outputs/adapters_folder\n",
    "\n",
    "# Config of the task adapter\n",
    "load_adapter = str(path_to_save_adapter)\n",
    "adapter_config = str(path_to_save_adapter) + \"/adapter_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the task adapter with head\n",
    "task_name = task\n",
    "model_qa.load_adapter(\n",
    "    load_adapter,\n",
    "    config=adapter_config,\n",
    "    load_as=task_name,\n",
    "    with_head = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the adapters to be used in every forward pass\n",
    "from transformers.adapters.composition import Stack\n",
    "if lang_adapter_name:\n",
    "    model_qa.active_adapters = Stack(task_mlm_load_as, task_name)\n",
    "else:\n",
    "    model_qa.set_active_adapters(task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"question-answering\", model=model_qa, tokenizer=tokenizer_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://pt.wikipedia.org/wiki/Pandemia_de_COVID-19\n",
    "context = r\"\"\"A pandemia de COVID-19, tamb√©m conhecida como pandemia de coronav√≠rus, √© uma pandemia em curso de COVID-19, \n",
    "uma doen√ßa respirat√≥ria causada pelo coronav√≠rus da s√≠ndrome respirat√≥ria aguda grave 2 (SARS-CoV-2). \n",
    "O v√≠rus tem origem zoon√≥tica e o primeiro caso conhecido da doen√ßa remonta a dezembro de 2019 em Wuhan, na China. \n",
    "Em 20 de janeiro de 2020, a Organiza√ß√£o Mundial da Sa√∫de (OMS) classificou o surto \n",
    "como Emerg√™ncia de Sa√∫de P√∫blica de √Çmbito Internacional e, em 11 de mar√ßo de 2020, como pandemia. \n",
    "Em 18 de junho de 2021, 177 349 274 casos foram confirmados em 192 pa√≠ses e territ√≥rios, \n",
    "com 3 840 181 mortes atribu√≠das √† doen√ßa, tornando-se uma das pandemias mais mortais da hist√≥ria.\n",
    "Os sintomas de COVID-19 s√£o altamente vari√°veis, variando de nenhum a doen√ßas com risco de morte. \n",
    "O v√≠rus se espalha principalmente pelo ar quando as pessoas est√£o perto umas das outras. \n",
    "Ele deixa uma pessoa infectada quando ela respira, tosse, espirra ou fala e entra em outra pessoa pela boca, nariz ou olhos.\n",
    "Ele tamb√©m pode se espalhar atrav√©s de superf√≠cies contaminadas. \n",
    "As pessoas permanecem contagiosas por at√© duas semanas e podem espalhar o v√≠rus mesmo se forem assintom√°ticas.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "t4_ezTchwKZl"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"Quando come√ßou a pandemia de Covid-19 no mundo?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "a_JgLD8fxdwn"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"Qual √© a data de in√≠cio da pandemia Covid-19 em todo o mundo?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "NecR00-wzRrn"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"A Covid-19 tem algo a ver com animais?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "RcK4pn1hbLhL"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"Onde foi descoberta a Covid-19?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "7rFEmmsjzRrn"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"Quantos casos houve?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "0v1TTQXDzRrn"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"Quantos mortes?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "f78AggHLzRro"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"Quantos paises tiveram casos?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "J0AGoVc_xhdo"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"Quais s√£o sintomas de COVID-19\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "YSQnntVgcHHq"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "question = \"Como se espalha o v√≠rus?\"\n",
    "\n",
    "result = nlp(question=question, context=context)\n",
    "\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Language Modeling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adapter-transformers",
   "language": "python",
   "name": "adapter-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
