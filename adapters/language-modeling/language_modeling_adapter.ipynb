{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a MLM (Masked Language Model) like BERT (base or large) with the library adapter-transformers (notebook version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Credit**: [Hugging Face](https://huggingface.co/) and [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)\n",
    "- **Author**: [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/)\n",
    "- **Date**: 05/07/2021\n",
    "- **Blog post**: [NLP nas empresas | Como ajustar um modelo de linguagem natural como BERT a um novo dom√≠nio lingu√≠stico com um Adapter?](https://medium.com/@pierre_guillou/nlp-nas-empresas-como-ajustar-um-modelo-de-linguagem-natural-como-bert-a-um-novo-dom%C3%ADnio-23752b73b185)\n",
    "- **Link to the folder in github with this notebook and all necessary scripts**: [language-modeling with adapters](https://github.com/piegu/language-models/tree/master/adapters/language-modeling/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3KD3WXU3l-O"
   },
   "source": [
    "The objective here is to **fine-tune a Masked Language Model (MLM) like BERT (base or large) by training adapters (library [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)), not the embeddings and transformers layers of the MLM model**, and to compare results with BERT model fully fine-tune for the same task.\n",
    "\n",
    "The interest is obvious: if you need models for different NLP tasks, instead of fine-tuning and storing one model by NLP task, **you store only one MLM model and the trained tasks adapters which sizes are between 6% and 13% of the MLM model one** (it depends of the choosen adapter configuration). More, the loading of these adapters in production is very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAscNNUD3l-P"
   },
   "source": [
    "In this notebook, we'll see how to fine-tune one of the [ü§ó Transformers](https://github.com/huggingface/transformers) model on a language modeling tasks. We will cover one type of language modeling tasks which is:\n",
    "\n",
    "- Masked language modeling: the model has to predict some tokens that are masked in the input. It still has access to the whole sentence, so it can use the tokens before and after the tokens masked to predict their value.\n",
    "\n",
    "![Widget inference representing the masked language modeling task](images/masked_language_modeling_adapter.png)\n",
    "\n",
    "We will see how to easily load and preprocess the dataset for each one of those tasks, and how to use the `Trainer` API to fine-tune a model on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History and Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an adaptation of the following notebooks and scripts for **fine-tuning a (transformer) Masked Language Model (MLM) like BERT (base or large) with any dataset** (we use here the texts of the [Portuguese Squad 1.1 dataset](https://forum.ailab.unb.br/t/datasets-em-portugues/251/4)):\n",
    "- **from [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)** | notebook [01_Adapter_Training.ipynb](https://github.com/Adapter-Hub/adapter-transformers/blob/master/notebooks/01_Adapter_Training.ipynb) and script [run_mlm.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/language-modeling/run_mlm.py) (this script was adapted from the script [run_mlm.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py) of HF)\n",
    "- **from [transformers](https://github.com/huggingface/transformers) of Hugging Face** | notebook [language_modeling.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb) and script [run_mlm.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py) \n",
    "\n",
    "In order to speed up the fine-tuning of the model on only one GPU, the library [DeepSpeed](https://www.deepspeed.ai/) could be used by applying the configuration provided by HF in the notebook [transformers + deepspeed CLI](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb) but as the library adapter-transformers is not synchronized with the last version of the library transformers of HF, we keep that option for the future.\n",
    "\n",
    "*Note: the paragraph about Causal language modeling (CLM) is not included in this notebook, and all the non necessary code about Masked Model Language (MLM) has been deleted from the original notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major changes from original notebooks and scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook [language_modeling.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb) and script [run_mlm.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/language-modeling/run_mlm.py) allow to evaluate the model performance against the validation loss at the end of each epoch, not against the metric accuracy. \n",
    "\n",
    "As a metric is better in order to select a model than the loss, we introduced in this notebook the metric accuracy for model evaluation (see the method `comput_metrics()`). However, as it needs many GB for the evaluation calculation, we do not use it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we updated the notebook [language_modeling.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb)  to [language_modeling_adapter.ipynb](https://github.com/piegu/language-models/blob/master/adapters/language_modeling/language_modeling_adapter.ipynb) with the following changes:\n",
    "- **Accuracy**: model evaluation through eval accuracy\n",
    "- **EarlyStopping** by selecting the model with the highest eval accuracy (patience of 3 before ending the training)\n",
    "- **MAD-X 2.0** that allows not to train adapters in the last transformer layer (read page 6 of [UNKs Everywhere: Adapting Multilingual Language Models to New Scripts](https://arxiv.org/pdf/2012.15562.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "#root path\n",
    "root = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "Pytorch: 1.9.0\n",
      "adapter-transformers: 2.0.1\n",
      "HF transformers: 4.5.1\n",
      "tokenizers: 0.10.3\n",
      "datasets: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import sys; print('python:',sys.version)\n",
    "\n",
    "import torch; print('Pytorch:',torch.__version__)\n",
    "\n",
    "import transformers; print('adapter-transformers:',transformers.__version__)\n",
    "import transformers; print('HF transformers:',transformers.__hf_version__)\n",
    "import tokenizers; print('tokenizers:',tokenizers.__version__)\n",
    "import datasets; print('datasets:',datasets.__version__)\n",
    "\n",
    "# import deepspeed; print('deepspeed:',deepspeed.__version__)\n",
    "\n",
    "# Versions used in the virtuel environment of this notebook:\n",
    "\n",
    "# python: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
    "# [GCC 7.5.0]\n",
    "# Pytorch: 1.9.0\n",
    "# adapter-transformers: 2.0.1\n",
    "# transformers: 4.5.1\n",
    "# tokenizers: 0.10.3\n",
    "# datasets: 1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a MLM BERT base or large in the dataset language\n",
    "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "# model_checkpoint = \"neuralmind/bert-large-portuguese-cased\"\n",
    "\n",
    "# SQuAD 1.1 in Portuguese\n",
    "dataset_name = \"squad11pt\" # SQuAD v1.1 em portugu√™s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mlm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments\n",
    "batch_size = 32\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_train_epochs = 100.\n",
    "early_stopping_patience = 10\n",
    "\n",
    "adam_epsilon = 1e-6\n",
    "\n",
    "fp16 = True\n",
    "ds = False # DeepSpeed\n",
    "\n",
    "# best model\n",
    "load_best_model_at_end = True \n",
    "if load_best_model_at_end:\n",
    "    metric_for_best_model = \"loss\" # could be accuracy, too\n",
    "    if metric_for_best_model == \"loss\":\n",
    "        greater_is_better = False\n",
    "    else:\n",
    "        greater_is_better = True # for accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train adapter\n",
    "train_adapter = True # we want to train an adapter\n",
    "load_adapter = None # we do not upload an existing adapter \n",
    "load_lang_adapter = None # we do not upload an existing lang adapter\n",
    "\n",
    "# if True, do not put adapter in the last transformer layer\n",
    "madx2 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "n_gpu = 1 # train on just one GPU\n",
    "gpu = 1 # select the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this notebook in GPU 0\n",
    "# As we do not launch a python script in this notebook, this cell is not mandatory\n",
    "import os\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "if gpu == 0:\n",
    "    os.environ['MASTER_PORT'] = '9996' # modify if RuntimeError: Address already in use # GPU 0\n",
    "elif gpu == 1:\n",
    "    os.environ['MASTER_PORT'] = '9997'\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = str(gpu)\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lang adapter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang adapter config\n",
    "adapter_config_name = \"houlsby+inv\" # houlsby+inv is possible, too\n",
    "if adapter_config_name == \"pfeiffer+inv\":\n",
    "    adapter_non_linearity = 'gelu' # relu is possible, too\n",
    "elif adapter_config_name == \"houlsby+inv\":\n",
    "    adapter_non_linearity = 'swish'\n",
    "adapter_reduction_factor = 2\n",
    "language = 'pt '# pt = Portuguese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training arguments of the HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the training argument\n",
    "do_train = True \n",
    "do_eval = True \n",
    "\n",
    "# epochs, bs, GA\n",
    "evaluation_strategy = \"epoch\" # no\n",
    "\n",
    "# fp16\n",
    "fp16_opt_level = 'O1'\n",
    "fp16_backend = \"auto\"\n",
    "fp16_full_eval = False\n",
    "\n",
    "# optimizer (AdamW)\n",
    "weight_decay = 0.01 # 0.0\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "\n",
    "# scheduler\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.0\n",
    "warmup_steps = 0\n",
    "\n",
    "# logs\n",
    "logging_strategy = \"steps\"\n",
    "logging_first_step = True # False\n",
    "logging_steps = 500     # if strategy = \"steps\"\n",
    "eval_steps = logging_steps # logging_steps\n",
    "\n",
    "# checkpoints\n",
    "save_strategy = \"epoch\" # steps\n",
    "save_steps = 500 # if save_strategy = \"steps\"\n",
    "save_total_limit = 1 # None\n",
    "\n",
    "# no cuda, seed\n",
    "no_cuda = False\n",
    "seed = 42\n",
    "\n",
    "# bar\n",
    "disable_tqdm = False # True\n",
    "remove_unused_columns = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder for training outputs\n",
    "\n",
    "outputs = model_checkpoint.replace('/','-') + '_' + dataset_name + '/'  \n",
    "outputs = outputs + str(task) \\\n",
    "+ '_lr' + str(learning_rate) \\\n",
    "+ '_bs' + str(batch_size) \\\n",
    "+ '_GAS' + str(gradient_accumulation_steps) \\\n",
    "+ '_eps' + str(adam_epsilon) \\\n",
    "+ '_epochs' + str(num_train_epochs) \\\n",
    "+ '_patience' + str(early_stopping_patience) \\\n",
    "+ '_madx2' + str(madx2) \\\n",
    "+ '_ds' + str(ds) \\\n",
    "+ '_fp16' + str(fp16) \\\n",
    "+ '_best' + str(load_best_model_at_end) \\\n",
    "+ '_metric' + str(metric_for_best_model) \\\n",
    "+ '_adapterconfig' + str(adapter_config_name)\n",
    "\n",
    "# path to outputs\n",
    "path_to_outputs = root/'models_outputs'/outputs\n",
    "\n",
    "# subfolder for model outputs\n",
    "output_dir = path_to_outputs/'output_dir' \n",
    "overwrite_output_dir = True # False\n",
    "\n",
    "# logs\n",
    "logging_dir = path_to_outputs/'logging_dir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r_n9OWV3l-Q"
   },
   "source": [
    "## 6. Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dataset_name == \"squad11pt\":\n",
    "    \n",
    "#     # create dataset folder \n",
    "#     path_to_dataset = root/'data'/dataset_name\n",
    "#     path_to_dataset.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "#     # Get dataset SQUAD in Portuguese\n",
    "#     %cd {path_to_dataset}\n",
    "#     !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn\" -O squad-pt.tar.gz && rm -rf /tmp/cookies.txt\n",
    "\n",
    "#     # unzip \n",
    "#     !tar -xvf squad-pt.tar.gz\n",
    "\n",
    "#     # Get the train and validation json file in the HF script format \n",
    "#     # inspiration: file squad.py at https://github.com/huggingface/datasets/tree/master/datasets/squad\n",
    "    \n",
    "#     import json \n",
    "#     files = ['squad-train-v1.1.json','squad-dev-v1.1.json']\n",
    "\n",
    "#     for file in files:\n",
    "\n",
    "#         # Opening JSON file & returns JSON object as a dictionary \n",
    "#         f = open(file, encoding=\"utf-8\") \n",
    "#         data = json.load(f) \n",
    "\n",
    "#         # Iterating through the json list \n",
    "#         context_list = list()\n",
    "#         id_list = list()\n",
    "\n",
    "#         for row in data['data']: \n",
    "\n",
    "#             for paragraph in row['paragraphs']:\n",
    "#                 context = (paragraph['context']).strip()\n",
    "#                 context_list.append(context)\n",
    "\n",
    "#         # Get unique context\n",
    "#         unique_context_list = list(set(context_list))\n",
    "\n",
    "#         # Closing file \n",
    "#         f.close() \n",
    "\n",
    "#         file_name = 'pt_' + str(file).replace('json','txt')\n",
    "#         with open(file_name, 'wb') as list_file:\n",
    "#             pickle.dump(unique_context_list, list_file)\n",
    "         \n",
    "#     %cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1-9jepM3l-W"
   },
   "source": [
    "You can replace the dataset above with any dataset hosted on [the hub](https://huggingface.co/datasets) or use your own files. Just uncomment the following cell and replace the paths with values that will lead to your files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uxSaGa_l3l-W"
   },
   "outputs": [],
   "source": [
    "# datasets = load_dataset(\"text\", data_files={\"train\": path_to_train.txt, \"validation\": path_to_validation.txt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY1SwIrY3l-a"
   },
   "source": [
    "You can also load datasets from a csv or a JSON file, see the [full documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"squad11pt\":\n",
    "    \n",
    "    path_to_data = root/'data'/dataset_name\n",
    "    files = ['pt_squad-train-v1.1.txt','pt_squad-dev-v1.1.txt']\n",
    "    \n",
    "    for i,file in enumerate(files):\n",
    "        path_to_file = path_to_data/file\n",
    "        with open(path_to_file, \"rb\") as f:   # Unpickling\n",
    "            text_list = pickle.load(f)\n",
    "\n",
    "            with open(file, \"w\") as output:\n",
    "                output.write(str(text_list))\n",
    "        \n",
    "        df = pd.DataFrame(text_list,columns=['text'])\n",
    "        if i == 0:\n",
    "            df_train = df.copy()\n",
    "        else:\n",
    "            df_validation = df.copy()\n",
    "            \n",
    "    from datasets import Dataset, DatasetDict\n",
    "    dataset_train = Dataset.from_pandas(df_train)\n",
    "    dataset_validation = Dataset.from_pandas(df_validation)\n",
    "\n",
    "    datasets = DatasetDict()\n",
    "    datasets['train'] = dataset_train\n",
    "    datasets['validation'] = dataset_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'O pante√≠smo sustenta que Deus √© o universo e o universo √© Deus, enquanto o panente√≠smo sustenta que Deus cont√©m, mas n√£o √© id√™ntico ao universo. √â tamb√©m a vis√£o da Igreja Cat√≥lica Liberal; Teosofia; algumas vis√µes do hindu√≠smo, exceto o vaisnavismo, que acredita no panente√≠smo; Sikhismo; algumas divis√µes do neopaganismo e tao√≠smo, juntamente com muitas denomina√ß√µes e indiv√≠duos variados dentro das denomina√ß√µes. A Cabala, Misticismo judaico, pinta uma vis√£o pante√≠sta / panente√≠sta de Deus - que tem ampla aceita√ß√£o no juda√≠smo hass√≠dico, particularmente de seu fundador The Baal Shem Tov - mas apenas como um complemento √† vis√£o judaica de um deus pessoal, n√£o no pante√≠sta original sensa√ß√£o que nega ou limita a persona a Deus. [cita√ß√£o necess√°rio]'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ur5sNUcZ3l-g"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1Uk8NROQ3l-k",
    "outputId": "a822dcec-51e3-4dba-c73c-dba9e0301726"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O campus principal em Provo, Utah, Estados Unidos, fica em aproximadamente 560 acres (2,3 km2) situado na base das montanhas Wasatch e inclui 295 edif√≠cios. Os edif√≠cios apresentam uma grande variedade de estilos arquitet√¥nicos, cada edif√≠cio sendo constru√≠do no estilo de seu tempo. A grama, as √°rvores e os canteiros de flores do campus da BYU s√£o mantidos impecavelmente. Al√©m disso, vistas das montanhas Wasatch (incluindo o Monte Timpanogos) podem ser vistas no campus. A Biblioteca Harold B. Lee da BYU (tamb√©m conhecida como \"HBLL\"), que a Princeton Review classificou como a \"1¬™ Grande Biblioteca da Faculdade\" em 2004, possui aproximadamente 8,5 milh√µes de itens em suas cole√ß√µes, cont√©m 158 km de prateleiras e pode acomodar 4.600 pessoas. A Torre de Spencer W. Kimball, abreviada para SWKT e pronunciada Swicket por muitos estudantes, abriga v√°rios departamentos e programas da universidade e √© o edif√≠cio mais alto de Provo, Utah. Al√©m disso, o Marriott Center da BYU, usado como uma arena de basquete, pode acomodar mais de 22.000 pessoas e √© uma das maiores arenas no campus do pa√≠s. Curiosamente ausente no campus desta universidade de propriedade da igreja √© uma capela do campus. N√£o obstante, todos os cultos dominicais da Igreja SUD para os alunos s√£o realizados no campus, mas devido ao grande n√∫mero de estudantes que frequentam esses servi√ßos, quase todos os pr√©dios e poss√≠veis espa√ßos de reuni√£o no campus s√£o utilizados (al√©m disso, muitos estudantes frequentam servi√ßos fora do campus no campus). Capelas SUD nas comunidades do entorno).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nos contextos liter√°rios, Apolo representa harmonia, ordem e raz√£o - caracter√≠sticas contrastadas com as de Dion√≠sio, deus do vinho, que representa √™xtase e desordem. O contraste entre os pap√©is desses deuses se reflete nos adjetivos apol√≠neo e dionis√≠aco. No entanto, os gregos pensavam nas duas qualidades como complementares: os dois deuses s√£o irm√£os, e quando Apolo no inverno partisse para Hyperborea, deixaria o or√°culo de Delfos para Dion√≠sio. Este contraste parece ser mostrado nos dois lados do vaso Borghese.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A liga realizou sua primeira temporada em 1992-1993 e era originalmente composta por 22 clubes. O primeiro gol de todos os tempos na Premier League foi marcado por Brian Deane, do Sheffield United, em uma vit√≥ria por 2 a 1 sobre o Manchester United. Os 22 membros inaugurais da nova Premier League foram Arsenal, Aston Villa, Blackburn Rovers, Chelsea, Coventry City, Crystal Palace, Everton, Ipswich Town, Leeds United, Liverpool, Manchester City, Manchester United, Middlesbrough, Norwich City, Nottingham Forest, Oldham Athletic, Queens Park Rangers, Sheffield United, Sheffield Wednesday, Southampton, Tottenham Hotspur e Wimbledon. Luton Town, Notts County e West Ham United foram as tr√™s equipes rebaixadas da antiga primeira divis√£o no final da temporada 1991-1992 e n√£o participaram da temporada inaugural da Premier League.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N√£o foi at√© o final do s√©culo XVIII que a migra√ß√£o como explica√ß√£o para o desaparecimento no inverno de aves dos climas do norte foi aceita. A History of British Birds (Volume 1, 1797), de Thomas Bewick, menciona um relat√≥rio de \"um mestre muito inteligente de um navio\" que, \"entre as ilhas de Minorca e Maiorca, viu um grande n√∫mero de andorinhas voando para o norte\" e afirma a situa√ß√£o na Gr√£-Bretanha da seguinte forma:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Na d√©cada de 1960, o centro de Houston consistia em uma cole√ß√£o de estruturas de escrit√≥rios. O centro da cidade estava no limiar de um boom liderado pelo setor de energia em 1970. Uma sucess√£o de arranha-c√©us foi constru√≠da ao longo da d√©cada de 1970 - muitos pelo promotor imobili√°rio Gerald D. Hines - culminando com o arranha-c√©u mais alto de Houston, com 75 andares e 1.002 p√©s ( Torre JPMorgan Chase de 305 m) (anteriormente denominada Texas Commerce Tower), conclu√≠da em 1982. √â a estrutura mais alta do Texas, 15¬∫ edif√≠cio mais alto dos Estados Unidos e o 85¬∫ arranha-c√©u mais alto do mundo, com base na maior caracter√≠stica arquitet√¥nica . Em 1983, o Wells Fargo Plaza, com 71 andares e 992 p√©s (302 m) de altura, foi conclu√≠do, tornando-se o segundo edif√≠cio mais alto de Houston e Texas. Baseado na caracter√≠stica arquitet√¥nica mais alta, √© o 17¬∫ mais alto dos Estados Unidos e o 95¬∫ mais alto do mundo. Em 2007, o centro de Houston tinha mais de 4.000.000 m¬≤ de espa√ßo para escrit√≥rio.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cardigan montou sua unidade e atacou o comprimento do vale do Balaclava, sob fogo de baterias russas nas colinas. A acusa√ß√£o da Brigada Leve causou 278 baixas na unidade de 700 homens. A Brigada Leve foi comemorada no famoso poema de Alfred Lord Tennyson, \"A Carga da Brigada Leve\". Embora tradicionalmente a acusa√ß√£o da Brigada Leve fosse vista como um sacrif√≠cio glorioso, por√©m desperdi√ßado, de homens e cavalos bons, historiadores recentes dizem que a acusa√ß√£o da Brigada Leve teve √™xito em pelo menos alguns de seus objetivos. O objetivo de qualquer carga de cavalaria √© espalhar as linhas inimigas e assustar o inimigo fora do campo de batalha. A carga da Brigada Ligeira havia t√£o enervado a cavalaria russa, que havia sido derrotada anteriormente pela Brigada Pesada, que a Cavalaria Russa foi posta em v√¥o em grande escala pela carga subsequente da Brigada Ligeira.:252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Os intestinos dos animais cont√™m uma grande popula√ß√£o de flora intestinal. Nos seres humanos, os quatro filos dominantes s√£o Firmicutes, Bacteroidetes, Actinobacteria e Proteobacteria. Eles s√£o essenciais para a digest√£o e tamb√©m s√£o afetados pelos alimentos consumidos. As bact√©rias no intestino desempenham muitas fun√ß√µes importantes para os seres humanos, incluindo a decomposi√ß√£o e auxiliando na absor√ß√£o de alimentos indigestos; estimular o crescimento celular; reprimindo o crescimento de bact√©rias nocivas, treinando o sistema imunol√≥gico para responder apenas a pat√≥genos; produ√ß√£o de vitamina B12; e defesa contra algumas doen√ßas infecciosas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As esp√©cies de plantas terrestres dominantes da √©poca eram as gimnospermas, que s√£o plantas vasculares, sem cone e sem flores, como con√≠feras que produzem sementes sem revestimento. Isso se op√µe √† flora atual da Terra, na qual as plantas terrestres dominantes em termos de n√∫mero de esp√©cies s√£o angiospermas. Pensa-se que um g√™nero de planta em particular, o Ginkgo, tenha evolu√≠do neste momento e seja representado hoje por uma √∫nica esp√©cie, Ginkgo biloba. Al√©m disso, acredita-se que o g√™nero existente Sequoia tenha evolu√≠do no Mesoz√≥ico.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Uma iniciativa de vota√ß√£o no Colorado, conhecida como Emenda 36, teria mudado a maneira pela qual o Estado distribui seus votos eleitorais. Em vez de atribuir todos os 9 eleitores do estado ao candidato com uma pluralidade de votos populares, sob a emenda, o Colorado teria designado eleitores presidenciais proporcionalmente √† contagem de votos em todo o estado, o que seria um sistema √∫nico (Nebraska e Maine atribuem votos eleitorais com base em total de votos em cada distrito do congresso). Os depoentes alegaram que essa divis√£o diminuiria a influ√™ncia do Colorado no Col√©gio Eleitoral, e a emenda acabou fracassando, recebendo apenas 34% dos votos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Embora os rios e a costa desta √°rea estivessem entre os primeiros lugares colonizados pelos portugueses, que criaram postos comerciais no s√©culo XVI, eles n√£o exploraram o interior at√© o s√©culo XIX. Os governantes africanos locais na Guin√©, alguns dos quais prosperaram muito com o com√©rcio de escravos, controlavam o com√©rcio interior e n√£o permitiam a entrada de europeus no interior. Eles os mantinham nos assentamentos costeiros fortificados onde o com√©rcio acontecia. As comunidades africanas que lutaram contra os comerciantes de escravos tamb√©m desconfiavam de aventureiros e pretensos colonos europeus. Os portugueses na Guin√© estavam amplamente restritos ao porto de Bissau e Cacheu. Um pequeno n√∫mero de colonos europeus estabeleceu fazendas isoladas ao longo dos rios do interior de Bissau.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKerdF353l-o"
   },
   "source": [
    "As we can see, some of the texts are a full paragraph of a Wikipedia article while others are just titles or empty lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-EIELH43l_T"
   },
   "source": [
    "## 7. Masked language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWk97-Ny3l_T"
   },
   "source": [
    "For masked language modeling (MLM) we are going to use the same preprocessing as before for our dataset with one additional step: we will randomly mask some tokens (by replacing them by `[MASK]`) and the labels will be adjusted to only include the masked tokens (we don't have to predict the non-masked tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iAYlS40Z3l-v"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpOiBrJ13l-y"
   },
   "source": [
    "We can now call the tokenizer on all our texts. This is very simple, using the [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method from the Datasets library. First we define a function that call the tokenizer on our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lS2m25YM3l-z"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12F1ulgT3l_V"
   },
   "source": [
    "We can apply the same tokenization function as before, we just need to update our tokenizer to use the checkpoint we just picked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "h8RCYcvr3l_V",
    "outputId": "a5ffeb0a-71da-4b27-e57a-c62f1927562e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "DVHs5aCA3l-_"
   },
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpNfGiMw3l_A"
   },
   "source": [
    "Then we write the preprocessing function that will group our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iaAJy5Hu3l_B"
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGJWXtNv3l_C"
   },
   "source": [
    "First note that we duplicate the inputs for our labels. This is because the model of the ü§ó Transformers library apply the shifting to the right, so we don't need to do it manually.\n",
    "\n",
    "Also note that by default, the `map` method will send a batch of 1,000 examples to be treated by the preprocessing function. So here, we will drop the remainder to make the concatenated tokenized texts a multiple of `block_size` every 1,000 examples. You can adjust this behavior by passing a higher batch size (which will also be processed slower). You can also speed-up the preprocessing by using multiprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTuy8UUs3l_X"
   },
   "source": [
    "And like before, we group texts together and chunk them in samples of length `block_size`. You can skip that step if your dataset is composed of individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LVYPMwEs3l_X",
    "outputId": "e71ed7f1-b182-4643-a8fb-3d731c70e40b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFJ49iHJ3l_Z"
   },
   "source": [
    "The rest is very similar to what we had, with two exceptions. First we use a model suitable for masked LM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PM10A9Za3l_Z",
    "outputId": "fff2d5bb-397d-4d5d-9aa9-933090cb6680"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108954466"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of model parameters\n",
    "model_num_param=0\n",
    "for p in model.parameters():\n",
    "    model_num_param+=p.numel()\n",
    "model_num_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lang adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup adapters\n",
    "if train_adapter:\n",
    "        \n",
    "    # new\n",
    "    if madx2:\n",
    "        # do not add adapter in the last transformer layers \n",
    "        leave_out = [len(model.bert.encoder.layer)-1]\n",
    "    else:\n",
    "        leave_out = []\n",
    "        \n",
    "    # new\n",
    "    # task_name = data_args.dataset_name or \"mlm\"\n",
    "    task_name = \"mlm\"\n",
    "        \n",
    "    # check if adapter already exists, otherwise add it\n",
    "    if task_name not in model.config.adapters:\n",
    "            \n",
    "#             # resolve the adapter config\n",
    "#             adapter_config = AdapterConfig.load(\n",
    "#                 adapter_args.adapter_config,\n",
    "#                 non_linearity=adapter_args.adapter_non_linearity,\n",
    "#                 reduction_factor=adapter_args.adapter_reduction_factor,\n",
    "#             )\n",
    "\n",
    "        # new\n",
    "        # resolve adapter config with (eventually) the MAD-X 2.0 option\n",
    "        if adapter_config_name == \"pfeiffer\":\n",
    "            from transformers.adapters.configuration import PfeifferConfig\n",
    "            adapter_config = PfeifferConfig(non_linearity=adapter_non_linearity,\n",
    "                                            reduction_factor=adapter_reduction_factor,\n",
    "                                            leave_out=leave_out)           \n",
    "        elif adapter_config_name == \"pfeiffer+inv\":\n",
    "            from transformers.adapters.configuration import PfeifferInvConfig\n",
    "            adapter_config = PfeifferInvConfig(non_linearity=adapter_non_linearity,\n",
    "                                               reduction_factor=adapter_reduction_factor,\n",
    "                                               leave_out=leave_out)          \n",
    "        elif adapter_config_name == \"houlsby\":\n",
    "            from transformers.adapters.configuration import HoulsbyConfig\n",
    "            adapter_config = HoulsbyConfig(non_linearity=adapter_non_linearity,\n",
    "                                           reduction_factor=adapter_reduction_factor,\n",
    "                                           leave_out=leave_out)\n",
    "        elif adapter_config_name == \"houlsby+inv\":\n",
    "            from transformers.adapters.configuration import HoulsbyInvConfig\n",
    "            adapter_config = HoulsbyInvConfig(non_linearity=adapter_non_linearity,\n",
    "                                              reduction_factor=adapter_reduction_factor,\n",
    "                                              leave_out=leave_out)              \n",
    "            \n",
    "        # load a pre-trained from Hub if specified\n",
    "        if load_adapter:\n",
    "            model.load_adapter(\n",
    "                    load_adapter,\n",
    "                    config=adapter_config,\n",
    "                    load_as=task_name,\n",
    "                    with_head = False\n",
    "                )\n",
    "        # otherwise, add a fresh adapter\n",
    "        else:\n",
    "            model.add_adapter(task_name, config=adapter_config)\n",
    "                \n",
    "    # optionally load another pre-trained language adapter\n",
    "    if load_lang_adapter:\n",
    "        # resolve the language adapter config\n",
    "        lang_adapter_config = AdapterConfig.load(\n",
    "                lang_adapter_config,\n",
    "                non_linearity=lang_adapter_non_linearity,\n",
    "                reduction_factor=lang_adapter_reduction_factor,\n",
    "                leave_out=leave_out,\n",
    "            )\n",
    "        # load the language adapter from Hub\n",
    "        lang_adapter_name = model.load_adapter(\n",
    "                load_lang_adapter,\n",
    "                config=lang_adapter_config,\n",
    "                load_as=language,\n",
    "                with_head = False\n",
    "            )\n",
    "    else:\n",
    "        lang_adapter_name = None\n",
    "    # Freeze all model weights except of those of this adapter\n",
    "    model.train_adapter([task_name])\n",
    "    # Set the adapters to be used in every forward pass\n",
    "    if lang_adapter_name:\n",
    "        model.set_active_adapters([lang_adapter_name, task_name])\n",
    "    else:\n",
    "        model.set_active_adapters([task_name])\n",
    "else:\n",
    "    if load_adapter or load_lang_adapter:\n",
    "        raise ValueError(\n",
    "                \"Adapters can only be loaded in adapters training mode.\"\n",
    "                \"Use --train_adapter to enable adapter training\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (invertible_adapters): ModuleDict(\n",
       "      (mlm): NICECouplingBlock(\n",
       "        (F): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (1): Activation_Function_Class()\n",
       "          (2): Linear(in_features=192, out_features=384, bias=True)\n",
       "        )\n",
       "        (G): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (1): Activation_Function_Class()\n",
       "          (2): Linear(in_features=192, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (mlm): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (mlm): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (mlm): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (mlm): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (mlm): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (mlm): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (mlm): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (mlm): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (mlm): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (mlm): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (mlm): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class()\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                    (1): Activation_Function_Class()\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=29794, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122252002"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adapter_num_param=0\n",
    "for p in model.parameters():\n",
    "    model_adapter_num_param+=p.numel()\n",
    "model_adapter_num_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the model with adapter: 122252002\n",
      "Number of parameters of the model without adapter: 108954466\n",
      "Number of parameters of the adapter: 13297536\n",
      "Pourcentage of additional parameters through adapter: 12.2 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters of the model with adapter: {model_adapter_num_param:.0f}\")\n",
    "print(f\"Number of parameters of the model without adapter: {model_num_param:.0f}\")\n",
    "print(f\"Number of parameters of the adapter: {model_adapter_num_param - model_num_param:.0f}\")\n",
    "print(f\"Pourcentage of additional parameters through adapter:\",round(((model_adapter_num_param - model_num_param)/model_num_param)*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jElf8LJ33l_K"
   },
   "source": [
    "## 9. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "YbSwEhQ63l_L"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "if ds:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        do_train=do_train,\n",
    "        do_eval=do_eval,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        adam_beta1=adam_beta1,\n",
    "        adam_beta2=adam_beta2,\n",
    "        adam_epsilon=adam_epsilon,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        warmup_steps=warmup_steps,\n",
    "        logging_dir=logging_dir,         # directory for storing logs\n",
    "        logging_strategy=evaluation_strategy,\n",
    "        logging_steps=logging_steps,     # if strategy = \"steps\"\n",
    "        save_strategy=evaluation_strategy,          # model checkpoint saving strategy\n",
    "        save_steps=logging_steps,        # if strategy = \"steps\"\n",
    "        save_total_limit=save_total_limit,\n",
    "        fp16=fp16,\n",
    "        eval_steps=logging_steps,        # if strategy = \"steps\"\n",
    "        load_best_model_at_end=load_best_model_at_end,\n",
    "        metric_for_best_model=metric_for_best_model,\n",
    "        greater_is_better=greater_is_better,\n",
    "        disable_tqdm=disable_tqdm,\n",
    "        local_rank=gpu,\n",
    "        deepspeed=ds_config\n",
    "        )\n",
    "else:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        do_train=do_train,\n",
    "        do_eval=do_eval,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        adam_beta1=adam_beta1,\n",
    "        adam_beta2=adam_beta2,\n",
    "        adam_epsilon=adam_epsilon,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        warmup_steps=warmup_steps,\n",
    "        logging_dir=logging_dir,         # directory for storing logs\n",
    "        logging_strategy=evaluation_strategy,\n",
    "        logging_steps=logging_steps,     # if strategy = \"steps\"\n",
    "        save_strategy=evaluation_strategy,          # model checkpoint saving strategy\n",
    "        save_steps=logging_steps,        # if strategy = \"steps\"\n",
    "        save_total_limit=save_total_limit,\n",
    "        fp16=fp16,\n",
    "        eval_steps=logging_steps,        # if strategy = \"steps\"\n",
    "        load_best_model_at_end=load_best_model_at_end,\n",
    "        metric_for_best_model=metric_for_best_model,\n",
    "        greater_is_better=greater_is_better,\n",
    "        disable_tqdm=disable_tqdm,\n",
    "        local_rank=gpu,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6uuUnvz3l_b"
   },
   "source": [
    "And second, we use a special `data_collator`. The `data_collator` is a function that is responsible of taking the samples and batching them in tensors. In the previous example, we had nothing special to do, so we just used the default for this argument. Here we want to do the random-masking. We could do it as a pre-processing step (like the tokenization) but then the tokens would always be masked the same way at each epoch. By doing this step inside the `data_collator`, we ensure this random masking is done in a new way each time we go over the data.\n",
    "\n",
    "To do this masking for us, the library provides a `DataCollatorForLanguageModeling`. We can adjust the probability of the masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "nRZ-5v_P3l_b"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a compute metrics (accuracy). Even if it is always better to eveluate a model against a metric, we will not use it to evaluate the best model during the training as it can make a CUDA out of memory. Instead, we will use the validation loss (in the case of fine-tuning a MLM on  a new dataset, it is a common procedure). At the end of the training, we will use our compute metrics (accuracy) to get the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric accuracy\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    indices = [[i for i, x in enumerate(labels[row]) if x != -100] for row in range(len(labels))]\n",
    "\n",
    "    labels = [labels[row][indices[row]] for row in range(len(labels))]\n",
    "    temp = list()\n",
    "    for item in labels:\n",
    "        temp += item.tolist()\n",
    "    labels = temp\n",
    "\n",
    "    predictions = [predictions[row][indices[row]] for row in range(len(predictions))]\n",
    "    temp = list()\n",
    "    for item in predictions:\n",
    "        temp += item.tolist()\n",
    "    predictions = temp\n",
    "    \n",
    "    results = metric.compute(predictions=predictions, references=labels)\n",
    "    results[\"eval_accuracy\"] = results[\"accuracy\"]\n",
    "    results.pop(\"accuracy\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqHnWcYC3l_d"
   },
   "source": [
    "Then we just have to pass everything to `Trainer` and begin training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "V-Y3gNqV3l_d"
   },
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"], # .shard(index=1, num_shards=90), to be used to reduce train to 1/90\n",
    "    eval_dataset=lm_datasets[\"validation\"], #.shard(index=1, num_shards=90), to be used to reduce validation to 1/90\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "    do_save_full_model=not train_adapter, \n",
    "    do_save_adapters=train_adapter,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)],\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.args._n_gpu = n_gpu # train on one GPU but as we use local_rank in training_args, it is not needed\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    " [ 90306/155700 9:36:09 < 6:57:13, 2.61 it/s, Epoch 58/100]\n",
    "Epoch\tTraining Loss\tValidation Loss\tRuntime\tSamples Per Second\n",
    "1\t1.964100\t1.868401\t48.642500\t163.705000\n",
    "2\t1.884600\t1.839256\t46.086900\t172.782000\n",
    "3\t1.849100\t1.821544\t44.457500\t179.115000\n",
    "4\t1.820000\t1.807454\t44.421500\t179.260000\n",
    "5\t1.802400\t1.812738\t70.805100\t112.464000\n",
    "6\t1.785700\t1.807045\t71.513400\t111.350000\n",
    "7\t1.765500\t1.807914\t70.185800\t113.456000\n",
    "8\t1.748500\t1.795039\t58.680800\t135.700000\n",
    "9\t1.735000\t1.801956\t71.341500\t111.618000\n",
    "10\t1.731300\t1.788569\t71.223500\t111.803000\n",
    "11\t1.722000\t1.793057\t71.501900\t111.368000\n",
    "12\t1.707200\t1.792752\t79.366800\t100.332000\n",
    "13\t1.697600\t1.778756\t69.999200\t113.758000\n",
    "14\t1.690600\t1.778784\t70.264500\t113.329000\n",
    "15\t1.685000\t1.774370\t71.556900\t111.282000\n",
    "16\t1.678300\t1.769456\t74.719500\t106.572000\n",
    "17\t1.664600\t1.766170\t73.120800\t108.902000\n",
    "18\t1.653300\t1.774301\t78.627900\t101.274000\n",
    "19\t1.648000\t1.772815\t77.317800\t102.991000\n",
    "20\t1.643000\t1.772505\t72.899100\t109.233000\n",
    "21\t1.635100\t1.763878\t71.642500\t111.149000\n",
    "22\t1.629000\t1.774589\t71.053400\t112.071000\n",
    "23\t1.619600\t1.774103\t70.501900\t112.947000\n",
    "24\t1.618100\t1.775866\t71.579600\t111.247000\n",
    "25\t1.608600\t1.784808\t71.177600\t111.875000\n",
    "26\t1.605500\t1.769531\t71.822100\t110.871000\n",
    "27\t1.599500\t1.773246\t71.530800\t111.323000\n",
    "28\t1.597200\t1.778355\t70.789500\t112.489000\n",
    "29\t1.591000\t1.771813\t70.624700\t112.751000\n",
    "30\t1.590600\t1.756565\t70.701000\t112.629000\n",
    "31\t1.587000\t1.757311\t71.865700\t110.804000\n",
    "32\t1.578500\t1.754790\t71.301100\t111.681000\n",
    "33\t1.574900\t1.757490\t70.887400\t112.333000\n",
    "34\t1.568900\t1.766495\t71.213400\t111.819000\n",
    "35\t1.562000\t1.757397\t70.766200\t112.526000\n",
    "36\t1.565900\t1.761957\t71.060400\t112.060000\n",
    "37\t1.557800\t1.753144\t71.445100\t111.456000\n",
    "38\t1.551700\t1.758267\t73.499100\t108.341000\n",
    "39\t1.547500\t1.761052\t78.165800\t101.873000\n",
    "40\t1.546500\t1.759224\t77.168200\t103.190000\n",
    "41\t1.537100\t1.765854\t48.721500\t163.439000\n",
    "42\t1.539700\t1.764972\t49.488800\t160.905000\n",
    "43\t1.531600\t1.762541\t48.618300\t163.786000\n",
    "44\t1.527000\t1.761401\t49.214100\t161.803000\n",
    "45\t1.524700\t1.772806\t48.609400\t163.816000\n",
    "46\t1.523900\t1.751668\t48.683200\t163.568000\n",
    "47\t1.519000\t1.764286\t49.266000\t161.633000\n",
    "48\t1.519900\t1.746866\t49.417400\t161.138000\n",
    "49\t1.516500\t1.757262\t49.317300\t161.465000\n",
    "50\t1.511100\t1.771098\t49.419500\t161.131000\n",
    "51\t1.514400\t1.752675\t49.327000\t161.433000\n",
    "52\t1.506900\t1.759398\t48.620300\t163.779000\n",
    "53\t1.506700\t1.774234\t49.339000\t161.394000\n",
    "54\t1.500000\t1.768661\t49.261600\t161.647000\n",
    "55\t1.500400\t1.754474\t48.522800\t164.109000\n",
    "56\t1.492200\t1.775785\t48.383900\t164.579000\n",
    "57\t1.492800\t1.750307\t48.982300\t162.569000\n",
    "58\t1.488500\t1.759718\t49.337400\t161.399000\n",
    "\n",
    "TrainOutput(global_step=90306, training_loss=1.6183313755799726, metrics={'train_runtime': 34570.5396, 'train_samples_per_second': 4.504, 'total_flos': 2.7131033530925056e+17, 'epoch': 58.0, 'init_mem_cpu_alloc_delta': 2080903168, 'init_mem_gpu_alloc_delta': 504964608, 'init_mem_cpu_peaked_delta': 88915968, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 137281536, 'train_mem_gpu_alloc_delta': 249311744, 'train_mem_cpu_peaked_delta': 183021568, 'train_mem_gpu_peaked_delta': 4172747264})\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dtype\n",
    "trainer.model.bert.embeddings.word_embeddings.weight.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the weights dtype is float16, use the script `zero_to_fp32.py` to get them in float32 as explained in [Getting The Model Weights Out](https://huggingface.co/transformers/main_classes/deepspeed.html?highlight=deepspeed#getting-the-model-weights-out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='498' max='249' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [249/249 13:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.7610177993774414,\n",
       " 'eval_runtime': 49.1643,\n",
       " 'eval_samples_per_second': 161.967,\n",
       " 'epoch': 58.0,\n",
       " 'eval_mem_cpu_alloc_delta': 0,\n",
       " 'eval_mem_gpu_alloc_delta': 0,\n",
       " 'eval_mem_cpu_peaked_delta': 0,\n",
       " 'eval_mem_gpu_peaked_delta': 990020608}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the metric accuracy\n",
    "# trainer.compute_metrics=compute_metrics\n",
    "\n",
    "# calculation of the performance on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "4hSaANqj3l_g",
    "outputId": "eeeb8727-2e27-4aeb-ac71-c98123214661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.82\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "# print(f\"Accuracy: {eval_results['eval_accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 141M\r\n",
      "-rw-rw-r-- 1 pierre pierre 598 Jul  8 12:27 adapter_config.json\r\n",
      "-rw-rw-r-- 1 pierre pierre 231 Jul  8 12:27 head_config.json\r\n",
      "-rw-rw-r-- 1 pierre pierre 51M Jul  8 12:27 pytorch_adapter.bin\r\n",
      "-rw-rw-r-- 1 pierre pierre 90M Jul  8 12:27 pytorch_model_head.bin\r\n"
     ]
    }
   ],
   "source": [
    "# save adapter + head\n",
    "adapters_folder = 'adapters-' + task_name\n",
    "path_to_save_adapter = path_to_outputs/adapters_folder\n",
    "trainer.model.save_adapter(str(path_to_save_adapter), adapter_name=task_name, with_head=True)\n",
    "\n",
    "!ls -lh {path_to_save_adapter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDBi0reX3l_g"
   },
   "source": [
    "Now, you can push the saved adapter + head to the [AdapterHub](https://adapterhub.ml/) (follow instructions at [Contributing to Adapter Hub](https://docs.adapterhub.ml/contributing.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = os.getenv('PATH')\n",
    "# replace xxxx by your username on your server (ex: paulo)\n",
    "# replace yyyy by the name of the virtual environment of this notebook (ex: adapter-transformers)\n",
    "%env PATH=/mnt/home/xxxx/anaconda3/envs/yyyy/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6091;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "%tensorboard --logdir {logging_dir} --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Application MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true,
    "id": "Ckl2Fzn3is0F"
   },
   "outputs": [],
   "source": [
    "### import transformers\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model original (without lang adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the model `neuralmind/bert-base-portuguese-cased` and its trainned lang adapter within the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true,
    "id": "IJA9CgOBis0F",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "tokenizer_mlm = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"fill-mask\", model=model_mlm, tokenizer=tokenizer_mlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take one sentence from the SQuAD 1.1 pt dataset and replace the word `Deus` by the token `[MASK]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'O pante√≠smo sustenta que Deus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.7392684817314148,\n",
       "  'token': 2538,\n",
       "  'token_str': 'Deus'},\n",
       " {'sequence': 'O pante√≠smo sustenta que deus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.042948465794324875,\n",
       "  'token': 4023,\n",
       "  'token_str': 'deus'},\n",
       " {'sequence': 'O pante√≠smo sustenta que ele √© o universo e o universo √© Deus.',\n",
       "  'score': 0.029601380228996277,\n",
       "  'token': 368,\n",
       "  'token_str': 'ele'},\n",
       " {'sequence': 'O pante√≠smo sustenta que Cristo √© o universo e o universo √© Deus.',\n",
       "  'score': 0.021081821992993355,\n",
       "  'token': 4184,\n",
       "  'token_str': 'Cristo'},\n",
       " {'sequence': 'O pante√≠smo sustenta que tudo √© o universo e o universo √© Deus.',\n",
       "  'score': 0.018854131922125816,\n",
       "  'token': 2745,\n",
       "  'token_str': 'tudo'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"O pante√≠smo sustenta que [MASK] √© o universo e o universo √© Deus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test now the original model with another sentence and `China` has masked word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na China.',\n",
       "  'score': 0.9124720096588135,\n",
       "  'token': 3278,\n",
       "  'token_str': 'China'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na √çndia.',\n",
       "  'score': 0.034306950867176056,\n",
       "  'token': 4340,\n",
       "  'token_str': '√çndia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Mal√°sia.',\n",
       "  'score': 0.023240933194756508,\n",
       "  'token': 17753,\n",
       "  'token_str': 'Mal√°sia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Tail√¢ndia.',\n",
       "  'score': 0.013218147680163383,\n",
       "  'token': 15582,\n",
       "  'token_str': 'Tail√¢ndia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Inglaterra.',\n",
       "  'score': 0.0027242223732173443,\n",
       "  'token': 2785,\n",
       "  'token_str': 'Inglaterra'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"O primeiro caso da COVID-19 foi descoberto em Wuhan, na [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with lang adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_checkpoint.replace('/','-') + '_' + dataset_name + '/'  \n",
    "outputs = outputs + str(task) \\\n",
    "+ '_lr' + str(learning_rate) \\\n",
    "+ '_bs' + str(batch_size) \\\n",
    "+ '_GAS' + str(gradient_accumulation_steps) \\\n",
    "+ '_eps' + str(adam_epsilon) \\\n",
    "+ '_epochs' + str(num_train_epochs) \\\n",
    "+ '_patience' + str(early_stopping_patience) \\\n",
    "+ '_madx2' + str(madx2) \\\n",
    "+ '_ds' + str(ds) \\\n",
    "+ '_fp16' + str(fp16) \\\n",
    "+ '_best' + str(load_best_model_at_end) \\\n",
    "+ '_metric' + str(metric_for_best_model) \\\n",
    "+ '_adapterconfig' + str(adapter_config_name) \n",
    "\n",
    "path_to_outputs = root/'models_outputs'/outputs\n",
    "\n",
    "# Config of the lang adapter\n",
    "lang_adapter_path = path_to_outputs/'adapters-mlm/'\n",
    "\n",
    "load_lang_adapter = lang_adapter_path\n",
    "lang_adapter_config = str(lang_adapter_path) + \"/adapter_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true,
    "id": "NiuPQTxuzRrm"
   },
   "outputs": [],
   "source": [
    "# load the language adapter\n",
    "task_mlm_load_as = 'mlm'\n",
    "lang_adapter_name = model_mlm.load_adapter(\n",
    "    str(load_lang_adapter),\n",
    "    config=lang_adapter_config,\n",
    "    load_as=task_mlm_load_as,\n",
    "    with_head=True\n",
    "    )\n",
    "\n",
    "# Set the adapters to be used in every forward pass\n",
    "model_mlm.set_active_adapters([lang_adapter_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"fill-mask\", model=model_mlm, tokenizer=tokenizer_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'O pante√≠smo sustenta que Deus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.9113659858703613,\n",
       "  'token': 2538,\n",
       "  'token_str': 'Deus'},\n",
       " {'sequence': 'O pante√≠smo sustenta que Cristo √© o universo e o universo √© Deus.',\n",
       "  'score': 0.01384457852691412,\n",
       "  'token': 4184,\n",
       "  'token_str': 'Cristo'},\n",
       " {'sequence': 'O pante√≠smo sustenta que Jesus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.013115585781633854,\n",
       "  'token': 3125,\n",
       "  'token_str': 'Jesus'},\n",
       " {'sequence': 'O pante√≠smo sustenta que deus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.008912090212106705,\n",
       "  'token': 4023,\n",
       "  'token_str': 'deus'},\n",
       " {'sequence': 'O pante√≠smo sustenta que tudo √© o universo e o universo √© Deus.',\n",
       "  'score': 0.006696512456983328,\n",
       "  'token': 2745,\n",
       "  'token_str': 'tudo'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"O pante√≠smo sustenta que [MASK] √© o universo e o universo √© Deus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fine-tuned model scored better (0.911 vs. 0.739) when finding the masked word `Deus`. It seems that our finetuning on the SQuAD 1.1 pt dataset with lang adapter worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test now our fine-tuned model with another sentence and `China` has masked word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na China.',\n",
       "  'score': 0.8868412971496582,\n",
       "  'token': 3278,\n",
       "  'token_str': 'China'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Mal√°sia.',\n",
       "  'score': 0.04414068162441254,\n",
       "  'token': 17753,\n",
       "  'token_str': 'Mal√°sia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na √çndia.',\n",
       "  'score': 0.034103650599718094,\n",
       "  'token': 4340,\n",
       "  'token_str': '√çndia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Tail√¢ndia.',\n",
       "  'score': 0.009829722344875336,\n",
       "  'token': 15582,\n",
       "  'token_str': 'Tail√¢ndia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Alemanha.',\n",
       "  'score': 0.003259493038058281,\n",
       "  'token': 2423,\n",
       "  'token_str': 'Alemanha'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"O primeiro caso da COVID-19 foi descoberto em Wuhan, na [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The masked word `China` was found with a high score of 0.887 but lower than the score of the orginal model (0.912). It was expected: by finetuning the original model, we specialized it to the \"language\" of the dataset used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Language Modeling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adapter-transformers",
   "language": "python",
   "name": "adapter-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
