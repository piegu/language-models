{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a MLM (Masked Language Model) like BERT (base or large) with the library adapter-transformers (notebook version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Credit**: [Hugging Face](https://huggingface.co/) and [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)\n",
    "- **Author**: [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/)\n",
    "- **Date**: 05/07/2021\n",
    "- **Blog post**: [NLP nas empresas | Como ajustar um modelo de linguagem natural como BERT a um novo dom√≠nio lingu√≠stico com um Adapter?](https://medium.com/@pierre_guillou/nlp-nas-empresas-como-ajustar-um-modelo-de-linguagem-natural-como-bert-a-um-novo-dom%C3%ADnio-23752b73b185)\n",
    "- **Link to the folder in github with this notebook and all necessary scripts**: [language-modeling with adapters](https://github.com/piegu/language-models/tree/master/adapters/language-modeling/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3KD3WXU3l-O"
   },
   "source": [
    "The objective here is to **fine-tune a Masked Language Model (MLM) like BERT (base or large) by training adapters (library [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)), not the embeddings and transformers layers of the MLM model**, and to compare results with BERT model fully fine-tune for the same task.\n",
    "\n",
    "The interest is obvious: if you need models for different NLP tasks, instead of fine-tuning and storing one model by NLP task, **you store only one MLM model and the trained tasks adapters which sizes are about 3% of the MLM model one**. More, the loading of these adapters in production is very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAscNNUD3l-P"
   },
   "source": [
    "In this notebook, we'll see how to fine-tune one of the [ü§ó Transformers](https://github.com/huggingface/transformers) model on a language modeling tasks. We will cover one type of language modeling tasks which is:\n",
    "\n",
    "- Masked language modeling: the model has to predict some tokens that are masked in the input. It still has access to the whole sentence, so it can use the tokens before and after the tokens masked to predict their value.\n",
    "\n",
    "![Widget inference representing the masked language modeling task](images/masked_language_modeling_adapter.png)\n",
    "\n",
    "We will see how to easily load and preprocess the dataset for each one of those tasks, and how to use the `Trainer` API to fine-tune a model on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History and Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an adaptation of the following notebooks and scripts for **fine-tuning a (transformer) Masked Language Model (MLM) like BERT (base or large) with any dataset** (we use here the texts of the [Portuguese Squad 1.1 dataset](https://forum.ailab.unb.br/t/datasets-em-portugues/251/4)):\n",
    "- **from [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)** | notebook [01_Adapter_Training.ipynb](https://github.com/Adapter-Hub/adapter-transformers/blob/master/notebooks/01_Adapter_Training.ipynb) and script [run_mlm.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/language-modeling/run_mlm.py) (this script was adapted from the script [run_mlm.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py) of HF)\n",
    "- **from [transformers](https://github.com/huggingface/transformers) of Hugging Face** | notebook [language_modeling.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb) and script [run_mlm.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py) \n",
    "\n",
    "In order to speed up the fine-tuning of the model on only one GPU, the library [DeepSpeed](https://www.deepspeed.ai/) could be used by applying the configuration provided by HF in the notebook [transformers + deepspeed CLI](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb) but as the library adapter-transformers is not synchronized with the last version of the library transformers of HF, we keep that option for the future.\n",
    "\n",
    "*Note: the paragraph about Causal language modeling (CLM) is not included in this notebook, and all the non necessary code about Masked Model Language (MLM) has been deleted from the original notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major changes from original notebooks and scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook [language_modeling.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb) and script [run_mlm.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/language-modeling/run_mlm.py) allow to evaluate the model performance against the validation loss at the end of each epoch, not against the metric accuracy. \n",
    "\n",
    "As a metric is better in order to select a model than the loss, we introduced in this notebook the metric accuracy for model evaluation (see the method `comput_metrics()`). However, as it needs many GB for the evaluation calculation, we do not use it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we updated the notebook [language_modeling.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb)  to [language_modeling_adapter.ipynb](https://github.com/piegu/language-models/blob/master/adapters/language_modeling/language_modeling_adapter.ipynb) with the following changes:\n",
    "- **Accuracy**: model evaluation through eval accuracy\n",
    "- **EarlyStopping** by selecting the model with the highest eval accuracy (patience of 3 before ending the training)\n",
    "- **MAD-X 2.0** that allows not to train adapters in the last transformer layer (read page 6 of [UNKs Everywhere: Adapting Multilingual Language Models to New Scripts](https://arxiv.org/pdf/2012.15562.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "#root path\n",
    "root = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "Pytorch: 1.9.0\n",
      "adapter-transformers: 2.0.1\n",
      "HF transformers: 4.5.1\n",
      "tokenizers: 0.10.3\n",
      "datasets: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import sys; print('python:',sys.version)\n",
    "\n",
    "import torch; print('Pytorch:',torch.__version__)\n",
    "\n",
    "import transformers; print('adapter-transformers:',transformers.__version__)\n",
    "import transformers; print('HF transformers:',transformers.__hf_version__)\n",
    "import tokenizers; print('tokenizers:',tokenizers.__version__)\n",
    "import datasets; print('datasets:',datasets.__version__)\n",
    "\n",
    "# import deepspeed; print('deepspeed:',deepspeed.__version__)\n",
    "\n",
    "# Versions used in the virtuel environment of this notebook:\n",
    "\n",
    "# python: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
    "# [GCC 7.5.0]\n",
    "# Pytorch: 1.9.0\n",
    "# adapter-transformers: 2.0.1\n",
    "# transformers: 4.5.1\n",
    "# tokenizers: 0.10.3\n",
    "# datasets: 1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a MLM BERT base or large in the dataset language\n",
    "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "# model_checkpoint = \"neuralmind/bert-large-portuguese-cased\"\n",
    "\n",
    "# SQuAD 1.1 in Portuguese\n",
    "dataset_name = \"squad11pt\" # SQuAD v1.1 em portugu√™s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mlm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments\n",
    "batch_size = 32\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_train_epochs = 100.\n",
    "early_stopping_patience = 5\n",
    "\n",
    "adam_epsilon = 1e-6\n",
    "\n",
    "fp16 = True\n",
    "ds = False # DeepSpeed\n",
    "\n",
    "# best model\n",
    "load_best_model_at_end = True \n",
    "if load_best_model_at_end:\n",
    "    metric_for_best_model = \"loss\" # could be accuracy, too\n",
    "    if metric_for_best_model == \"accuracy\":\n",
    "        greater_is_better = True\n",
    "    else:\n",
    "        greater_is_better = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train adapter\n",
    "train_adapter = True # we want to train an adapter\n",
    "load_adapter = None # we do not upload an existing adapter \n",
    "load_lang_adapter = None # we do not upload an existing lang adapter\n",
    "\n",
    "# if True, do not put adapter in the last transformer layer\n",
    "madx2 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "n_gpu = 1 # train on just one GPU\n",
    "gpu = 0 # select the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this notebook in GPU 0\n",
    "# As we do not launch a python script in this notebook, this cell is not mandatory\n",
    "import os\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "if gpu == 0:\n",
    "    os.environ['MASTER_PORT'] = '9996' # modify if RuntimeError: Address already in use # GPU 0\n",
    "elif gpu == 1:\n",
    "    os.environ['MASTER_PORT'] = '9995'\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = str(gpu)\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lang adapter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang adapter config\n",
    "adapter_config = \"pfeiffer+inv\" # houlsby+inv is possible, too\n",
    "adapter_non_linearity = 'gelu' # relu is possible, too\n",
    "adapter_reduction_factor = 2\n",
    "language = 'pt '# pt = Portuguese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training arguments of the HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the training argument\n",
    "do_train = True \n",
    "do_eval = True \n",
    "\n",
    "# epochs, bs, GA\n",
    "evaluation_strategy = \"epoch\" # no\n",
    "\n",
    "# fp16\n",
    "fp16_opt_level = 'O1'\n",
    "fp16_backend = \"auto\"\n",
    "fp16_full_eval = False\n",
    "\n",
    "# optimizer (AdamW)\n",
    "weight_decay = 0.01 # 0.0\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "\n",
    "# scheduler\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.0\n",
    "warmup_steps = 0\n",
    "\n",
    "# logs\n",
    "logging_strategy = \"steps\"\n",
    "logging_first_step = True # False\n",
    "logging_steps = 500     # if strategy = \"steps\"\n",
    "eval_steps = logging_steps # logging_steps\n",
    "\n",
    "# checkpoints\n",
    "save_strategy = \"epoch\" # steps\n",
    "save_steps = 500 # if save_strategy = \"steps\"\n",
    "save_total_limit = 1 # None\n",
    "\n",
    "# no cuda, seed\n",
    "no_cuda = False\n",
    "seed = 42\n",
    "\n",
    "# bar\n",
    "disable_tqdm = False # True\n",
    "remove_unused_columns = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder for training outputs\n",
    "\n",
    "outputs = model_checkpoint.replace('/','-') + '_' + dataset_name + '/'  \n",
    "outputs = outputs + str(task) \\\n",
    "+ '_lr' + str(learning_rate) \\\n",
    "+ '_bs' + str(batch_size) \\\n",
    "+ '_GAS' + str(gradient_accumulation_steps) \\\n",
    "+ '_eps' + str(adam_epsilon) \\\n",
    "+ '_epochs' + str(num_train_epochs) \\\n",
    "+ '_patience' + str(early_stopping_patience) \\\n",
    "+ '_madx2' + str(madx2) \\\n",
    "+ '_ds' + str(ds) \\\n",
    "+ '_fp16' + str(fp16) \\\n",
    "+ '_best' + str(load_best_model_at_end) \\\n",
    "+ '_metric' + str(metric_for_best_model) \\\n",
    "+ '_adapterconfig' + str(adapter_config)\n",
    "\n",
    "# path to outputs\n",
    "path_to_outputs = root/'models_outputs'/outputs\n",
    "\n",
    "# subfolder for model outputs\n",
    "output_dir = path_to_outputs/'output_dir' \n",
    "overwrite_output_dir = True # False\n",
    "\n",
    "# logs\n",
    "logging_dir = path_to_outputs/'logging_dir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r_n9OWV3l-Q"
   },
   "source": [
    "## 6. Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dataset_name == \"squad11pt\":\n",
    "    \n",
    "#     # create dataset folder \n",
    "#     path_to_dataset = root/'data'/dataset_name\n",
    "#     path_to_dataset.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "#     # Get dataset SQUAD in Portuguese\n",
    "#     %cd {path_to_dataset}\n",
    "#     !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn\" -O squad-pt.tar.gz && rm -rf /tmp/cookies.txt\n",
    "\n",
    "#     # unzip \n",
    "#     !tar -xvf squad-pt.tar.gz\n",
    "\n",
    "#     # Get the train and validation json file in the HF script format \n",
    "#     # inspiration: file squad.py at https://github.com/huggingface/datasets/tree/master/datasets/squad\n",
    "    \n",
    "#     import json \n",
    "#     files = ['squad-train-v1.1.json','squad-dev-v1.1.json']\n",
    "\n",
    "#     for file in files:\n",
    "\n",
    "#         # Opening JSON file & returns JSON object as a dictionary \n",
    "#         f = open(file, encoding=\"utf-8\") \n",
    "#         data = json.load(f) \n",
    "\n",
    "#         # Iterating through the json list \n",
    "#         context_list = list()\n",
    "#         id_list = list()\n",
    "\n",
    "#         for row in data['data']: \n",
    "\n",
    "#             for paragraph in row['paragraphs']:\n",
    "#                 context = (paragraph['context']).strip()\n",
    "#                 context_list.append(context)\n",
    "\n",
    "#         # Get unique context\n",
    "#         unique_context_list = list(set(context_list))\n",
    "\n",
    "#         # Closing file \n",
    "#         f.close() \n",
    "\n",
    "#         file_name = 'pt_' + str(file).replace('json','txt')\n",
    "#         with open(file_name, 'wb') as list_file:\n",
    "#             pickle.dump(unique_context_list, list_file)\n",
    "         \n",
    "#     %cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1-9jepM3l-W"
   },
   "source": [
    "You can replace the dataset above with any dataset hosted on [the hub](https://huggingface.co/datasets) or use your own files. Just uncomment the following cell and replace the paths with values that will lead to your files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uxSaGa_l3l-W"
   },
   "outputs": [],
   "source": [
    "# datasets = load_dataset(\"text\", data_files={\"train\": path_to_train.txt, \"validation\": path_to_validation.txt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY1SwIrY3l-a"
   },
   "source": [
    "You can also load datasets from a csv or a JSON file, see the [full documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"squad11pt\":\n",
    "    \n",
    "    path_to_data = root/'data'/dataset_name\n",
    "    files = ['pt_squad-train-v1.1.txt','pt_squad-dev-v1.1.txt']\n",
    "    \n",
    "    for i,file in enumerate(files):\n",
    "        path_to_file = path_to_data/file\n",
    "        with open(path_to_file, \"rb\") as f:   # Unpickling\n",
    "            text_list = pickle.load(f)\n",
    "\n",
    "            with open(file, \"w\") as output:\n",
    "                output.write(str(text_list))\n",
    "        \n",
    "        df = pd.DataFrame(text_list,columns=['text'])\n",
    "        if i == 0:\n",
    "            df_train = df.copy()\n",
    "        else:\n",
    "            df_validation = df.copy()\n",
    "            \n",
    "    from datasets import Dataset, DatasetDict\n",
    "    dataset_train = Dataset.from_pandas(df_train)\n",
    "    dataset_validation = Dataset.from_pandas(df_validation)\n",
    "\n",
    "    datasets = DatasetDict()\n",
    "    datasets['train'] = dataset_train\n",
    "    datasets['validation'] = dataset_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'O pante√≠smo sustenta que Deus √© o universo e o universo √© Deus, enquanto o panente√≠smo sustenta que Deus cont√©m, mas n√£o √© id√™ntico ao universo. √â tamb√©m a vis√£o da Igreja Cat√≥lica Liberal; Teosofia; algumas vis√µes do hindu√≠smo, exceto o vaisnavismo, que acredita no panente√≠smo; Sikhismo; algumas divis√µes do neopaganismo e tao√≠smo, juntamente com muitas denomina√ß√µes e indiv√≠duos variados dentro das denomina√ß√µes. A Cabala, Misticismo judaico, pinta uma vis√£o pante√≠sta / panente√≠sta de Deus - que tem ampla aceita√ß√£o no juda√≠smo hass√≠dico, particularmente de seu fundador The Baal Shem Tov - mas apenas como um complemento √† vis√£o judaica de um deus pessoal, n√£o no pante√≠sta original sensa√ß√£o que nega ou limita a persona a Deus. [cita√ß√£o necess√°rio]'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ur5sNUcZ3l-g"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1Uk8NROQ3l-k",
    "outputId": "a822dcec-51e3-4dba-c73c-dba9e0301726"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A √∫nica irm√£ de Elizabeth, princesa Margaret, nasceu em 1930. As duas princesas foram educadas em casa, sob a supervis√£o de sua m√£e e de sua governanta, Marion Crawford, que era conhecida casualmente como \"Crawfie\". As aulas concentraram-se em hist√≥ria, linguagem, literatura e m√∫sica. Crawford publicou uma biografia dos anos de inf√¢ncia de Elizabeth e Margaret, intitulada As princesinhas, em 1950, para grande consterna√ß√£o da fam√≠lia real. O livro descreve o amor de Elizabeth por cavalos e c√£es, sua ordem e sua atitude de responsabilidade. Outros ecoaram tais observa√ß√µes: Winston Churchill descreveu Elizabeth quando ela tinha dois anos como \"uma personagem. Ela tem um ar de autoridade e reflexividade surpreendentes em uma crian√ßa\". Sua prima Margaret Rhodes a descreveu como \"uma menina alegre, mas fundamentalmente sensata e bem-comportada\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Um corante vermelho chamado Kermes foi produzido a partir do per√≠odo neol√≠tico, secando e depois esmagando os corpos das f√™meas de um inseto de pequena escala do g√™nero Kermes, principalmente o Kermes vermilio. Os insetos vivem na seiva de certas √°rvores, especialmente os carvalhos de Kermes, perto da regi√£o do Mediterr√¢neo. Frascos de kermes foram encontrados em um enterro neol√≠tico em Adaoutse, Bouches-du-Rh√¥ne. Kermes de carvalhos foi mais tarde usado pelos romanos, que o importaram da Espanha. Uma variedade diferente de corante foi feita a partir de insetos de escama Porphyrophora hamelii (cochonilha arm√™nia) que viviam nas ra√≠zes e caules de certas ervas. Foi mencionado em textos j√° no s√©culo VIII aC, e foi usado pelos antigos ass√≠rios e persas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outras refer√™ncias ao material de Fleming podem ser encontradas ao longo do filme; um esconderijo do MI6 √© chamado \"Hildebrand Rarities and Antiques\", uma refer√™ncia ao conto \"The Hildebrand Rarity\" da cole√ß√£o de contos Somente para seus olhos. [cita√ß√£o necess√°rio] A tortura de Bond por Blofeld espelha sua tortura pelo personagem-t√≠tulo de Kingsley O romance de continua√ß√£o de Amis, Coronel Sun. [cita√ß√£o necess√°rio]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A s√©rie se passa tr√™s anos ap√≥s os eventos de Digimon Adventure 02, quando Digimon, que fica desonesto por uma infec√ß√£o misteriosa, parece causar estragos no mundo humano. Tai e os outros DigiDestined da s√©rie original se re√∫nem com seus parceiros e come√ßam a revidar com o apoio do governo japon√™s, enquanto Davis, Yolei, Cody e Ken s√£o derrotados por um poderoso inimigo chamado Alphamon e desaparecem sem deixar rasto. Tai e os outros tamb√©m conhecem outro DigiDestined chamado Meiko Mochizuki e seu parceiro Meicoomon que se tornam seus amigos, at√© Meicoomon se tornar hostil tamb√©m e fugir ap√≥s um encontro com Ken, que reaparece repentinamente, mais uma vez como o Imperador Digimon. A s√©rie de filmes tamb√©m apresenta v√°rios DigiDestined com seus parceiros Digivolve at√© o n√≠vel mega pela primeira vez, um feito que Tai e Matt haviam conseguido anteriormente.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As reuni√µes para adora√ß√£o e estudo s√£o realizadas nos Sal√µes do Reino, que geralmente t√™m car√°ter funcional e n√£o cont√™m s√≠mbolos religiosos. As testemunhas s√£o designadas para uma congrega√ß√£o em cujo \"territ√≥rio\" geralmente residem e participam de cultos semanais a que se referem como \"reuni√µes\", conforme agendado pelos anci√£os da congrega√ß√£o. As reuni√µes s√£o amplamente dedicadas ao estudo da literatura da Watch Tower Society e da B√≠blia. O formato das reuni√µes √© estabelecido pela sede da religi√£o, e o assunto da maioria das reuni√µes √© o mesmo em todo o mundo. As congrega√ß√µes se re√∫nem para duas sess√µes por semana, compreendendo cinco reuni√µes distintas que totalizam cerca de tr√™s horas e meia, geralmente reunindo no meio da semana (tr√™s reuni√µes) e no fim de semana (duas reuni√µes). Antes de 2009, as congrega√ß√µes se reuniam tr√™s vezes por semana; essas reuni√µes foram condensadas, com a inten√ß√£o de que os membros dediquem uma noite ao \"culto em fam√≠lia\". As reuni√µes s√£o abertas e fechadas com kingdom can√ß√µes (hinos) e breves ora√ß√µes. Duas vezes por ano, Testemunhas de v√°rias congrega√ß√µes que formam um \"circuito\" se re√∫nem para uma assembl√©ia de um dia. Grupos maiores de congrega√ß√µes se re√∫nem uma vez por ano para uma \"conven√ß√£o regional\" de tr√™s dias, geralmente em est√°dios ou audit√≥rios alugados. Seu evento mais importante e solene √© a comemora√ß√£o da \"Refei√ß√£o Noturna do Senhor\", ou \"Memorial da Morte de Cristo\" na data da P√°scoa Judaica.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O protestantismo teve uma influ√™ncia importante na ci√™ncia. De acordo com a tese de Merton, houve uma correla√ß√£o positiva entre a ascens√£o do puritanismo ingl√™s e o pietismo alem√£o, por um lado, e a ci√™ncia experimental inicial, por outro. A tese de Merton tem duas partes distintas: em primeiro lugar, apresenta uma teoria de que a ci√™ncia muda devido ao ac√∫mulo de observa√ß√µes e √† melhoria da t√©cnica e metodologia experimentais; em segundo lugar, argumenta que a popularidade da ci√™ncia na Inglaterra do s√©culo XVII e a demografia religiosa da Royal Society (cientistas ingleses da √©poca eram predominantemente puritanos ou outros protestantes) podem ser explicadas por uma correla√ß√£o entre o protestantismo e os valores cient√≠ficos . Merton se concentrou no puritanismo ingl√™s e no pietismo alem√£o como respons√°veis pelo desenvolvimento da revolu√ß√£o cient√≠fica dos s√©culos XVII e XVIII. Ele explicou que a conex√£o entre afilia√ß√£o religiosa e interesse pela ci√™ncia era resultado de uma sinergia significativa entre os valores protestantes asc√©ticos e os da ci√™ncia moderna. Os valores protestantes incentivaram a pesquisa cient√≠fica, permitindo que a ci√™ncia identificasse a influ√™ncia de Deus no mundo - sua cria√ß√£o - e, assim, fornecendo uma justificativa religiosa para a pesquisa cient√≠fica.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>As l√≠nguas iranianas ou l√≠nguas iranianas formam um ramo das l√≠nguas indo-iranianas, que por sua vez s√£o um ramo da Fam√≠lia de l√≠nguas indo-europ√©ias. Os falantes das l√≠nguas iranianas s√£o conhecidos como povos iranianos. As l√≠nguas iranianas hist√≥ricas est√£o agrupadas em tr√™s est√°gios: iraniano antigo (at√© 400 aC), iraniano m√©dio (400 aC - 900 dC) e novo iraniano (desde 900 dC). Das l√≠nguas iranianas antigas, as mais compreendidas e registradas s√£o o persa antigo (uma l√≠ngua do Ir√£ Aquem√™nida) e o avestan (a l√≠ngua do Avesta). As l√≠nguas iranianas m√©dias inclu√≠am o persa m√©dio (uma l√≠ngua do sass√¢nida Ir√£), o parta e o bactriano.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Os sistemas mais antigos ou simplificados podem suportar apenas os valores de TZ exigidos pelo POSIX, que especificam no m√°ximo uma regra de in√≠cio e fim explicitamente no valor. Por exemplo, TZ = 'EST5EDT, M3.2.0 / 02: 00, M11.1.0 / 02: 00' especifica o hor√°rio no leste dos Estados Unidos a partir de 2007. Esse valor de TZ deve ser alterado sempre que as regras do hor√°rio de ver√£o mudarem, e o novo valor se aplica a todos os anos, manipulando incorretamente alguns registros de data e hora mais antigos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Em 2003, foi introduzida uma taxa de congestionamento para reduzir o volume de tr√°fego no centro da cidade. Com algumas exce√ß√µes, os motoristas s√£o obrigados a pagar ¬£ 10 por dia para dirigir dentro de uma zona definida que abrange grande parte do centro de Londres. Os motoristas residentes na zona definida podem comprar um passe de temporada bastante reduzido. O governo de Londres inicialmente esperava que a Zona de Cobran√ßa de Congestionamento aumente o per√≠odo de pico di√°rio de usu√°rios de metr√¥ e √¥nibus em 20.000 pessoas, reduza o tr√°fego rodovi√°rio em 10 a 15%, aumente a velocidade do tr√°fego em 10 a 15% e reduza as filas em 20 a 30% . Ao longo de v√°rios anos, o n√∫mero m√©dio de carros que entraram no centro de Londres em um dia da semana foi reduzido de 195.000 para 125.000 carros - uma redu√ß√£o de 35% dos ve√≠culos dirigidos por dia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Magadha (s√¢nscrito: formed) formou um dos dezesseis MahƒÅ-Janapadas (s√¢nscrito: \"Grandes Pa√≠ses\") ou reinos na √çndia antiga. O n√∫cleo do reino era a √°rea de Bihar, ao sul do Ganges; sua primeira capital foi Rajagriha (moderna Rajgir) e depois Pataliputra (moderna Patna). Magadha expandiu-se para incluir a maior parte de Bihar e Bengala com a conquista de Licchavi e Anga, respectivamente, seguidas por grande parte do leste de Uttar Pradesh e Orissa. O antigo reino de Magadha √© muito mencionado nos textos jainistas e budistas. Tamb√©m √© mencionado no Ramayana, Mahabharata, Puranas. Um estado de Magadha, possivelmente um reino tribal, √© registrado nos textos v√©dicos muito antes no tempo de 600 aC. O Imp√©rio Magadha tinha grandes governantes como Bimbisara e Ajatshatru.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKerdF353l-o"
   },
   "source": [
    "As we can see, some of the texts are a full paragraph of a Wikipedia article while others are just titles or empty lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-EIELH43l_T"
   },
   "source": [
    "## 7. Masked language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWk97-Ny3l_T"
   },
   "source": [
    "For masked language modeling (MLM) we are going to use the same preprocessing as before for our dataset with one additional step: we will randomly mask some tokens (by replacing them by `[MASK]`) and the labels will be adjusted to only include the masked tokens (we don't have to predict the non-masked tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iAYlS40Z3l-v"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpOiBrJ13l-y"
   },
   "source": [
    "We can now call the tokenizer on all our texts. This is very simple, using the [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method from the Datasets library. First we define a function that call the tokenizer on our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lS2m25YM3l-z"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12F1ulgT3l_V"
   },
   "source": [
    "We can apply the same tokenization function as before, we just need to update our tokenizer to use the checkpoint we just picked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "h8RCYcvr3l_V",
    "outputId": "a5ffeb0a-71da-4b27-e57a-c62f1927562e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "DVHs5aCA3l-_"
   },
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpNfGiMw3l_A"
   },
   "source": [
    "Then we write the preprocessing function that will group our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iaAJy5Hu3l_B"
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGJWXtNv3l_C"
   },
   "source": [
    "First note that we duplicate the inputs for our labels. This is because the model of the ü§ó Transformers library apply the shifting to the right, so we don't need to do it manually.\n",
    "\n",
    "Also note that by default, the `map` method will send a batch of 1,000 examples to be treated by the preprocessing function. So here, we will drop the remainder to make the concatenated tokenized texts a multiple of `block_size` every 1,000 examples. You can adjust this behavior by passing a higher batch size (which will also be processed slower). You can also speed-up the preprocessing by using multiprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTuy8UUs3l_X"
   },
   "source": [
    "And like before, we group texts together and chunk them in samples of length `block_size`. You can skip that step if your dataset is composed of individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LVYPMwEs3l_X",
    "outputId": "e71ed7f1-b182-4643-a8fb-3d731c70e40b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFJ49iHJ3l_Z"
   },
   "source": [
    "The rest is very similar to what we had, with two exceptions. First we use a model suitable for masked LM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PM10A9Za3l_Z",
    "outputId": "fff2d5bb-397d-4d5d-9aa9-933090cb6680"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108954466"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of model parameters\n",
    "model_num_param=0\n",
    "for p in model.parameters():\n",
    "    model_num_param+=p.numel()\n",
    "model_num_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lang adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup adapters\n",
    "if train_adapter:\n",
    "        \n",
    "    # new\n",
    "    if madx2:\n",
    "        # do not add adapter in the last transformer layers \n",
    "        leave_out = [len(model.bert.encoder.layer)-1]\n",
    "    else:\n",
    "        leave_out = []\n",
    "        \n",
    "    # new\n",
    "    # task_name = data_args.dataset_name or \"mlm\"\n",
    "    task_name = \"mlm\"\n",
    "        \n",
    "    # check if adapter already exists, otherwise add it\n",
    "    if task_name not in model.config.adapters:\n",
    "            \n",
    "#             # resolve the adapter config\n",
    "#             adapter_config = AdapterConfig.load(\n",
    "#                 adapter_args.adapter_config,\n",
    "#                 non_linearity=adapter_args.adapter_non_linearity,\n",
    "#                 reduction_factor=adapter_args.adapter_reduction_factor,\n",
    "#             )\n",
    "\n",
    "        # new\n",
    "        # resolve adapter config with (eventually) the MAD-X 2.0 option\n",
    "        if adapter_config == \"pfeiffer\":\n",
    "            from transformers.adapters.configuration import PfeifferConfig\n",
    "            adapter_config = PfeifferConfig(non_linearity=adapter_non_linearity,\n",
    "                                            reduction_factor=adapter_reduction_factor,\n",
    "                                            leave_out=leave_out)           \n",
    "        elif adapter_config == \"pfeiffer+inv\":\n",
    "            from transformers.adapters.configuration import PfeifferInvConfig\n",
    "            adapter_config = PfeifferInvConfig(non_linearity=adapter_non_linearity,\n",
    "                                               reduction_factor=adapter_reduction_factor,\n",
    "                                               leave_out=leave_out)          \n",
    "        elif adapter_config == \"houlsby\":\n",
    "            from transformers.adapters.configuration import HoulsbyConfig\n",
    "            adapter_config = HoulsbyConfig(non_linearity=adapter_non_linearity,\n",
    "                                           reduction_factor=adapter_reduction_factor,\n",
    "                                           leave_out=leave_out)\n",
    "        elif adapter_config == \"houlsby+inv\":\n",
    "            from transformers.adapters.configuration import HoulsbyInvConfig\n",
    "            adapter_config = HoulsbyInvConfig(non_linearity=adapter_non_linearity,\n",
    "                                              reduction_factor=adapter_reduction_factor,\n",
    "                                              leave_out=leave_out)              \n",
    "            \n",
    "        # load a pre-trained from Hub if specified\n",
    "        if load_adapter:\n",
    "            model.load_adapter(\n",
    "                    load_adapter,\n",
    "                    config=adapter_config,\n",
    "                    load_as=task_name,\n",
    "                    with_head = False\n",
    "                )\n",
    "        # otherwise, add a fresh adapter\n",
    "        else:\n",
    "            model.add_adapter(task_name, config=adapter_config)\n",
    "                \n",
    "    # optionally load another pre-trained language adapter\n",
    "    if load_lang_adapter:\n",
    "        # resolve the language adapter config\n",
    "        lang_adapter_config = AdapterConfig.load(\n",
    "                lang_adapter_config,\n",
    "                non_linearity=lang_adapter_non_linearity,\n",
    "                reduction_factor=lang_adapter_reduction_factor,\n",
    "                leave_out=leave_out,\n",
    "            )\n",
    "        # load the language adapter from Hub\n",
    "        lang_adapter_name = model.load_adapter(\n",
    "                load_lang_adapter,\n",
    "                config=lang_adapter_config,\n",
    "                load_as=language,\n",
    "                with_head = False\n",
    "            )\n",
    "    else:\n",
    "        lang_adapter_name = None\n",
    "    # Freeze all model weights except of those of this adapter\n",
    "    model.train_adapter([task_name])\n",
    "    # Set the adapters to be used in every forward pass\n",
    "    if lang_adapter_name:\n",
    "        model.set_active_adapters([lang_adapter_name, task_name])\n",
    "    else:\n",
    "        model.set_active_adapters([task_name])\n",
    "else:\n",
    "    if load_adapter or load_lang_adapter:\n",
    "        raise ValueError(\n",
    "                \"Adapters can only be loaded in adapters training mode.\"\n",
    "                \"Use --train_adapter to enable adapter training\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (invertible_adapters): ModuleDict(\n",
       "      (mlm): NICECouplingBlock(\n",
       "        (F): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (1): Activation_Function_Class()\n",
       "          (2): Linear(in_features=192, out_features=384, bias=True)\n",
       "        )\n",
       "        (G): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (1): Activation_Function_Class()\n",
       "          (2): Linear(in_features=192, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=29794, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115751266"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adapter_num_param=0\n",
    "for p in model.parameters():\n",
    "    model_adapter_num_param+=p.numel()\n",
    "model_adapter_num_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the model with adapter: 115751266\n",
      "Number of parameters of the model without adapter: 108954466\n",
      "Number of parameters of the adapter: 6796800\n",
      "Pourcentage of additional parameters through adapter: 6.24 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters of the model with adapter: {model_adapter_num_param:.0f}\")\n",
    "print(f\"Number of parameters of the model without adapter: {model_num_param:.0f}\")\n",
    "print(f\"Number of parameters of the adapter: {model_adapter_num_param - model_num_param:.0f}\")\n",
    "print(f\"Pourcentage of additional parameters through adapter:\",round(((model_adapter_num_param - model_num_param)/model_num_param)*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jElf8LJ33l_K"
   },
   "source": [
    "## 9. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "YbSwEhQ63l_L"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    do_train=do_train,\n",
    "    do_eval=do_eval,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    adam_beta1=adam_beta1,\n",
    "    adam_beta2=adam_beta2,\n",
    "    adam_epsilon=adam_epsilon,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    warmup_steps=warmup_steps,\n",
    "    logging_dir=logging_dir,         # directory for storing logs\n",
    "    logging_strategy=evaluation_strategy,\n",
    "    logging_steps=logging_steps,     # if strategy = \"steps\"\n",
    "    save_strategy=evaluation_strategy,          # model checkpoint saving strategy\n",
    "    save_steps=logging_steps,        # if strategy = \"steps\"\n",
    "    save_total_limit=save_total_limit,\n",
    "    fp16=fp16,\n",
    "    eval_steps=logging_steps,        # if strategy = \"steps\"\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    "    metric_for_best_model=metric_for_best_model,\n",
    "    greater_is_better=greater_is_better,\n",
    "    )\n",
    "\n",
    "if ds:\n",
    "    training_args.deepspeed = ds_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6uuUnvz3l_b"
   },
   "source": [
    "And second, we use a special `data_collator`. The `data_collator` is a function that is responsible of taking the samples and batching them in tensors. In the previous example, we had nothing special to do, so we just used the default for this argument. Here we want to do the random-masking. We could do it as a pre-processing step (like the tokenization) but then the tokens would always be masked the same way at each epoch. By doing this step inside the `data_collator`, we ensure this random masking is done in a new way each time we go over the data.\n",
    "\n",
    "To do this masking for us, the library provides a `DataCollatorForLanguageModeling`. We can adjust the probability of the masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nRZ-5v_P3l_b"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a compute metrics (accuracy). Even if it is always better to eveluate a model against a metric, we will not use it to evaluate the best model during the training as it can make a CUDA out of memory. Instead, we will use the validation loss (in the case of fine-tuning a MLM on  a new dataset, it is a common procedure). At the end of the training, we will use our compute metrics (accuracy) to get the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric accuracy\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    indices = [[i for i, x in enumerate(labels[row]) if x != -100] for row in range(len(labels))]\n",
    "\n",
    "    labels = [labels[row][indices[row]] for row in range(len(labels))]\n",
    "    temp = list()\n",
    "    for item in labels:\n",
    "        temp += item.tolist()\n",
    "    labels = temp\n",
    "\n",
    "    predictions = [predictions[row][indices[row]] for row in range(len(predictions))]\n",
    "    temp = list()\n",
    "    for item in predictions:\n",
    "        temp += item.tolist()\n",
    "    predictions = temp\n",
    "    \n",
    "    results = metric.compute(predictions=predictions, references=labels)\n",
    "    results[\"eval_accuracy\"] = results[\"accuracy\"]\n",
    "    results.pop(\"accuracy\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqHnWcYC3l_d"
   },
   "source": [
    "Then we just have to pass everything to `Trainer` and begin training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "V-Y3gNqV3l_d"
   },
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"], # .shard(index=1, num_shards=90), to be used to reduce train to 1/90\n",
    "    eval_dataset=lm_datasets[\"validation\"], #.shard(index=1, num_shards=90), to be used to reduce validation to 1/90\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "    do_save_full_model=not train_adapter, \n",
    "    do_save_adapters=train_adapter,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)],\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.args._n_gpu = n_gpu # train on one GPU\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the metric accuracy\n",
    "# trainer.compute_metrics=compute_metrics\n",
    "\n",
    "# calculation of the performance on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hSaANqj3l_g",
    "outputId": "eeeb8727-2e27-4aeb-ac71-c98123214661"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "# print(f\"Accuracy: {eval_results['eval_accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save adapter + head\n",
    "adapters_folder = 'adapters-' + task_name\n",
    "path_to_save_adapter = path_to_outputs/adapters_folder\n",
    "trainer.model.save_adapter(str(path_to_save_adapter), adapter_name=task_name, with_head=True)\n",
    "\n",
    "!ls -lh {path_to_save_adapter}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDBi0reX3l_g"
   },
   "source": [
    "Now, you can push the saved adapter + head to the [AdapterHub](https://adapterhub.ml/) (follow instructions at [Contributing to Adapter Hub](https://docs.adapterhub.ml/contributing.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = os.getenv('PATH')\n",
    "# replace xxxx by your username on your server (ex: paulo)\n",
    "# replace yyyy by the name of the virtual environment of this notebook (ex: adapter-transformers)\n",
    "%env PATH=/mnt/home/xxxx/anaconda3/envs/yyyy/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "%tensorboard --logdir {logging_dir} --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Application MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true,
    "id": "Ckl2Fzn3is0F"
   },
   "outputs": [],
   "source": [
    "### import transformers\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model original (without lang adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the model `neuralmind/bert-base-portuguese-cased` and its trainned lang adapter within the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true,
    "id": "IJA9CgOBis0F",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "tokenizer_mlm = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"fill-mask\", model=model_mlm, tokenizer=tokenizer_mlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take one sentence from the SQuAD 1.1 pt dataset and replace the word `Deus` by the token `[MASK]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'O pante√≠smo sustenta que Deus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.7392684817314148,\n",
       "  'token': 2538,\n",
       "  'token_str': 'Deus'},\n",
       " {'sequence': 'O pante√≠smo sustenta que deus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.042948465794324875,\n",
       "  'token': 4023,\n",
       "  'token_str': 'deus'},\n",
       " {'sequence': 'O pante√≠smo sustenta que ele √© o universo e o universo √© Deus.',\n",
       "  'score': 0.029601380228996277,\n",
       "  'token': 368,\n",
       "  'token_str': 'ele'},\n",
       " {'sequence': 'O pante√≠smo sustenta que Cristo √© o universo e o universo √© Deus.',\n",
       "  'score': 0.021081821992993355,\n",
       "  'token': 4184,\n",
       "  'token_str': 'Cristo'},\n",
       " {'sequence': 'O pante√≠smo sustenta que tudo √© o universo e o universo √© Deus.',\n",
       "  'score': 0.018854131922125816,\n",
       "  'token': 2745,\n",
       "  'token_str': 'tudo'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"O pante√≠smo sustenta que [MASK] √© o universo e o universo √© Deus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test now the original model with another sentence and `China` has masked word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na China.',\n",
       "  'score': 0.9124720096588135,\n",
       "  'token': 3278,\n",
       "  'token_str': 'China'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na √çndia.',\n",
       "  'score': 0.034306950867176056,\n",
       "  'token': 4340,\n",
       "  'token_str': '√çndia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Mal√°sia.',\n",
       "  'score': 0.023240933194756508,\n",
       "  'token': 17753,\n",
       "  'token_str': 'Mal√°sia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Tail√¢ndia.',\n",
       "  'score': 0.013218147680163383,\n",
       "  'token': 15582,\n",
       "  'token_str': 'Tail√¢ndia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Inglaterra.',\n",
       "  'score': 0.0027242223732173443,\n",
       "  'token': 2785,\n",
       "  'token_str': 'Inglaterra'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"O primeiro caso da COVID-19 foi descoberto em Wuhan, na [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with lang adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_adapters_mlm = True\n",
    "\n",
    "if with_adapters_mlm:\n",
    "    # hyperparameters used for fine-tuning the MLM with lang adapter\n",
    "    learning_rate_mlm = 1e-4\n",
    "    batch_size_mlm = 32\n",
    "    gradient_accumulation_steps_mlm = 1\n",
    "    adam_epsilon_mlm = 1e-6\n",
    "    num_train_epoch_mlm = 100.\n",
    "    madx2_mlm = True\n",
    "    ds_mlm = False\n",
    "    fp16_mlm = True\n",
    "    load_best_model_at_end_mlm = True\n",
    "    metric_for_best_model_mlm = \"loss\"\n",
    "\n",
    "    # path to lang adapter\n",
    "    outputs_mlm = model_checkpoint.replace('/','-') + '_' + dataset_name + '/mlm' \\\n",
    "    + '_lr' + str(learning_rate_mlm) \\\n",
    "    + '_bs' + str(batch_size_mlm) \\\n",
    "    + '_GAS' + str(gradient_accumulation_steps_mlm) \\\n",
    "    + '_eps' + str(adam_epsilon_mlm) \\\n",
    "    + '_epochs' + str(num_train_epoch_mlm) \\\n",
    "    + '_madx2' + str(madx2_mlm) \\\n",
    "    + '_ds' + str(ds_mlm) \\\n",
    "    + '_fp16' + str(fp16_mlm) \\\n",
    "    + '_best' + str(load_best_model_at_end_mlm) \\\n",
    "    + '_metric' + str(metric_for_best_model_mlm)\n",
    "\n",
    "    path_to_outputs = root/'models_outputs'/outputs_mlm\n",
    "\n",
    "    # Config of the lang adapter\n",
    "    lang_adapter_path = path_to_outputs/'adapters-mlm/'\n",
    "\n",
    "    load_lang_adapter = lang_adapter_path\n",
    "    lang_adapter_config = str(lang_adapter_path) + \"/adapter_config.json\"\n",
    "    lang_adapter_non_linearity = 'gelu'\n",
    "    lang_adapter_reduction_factor = 2\n",
    "    language_mlm = 'pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true,
    "id": "NiuPQTxuzRrm"
   },
   "outputs": [],
   "source": [
    "# load the language adapter\n",
    "if with_adapters_mlm:\n",
    "    task_mlm_load_as = 'mlm'\n",
    "    lang_adapter_name = model_mlm.load_adapter(\n",
    "        str(load_lang_adapter),\n",
    "        config=lang_adapter_config,\n",
    "        load_as=task_mlm_load_as,\n",
    "        with_head=True\n",
    "    )\n",
    "else:\n",
    "    lang_adapter_name = None\n",
    "    \n",
    "# Set the adapters to be used in every forward pass\n",
    "if lang_adapter_name:\n",
    "    model_mlm.set_active_adapters([lang_adapter_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"fill-mask\", model=model_mlm, tokenizer=tokenizer_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'O pante√≠smo sustenta que Deus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.8649417161941528,\n",
       "  'token': 2538,\n",
       "  'token_str': 'Deus'},\n",
       " {'sequence': 'O pante√≠smo sustenta que ele √© o universo e o universo √© Deus.',\n",
       "  'score': 0.020680619403719902,\n",
       "  'token': 368,\n",
       "  'token_str': 'ele'},\n",
       " {'sequence': 'O pante√≠smo sustenta que Cristo √© o universo e o universo √© Deus.',\n",
       "  'score': 0.020070625469088554,\n",
       "  'token': 4184,\n",
       "  'token_str': 'Cristo'},\n",
       " {'sequence': 'O pante√≠smo sustenta que tudo √© o universo e o universo √© Deus.',\n",
       "  'score': 0.012983395718038082,\n",
       "  'token': 2745,\n",
       "  'token_str': 'tudo'},\n",
       " {'sequence': 'O pante√≠smo sustenta que Jesus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.010557097382843494,\n",
       "  'token': 3125,\n",
       "  'token_str': 'Jesus'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"O pante√≠smo sustenta que [MASK] √© o universo e o universo √© Deus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fine-tuned model scored better (0.865 vs. 0.739) when finding the masked word `Deus`. It seems that our finetuning on the SQuAD 1.1 pt dataset with lang adapter worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test now our fine-tuned model with another sentence and `China` has masked word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na China.',\n",
       "  'score': 0.8676925897598267,\n",
       "  'token': 3278,\n",
       "  'token_str': 'China'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na √çndia.',\n",
       "  'score': 0.0645902156829834,\n",
       "  'token': 4340,\n",
       "  'token_str': '√çndia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Mal√°sia.',\n",
       "  'score': 0.018060894683003426,\n",
       "  'token': 17753,\n",
       "  'token_str': 'Mal√°sia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Tail√¢ndia.',\n",
       "  'score': 0.015696141868829727,\n",
       "  'token': 15582,\n",
       "  'token_str': 'Tail√¢ndia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Alemanha.',\n",
       "  'score': 0.0037865887861698866,\n",
       "  'token': 2423,\n",
       "  'token_str': 'Alemanha'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"O primeiro caso da COVID-19 foi descoberto em Wuhan, na [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The masked word `China` was found with a high score of 0.868 but lower than the score of the orginal model (0.913). It was expected: by finetuning the original model, we specialized it to the \"language\" of the dataset used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Language Modeling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adapter-transformers",
   "language": "python",
   "name": "adapter-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
