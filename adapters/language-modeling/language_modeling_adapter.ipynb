{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a MLM (Masked Language Model) like BERT (base or large) with the library adapter-transformers (notebook version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Credit**: [Hugging Face](https://huggingface.co/) and [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)\n",
    "- **Author**: [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/)\n",
    "- **Date**: edited on 07/14/2021 (version 1.0: 07/05/2021)\n",
    "- **Blog post**: [NLP nas empresas | Como ajustar um modelo de linguagem natural como BERT a um novo dom√≠nio lingu√≠stico com um Adapter?](https://medium.com/@pierre_guillou/nlp-nas-empresas-como-ajustar-um-modelo-de-linguagem-natural-como-bert-a-um-novo-dom%C3%ADnio-23752b73b185)\n",
    "- **Link to the folder in github with this notebook and all necessary scripts**: [language-modeling with adapters](https://github.com/piegu/language-models/tree/master/adapters/language-modeling/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3KD3WXU3l-O"
   },
   "source": [
    "The objective here is to **fine-tune a Masked Language Model (MLM) like BERT (base or large) by training adapters (library [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)), not the embeddings and transformers layers of the MLM model**, and to compare results with BERT model fully fine-tune for the same task.\n",
    "\n",
    "The interest is obvious: if you need models for different NLP tasks, instead of fine-tuning and storing one model by NLP task, **you store only one MLM model and the trained tasks adapters which sizes are between 6% and 13% of the MLM model one** (it depends of the choosen adapter configuration). More, the loading of these adapters in production is very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAscNNUD3l-P"
   },
   "source": [
    "In this notebook, we'll see how to fine-tune one of the [ü§ó Transformers](https://github.com/huggingface/transformers) model on a language modeling tasks. We will cover one type of language modeling tasks which is:\n",
    "\n",
    "- Masked language modeling: the model has to predict some tokens that are masked in the input. It still has access to the whole sentence, so it can use the tokens before and after the tokens masked to predict their value.\n",
    "\n",
    "![Widget inference representing the masked language modeling task](images/masked_language_modeling_adapter.png)\n",
    "\n",
    "We will see how to easily load and preprocess the dataset for each one of those tasks, and how to use the `Trainer` API to fine-tune a model on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History and Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an adaptation of the following notebooks and scripts for **fine-tuning a (transformer) Masked Language Model (MLM) like BERT (base or large) with any dataset** (we use here the texts of the [Portuguese Squad 1.1 dataset](https://forum.ailab.unb.br/t/datasets-em-portugues/251/4)):\n",
    "- **from [adapter-transformers](https://github.com/Adapter-Hub/adapter-transformers)** | notebook [01_Adapter_Training.ipynb](https://github.com/Adapter-Hub/adapter-transformers/blob/master/notebooks/01_Adapter_Training.ipynb) and script [run_mlm.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/language-modeling/run_mlm.py) (this script was adapted from the script [run_mlm.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py) of HF)\n",
    "- **from [transformers](https://github.com/huggingface/transformers) of Hugging Face** | notebook [language_modeling.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb) and script [run_mlm.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py) \n",
    "\n",
    "In order to speed up the fine-tuning of the model on only one GPU, the library [DeepSpeed](https://www.deepspeed.ai/) could be used by applying the configuration provided by HF in the notebook [transformers + deepspeed CLI](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb) but as the library adapter-transformers is not synchronized with the last version of the library transformers of HF, we keep that option for the future.\n",
    "\n",
    "*Note: the paragraph about Causal language modeling (CLM) is not included in this notebook, and all the non necessary code about Masked Model Language (MLM) has been deleted from the original notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major changes from original notebooks and scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook [language_modeling.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb) and script [run_mlm.py](https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/language-modeling/run_mlm.py) allow to evaluate the model performance against the validation loss at the end of each epoch, not against the metric accuracy. \n",
    "\n",
    "As a metric is better in order to select a model than the loss, we introduced in this notebook the metric accuracy for model evaluation (see the method `comput_metrics()`). However, as it needs many GB for the evaluation calculation (and time!) depending on the evaluation dataset size, we do not use it during the training (we keep the loss as evaluation metric) but only before and after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we updated the notebook [language_modeling.ipynb](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb)  to [language_modeling_adapter.ipynb](https://github.com/piegu/language-models/blob/master/adapters/language_modeling/language_modeling_adapter.ipynb) with the following changes:\n",
    "- **Accuracy**: model evaluation through eval accuracy\n",
    "- **EarlyStopping** by selecting the model with the highest eval accuracy (patience of 3 before ending the training)\n",
    "- **MAD-X 2.0** that allows not to train adapters in the last transformer layer for the Pfeiffer configuration (read page 6 of [UNKs Everywhere: Adapting Multilingual Language Models to New Scripts](https://arxiv.org/pdf/2012.15562.pdf))\n",
    "- **Houlsby MHA last layer** that allows no to train adapter after the Feed Fordward but only after the MHA (Multi-Head Attention) in the last layer for the Houlsby configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "#root path\n",
    "root = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "Pytorch: 1.9.0\n",
      "adapter-transformers: 2.1.1\n",
      "HF transformers: 4.8.2\n",
      "tokenizers: 0.10.3\n",
      "datasets: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys; print('python:',sys.version)\n",
    "\n",
    "import torch; print('Pytorch:',torch.__version__)\n",
    "\n",
    "import transformers; print('adapter-transformers:',transformers.__version__)\n",
    "import transformers; print('HF transformers:',transformers.__hf_version__)\n",
    "import tokenizers; print('tokenizers:',tokenizers.__version__)\n",
    "import datasets; print('datasets:',datasets.__version__)\n",
    "\n",
    "# import deepspeed; print('deepspeed:',deepspeed.__version__)\n",
    "\n",
    "# Versions used in the virtuel environment of this notebook:\n",
    "\n",
    "# python: 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
    "# [GCC 7.5.0]\n",
    "# Pytorch: 1.9.0\n",
    "# adapter-transformers: 2.1.1\n",
    "# HF transformers: 4.8.2\n",
    "# tokenizers: 0.10.3\n",
    "# datasets: 1.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a MLM BERT base or large in the dataset language\n",
    "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
    "# model_checkpoint = \"neuralmind/bert-large-portuguese-cased\"\n",
    "\n",
    "# SQuAD 1.1 in Portuguese\n",
    "dataset_name = \"squad11pt\" # SQuAD v1.1 em portugu√™s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mlm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments\n",
    "batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_train_epochs = 100.\n",
    "early_stopping_patience = 10\n",
    "\n",
    "adam_epsilon = 1e-6\n",
    "\n",
    "fp16 = True\n",
    "ds = False # DeepSpeed\n",
    "\n",
    "# best model\n",
    "load_best_model_at_end = True \n",
    "if load_best_model_at_end:\n",
    "    metric_for_best_model = \"loss\" # could be accuracy, too\n",
    "    if metric_for_best_model == \"loss\":\n",
    "        greater_is_better = False\n",
    "    else:\n",
    "        greater_is_better = True # for accuracy\n",
    "\n",
    "# (option) number of evaluation steps to do on GPU before to put results on CPU\n",
    "eval_accumulation_steps = 5 # min = 1 (this is the recommended value to use the min GPU RAM for evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train adapter\n",
    "train_adapter = True # we want to train an adapter\n",
    "load_adapter = None # we do not upload an existing adapter \n",
    "load_lang_adapter = None # we do not upload an existing lang adapter\n",
    "\n",
    "# if True, do not put adapter in the last transformer layer (Pfeiffer configuration)\n",
    "madx2 = True\n",
    "\n",
    "# if True, put only an adapter after the MHA but not after the FF in the last layer (Houlsby configuration)\n",
    "houlsby_MHA_lastlayer = True\n",
    "if madx2:\n",
    "    houlsby_MHA_lastlayer = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "n_gpu = 1 # train on just one GPU\n",
    "gpu = 0 # select the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this notebook in GPU 0\n",
    "# As we do not launch a python script in this notebook, this cell is not mandatory\n",
    "import os\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "if gpu == 0:\n",
    "    os.environ['MASTER_PORT'] = '9996' # modify if RuntimeError: Address already in use # GPU 0\n",
    "elif gpu == 1:\n",
    "    os.environ['MASTER_PORT'] = '9995'\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = str(gpu)\n",
    "os.environ['WORLD_SIZE'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lang adapter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang adapter config\n",
    "adapter_config_name = \"pfeiffer+inv\" # houlsby+inv is possible, too\n",
    "if adapter_config_name == \"pfeiffer\" or adapter_config_name == \"pfeiffer+inv\":\n",
    "    adapter_non_linearity = 'gelu' # relu is possible, too\n",
    "elif adapter_config_name == \"houlsby\" or adapter_config_name == \"houlsby+inv\":\n",
    "    adapter_non_linearity = 'swish'\n",
    "adapter_reduction_factor = 2\n",
    "language = 'pt '# pt = Portuguese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training arguments of the HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the training argument\n",
    "do_train = True \n",
    "do_eval = True \n",
    "\n",
    "# epochs, bs, GA\n",
    "evaluation_strategy = \"epoch\" # no\n",
    "\n",
    "# fp16\n",
    "fp16_opt_level = 'O1'\n",
    "fp16_backend = \"auto\"\n",
    "fp16_full_eval = False\n",
    "\n",
    "# optimizer (AdamW)\n",
    "weight_decay = 0.01 # 0.0\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "\n",
    "# scheduler\n",
    "lr_scheduler_type = 'linear'\n",
    "warmup_ratio = 0.0\n",
    "warmup_steps = 0\n",
    "\n",
    "# logs\n",
    "logging_strategy = \"steps\"\n",
    "logging_first_step = True # False\n",
    "logging_steps = 500     # if strategy = \"steps\"\n",
    "eval_steps = logging_steps # logging_steps\n",
    "\n",
    "# checkpoints\n",
    "save_strategy = \"epoch\" # steps\n",
    "save_steps = 500 # if save_strategy = \"steps\"\n",
    "save_total_limit = 1 # None\n",
    "\n",
    "# no cuda, seed\n",
    "no_cuda = False\n",
    "seed = 42\n",
    "\n",
    "# bar\n",
    "disable_tqdm = False # True\n",
    "remove_unused_columns = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder for training outputs\n",
    "\n",
    "outputs = model_checkpoint.replace('/','-') + '_' + dataset_name + '/' + str(task) + '/'\n",
    "outputs = outputs \\\n",
    "+ 'lr' + str(learning_rate) \\\n",
    "+ '_bs' + str(batch_size) \\\n",
    "+ '_GAS' + str(gradient_accumulation_steps) \\\n",
    "+ '_eps' + str(adam_epsilon) \\\n",
    "+ '_epochs' + str(num_train_epochs) \\\n",
    "+ '_patience' + str(early_stopping_patience) \\\n",
    "+ '_madx2' + str(madx2) \\\n",
    "+ '_houlsby_MHA_lastlayer' + str(houlsby_MHA_lastlayer) \\\n",
    "+ '_ds' + str(ds) \\\n",
    "+ '_fp16' + str(fp16) \\\n",
    "+ '_best' + str(load_best_model_at_end) \\\n",
    "+ '_metric' + str(metric_for_best_model) \\\n",
    "+ '_adapterconfig' + str(adapter_config_name)\n",
    "\n",
    "# path to outputs\n",
    "path_to_outputs = root/'outputs'/outputs\n",
    "\n",
    "# subfolder for model outputs\n",
    "output_dir = path_to_outputs/'output_dir' \n",
    "overwrite_output_dir = True # False\n",
    "\n",
    "# logs\n",
    "logging_dir = path_to_outputs/'logging_dir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r_n9OWV3l-Q"
   },
   "source": [
    "## 6. Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dataset_name == \"squad11pt\":\n",
    "    \n",
    "#     # create dataset folder \n",
    "#     path_to_dataset = root/'data'/dataset_name\n",
    "#     path_to_dataset.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "#     # Get dataset SQUAD in Portuguese\n",
    "#     %cd {path_to_dataset}\n",
    "#     !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn\" -O squad-pt.tar.gz && rm -rf /tmp/cookies.txt\n",
    "\n",
    "#     # unzip \n",
    "#     !tar -xvf squad-pt.tar.gz\n",
    "\n",
    "#     # Get the train and validation json file in the HF script format \n",
    "#     # inspiration: file squad.py at https://github.com/huggingface/datasets/tree/master/datasets/squad\n",
    "    \n",
    "#     import json \n",
    "#     files = ['squad-train-v1.1.json','squad-dev-v1.1.json']\n",
    "\n",
    "#     for file in files:\n",
    "\n",
    "#         # Opening JSON file & returns JSON object as a dictionary \n",
    "#         f = open(file, encoding=\"utf-8\") \n",
    "#         data = json.load(f) \n",
    "\n",
    "#         # Iterating through the json list \n",
    "#         context_list = list()\n",
    "#         id_list = list()\n",
    "\n",
    "#         for row in data['data']: \n",
    "\n",
    "#             for paragraph in row['paragraphs']:\n",
    "#                 context = (paragraph['context']).strip()\n",
    "#                 context_list.append(context)\n",
    "\n",
    "#         # Get unique context\n",
    "#         unique_context_list = list(set(context_list))\n",
    "\n",
    "#         # Closing file \n",
    "#         f.close() \n",
    "\n",
    "#         file_name = 'pt_' + str(file).replace('json','txt')\n",
    "#         with open(file_name, 'wb') as list_file:\n",
    "#             pickle.dump(unique_context_list, list_file)\n",
    "         \n",
    "#     %cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1-9jepM3l-W"
   },
   "source": [
    "You can replace the dataset above with any dataset hosted on [the hub](https://huggingface.co/datasets) or use your own files. Just uncomment the following cell and replace the paths with values that will lead to your files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY1SwIrY3l-a"
   },
   "source": [
    "You can also load datasets from a csv or a JSON file, see the [full documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dataset_name == \"squad11pt\":\n",
    "    \n",
    "#     path_to_data = root/'data'/dataset_name\n",
    "#     files = ['pt_squad-train-v1.1.txt','pt_squad-dev-v1.1.txt']\n",
    "    \n",
    "#     for i,file in enumerate(files):\n",
    "#         path_to_file = path_to_data/file\n",
    "#         with open(path_to_file, \"rb\") as f:   # Unpickling\n",
    "#             text_list = pickle.load(f)\n",
    "\n",
    "#             with open(file, \"w\") as output:\n",
    "#                 output.write(str(text_list))\n",
    "        \n",
    "#         df = pd.DataFrame(text_list,columns=['text'])\n",
    "#         if i == 0:\n",
    "#             df_train = df.copy()\n",
    "#         else:\n",
    "#             df_validation = df.copy()\n",
    "            \n",
    "#     from datasets import Dataset, DatasetDict\n",
    "#     dataset_train = Dataset.from_pandas(df_train)\n",
    "#     dataset_validation = Dataset.from_pandas(df_validation)\n",
    "\n",
    "#     datasets = DatasetDict()\n",
    "#     datasets['train'] = dataset_train\n",
    "#     datasets['validation'] = dataset_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to save datasets in order to load it for each new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "# path_to_datasets = root/'data'/dataset_name\n",
    "# datasets.save_to_disk(path_to_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36815\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5644\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load\n",
    "path_to_datasets = root/'data'/dataset_name\n",
    "datasets = datasets.load_from_disk(str(path_to_datasets))\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'O pante√≠smo sustenta que Deus √© o universo e o universo √© Deus, enquanto o panente√≠smo sustenta que Deus cont√©m, mas n√£o √© id√™ntico ao universo. √â tamb√©m a vis√£o da Igreja Cat√≥lica Liberal; Teosofia; algumas vis√µes do hindu√≠smo, exceto o vaisnavismo, que acredita no panente√≠smo; Sikhismo; algumas divis√µes do neopaganismo e tao√≠smo, juntamente com muitas denomina√ß√µes e indiv√≠duos variados dentro das denomina√ß√µes. A Cabala, Misticismo judaico, pinta uma vis√£o pante√≠sta / panente√≠sta de Deus - que tem ampla aceita√ß√£o no juda√≠smo hass√≠dico, particularmente de seu fundador The Baal Shem Tov - mas apenas como um complemento √† vis√£o judaica de um deus pessoal, n√£o no pante√≠sta original sensa√ß√£o que nega ou limita a persona a Deus. [cita√ß√£o necess√°rio]'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ur5sNUcZ3l-g"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1Uk8NROQ3l-k",
    "outputId": "a822dcec-51e3-4dba-c73c-dba9e0301726"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>De acordo com as regras da FIG, apenas as mulheres competem na gin√°stica r√≠tmica. √â um esporte que combina elementos de manipula√ß√£o de bal√©, gin√°stica, dan√ßa e aparelhos. O esporte envolve a realiza√ß√£o de cinco rotinas separadas com o uso de cinco aparelhos; bola, fita, aro, tacos, corda - na √°rea do piso, com uma √™nfase muito maior na est√©tica do que na acrob√°tica. Existem tamb√©m rotinas de grupo que consistem em 5 ginastas e 5 aparelhos de sua escolha. Rotinas r√≠tmicas s√£o pontuadas em 30 pontos poss√≠veis; a pontua√ß√£o da arte (coreografia e m√∫sica) √© calculada com a pontua√ß√£o da dificuldade dos movimentos e depois adicionada √† pontua√ß√£o da execu√ß√£o.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tens√µes logo se desenvolveram entre diferentes fac√ß√µes gregas, levando a duas guerras civis consecutivas. Enquanto isso, o sult√£o otomano negociou com Mehmet Ali, do Egito, que concordou em enviar seu filho Ibrahim Pasha para a Gr√©cia com um ex√©rcito para reprimir a revolta em troca de ganhos territoriais. Ibrahim desembarcou no Peloponeso em fevereiro de 1825 e teve sucesso imediato: no final de 1825, a maior parte do Peloponeso estava sob controle eg√≠pcio, e a cidade de Missolonghi - sitiada pelos turcos desde abril de 1825 - caiu em abril de 1826. Embora Ibrahim foi derrotado em Mani, ele conseguiu reprimir a maior parte da revolta no Peloponeso e Atenas foi retomada.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Para os machos, o sistema reprodutivo √© o test√≠culo, suspenso na cavidade do corpo pelas traqu√©ias e pelo corpo gordo. A maioria dos insetos masculinos tem um par de test√≠culos, dentro dos quais existem tubos de esperma ou fol√≠culos que est√£o dentro de um saco membranoso. Os fol√≠culos se conectam ao ducto deferente pelo ducto efferens, e os dois vasos tubulares deferentes se conectam a um ducto ejaculat√≥rio mediano que leva ao exterior. Uma por√ß√£o do ducto deferente √© frequentemente aumentada para formar a ves√≠cula seminal, que armazena o esperma antes de serem descarregados na f√™mea. As ves√≠culas seminais t√™m revestimentos glandulares que secretam nutrientes para nutri√ß√£o e manuten√ß√£o do esperma. O ducto ejaculat√≥rio √© derivado de uma invagina√ß√£o das c√©lulas epid√©rmicas durante o desenvolvimento e, como resultado, possui um revestimento cuticular. A por√ß√£o terminal do ducto ejaculat√≥rio pode ser esclerotizada para formar o √≥rg√£o intromitente, o edema. O restante do sistema reprodutor masculino √© derivado do mesoderma embrion√°rio, exceto as c√©lulas germinativas, ou espermatog√¥nias, que descem das c√©lulas primordiais do p√≥lo muito cedo durante a embriog√™nese.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Papa Paulo VI se tornou o primeiro pont√≠fice reinante a visitar as Am√©ricas quando voou para Nova York em outubro de 1965 para se dirigir √†s Na√ß√µes Unidas. Como gesto de boa vontade, o papa deu √† ONU duas pe√ßas de joalharia papal, uma cruz e um anel de diamantes, na esperan√ßa de que o produto da venda em leil√£o contribua para os esfor√ßos da ONU para acabar com o sofrimento humano. Durante a visita do papa, √† medida que o envolvimento dos EUA na Guerra do Vietn√£ se intensificou sob o presidente Johnson, Paulo VI implorou por paz perante a ONU:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O termo \"In√≠cio da era moderna\" foi introduzido no idioma ingl√™s na d√©cada de 1930. distinguir o tempo entre o que chamamos Idade M√©dia e o tempo do Iluminismo tardio (1800) (quando o significado do termo Idade Moderna estava desenvolvendo sua forma contempor√¢nea). √â importante notar que esses termos derivam da hist√≥ria da Europa. Em uso em outras partes do mundo, como na √Åsia e em pa√≠ses mu√ßulmanos, os termos s√£o aplicados de uma maneira muito diferente, mas geralmente no contexto do contato com a cultura europ√©ia na Era dos Descobrimentos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A Galiza tem uma √°rea de 29.574 quil√¥metros quadrados (11.419 milhas quadradas). Seu ponto mais ao norte, a 43 ¬∞ 47 ‚Ä≤ N, √© a Estaca de Bares (tamb√©m o ponto mais ao norte da Espanha); o extremo sul, a 41 ¬∞ 49‚Ä≤N, fica na fronteira portuguesa no Parque Natural da Baixa Limia - Serra do Xur√©s. A longitude mais oriental fica a 6 ¬∞ 42‚Ä≤W na fronteira entre a prov√≠ncia de Ourense e a prov√≠ncia castelhana e leonesa de Zamora) e a oeste a 9 ¬∞ 18‚Ä≤W, alcan√ßada em dois lugares: o cabo A Nave em Fisterra (tamb√©m conhecido Finisterra) e Cabo Touri√±√°n, ambos na prov√≠ncia da Corunha.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sob uma bandeira de \"reduzir a embriaguez p√∫blica\", a Lei da Cerveja de 1830 introduziu um novo n√≠vel inferior de instala√ß√µes autorizadas a vender √°lcool, as Cervejarias. Na √©poca, a cerveja era vista como inofensiva, nutritiva e at√© saud√°vel. As crian√ßas pequenas recebiam frequentemente o que era descrito como cerveja pequena, fabricada com baixo teor alco√≥lico, pois a √°gua local geralmente era insegura. At√© a igreja evang√©lica e os movimentos de temperan√ßa do dia viam o consumo de cerveja muito como um mal secund√°rio e um acompanhamento normal para uma refei√ß√£o. A cerveja dispon√≠vel gratuitamente pretendia, assim, afastar os bebedores dos males do gin, ou era o que pensava.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Quando um circuito indutivo √© aberto, a corrente atrav√©s da indut√¢ncia entra em colapso rapidamente, criando uma grande tens√£o no circuito aberto do comutador ou rel√©. Se a indut√¢ncia for grande o suficiente, a energia gerar√° uma fa√≠sca, fazendo com que os pontos de contato se oxidem, se deteriorem ou, √†s vezes, se fundam ou destruam uma chave de estado s√≥lido. Um capacitor de amortecedor no circuito rec√©m-aberto cria um caminho para esse impulso desviar dos pontos de contato, preservando sua vida √∫til; estes foram comumente encontrados em sistemas de igni√ß√£o por disjuntor, por exemplo. Da mesma forma, em circuitos de menor escala, a fa√≠sca pode n√£o ser suficiente para danificar o comutador, mas ainda irradia interfer√™ncia de radiofrequ√™ncia indesej√°vel (RFI), absorvida por um capacitor de filtro. Os capacitores amortecedores s√£o geralmente empregados com um resistor de baixo valor em s√©rie, para dissipar energia e minimizar o RFI. Essas combina√ß√µes resistor-capacitor est√£o dispon√≠veis em um √∫nico pacote.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Diferentes campos da ci√™ncia usam o termo mat√©ria de maneiras diferentes e, √†s vezes, incompat√≠veis. Algumas dessas maneiras s√£o baseadas em significados hist√≥ricos frouxos, de uma √©poca em que n√£o havia raz√£o para distinguir massa e mat√©ria. Como tal, n√£o existe um significado cient√≠fico universalmente aceito da palavra \"mat√©ria\". Cientificamente, o termo \"massa\" √© bem definido, mas \"mat√©ria\" n√£o √©. √Äs vezes, no campo da f√≠sica, \"mat√©ria\" √© simplesmente equiparada a part√≠culas que exibem massa em repouso (isto √©, que n√£o podem viajar na velocidade da luz), como quarks e leptons. No entanto, tanto na f√≠sica quanto na qu√≠mica, a mat√©ria exibe propriedades semelhantes a ondas e part√≠culas, a chamada dualidade onda-part√≠cula.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alguns criticaram a decis√£o de Paulo VI; o rec√©m-criado S√≠nodo dos Bispos teve apenas um papel consultivo e n√£o p√¥de tomar decis√µes por conta pr√≥pria, embora o Conselho tenha decidido exatamente isso. Durante o pontificado de Paulo VI, cinco desses s√≠nodos ocorreram, e ele registra a implementa√ß√£o de todas as suas decis√µes. Quest√µes relacionadas foram levantadas sobre as novas Confer√™ncias Nacionais de Bispos, que se tornaram obrigat√≥rias ap√≥s o Vaticano II. Outros questionaram seu Ostpolitik e contatos com O comunismo e os acordos que ele realizou para os fi√©is.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKerdF353l-o"
   },
   "source": [
    "As we can see, some of the texts are a full paragraph of a Wikipedia article while others are just titles or empty lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-EIELH43l_T"
   },
   "source": [
    "## 7. Masked language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWk97-Ny3l_T"
   },
   "source": [
    "For masked language modeling (MLM) we are going to use the same preprocessing as before for our dataset with one additional step: we will randomly mask some tokens (by replacing them by `[MASK]`) and the labels will be adjusted to only include the masked tokens (we don't have to predict the non-masked tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iAYlS40Z3l-v"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpOiBrJ13l-y"
   },
   "source": [
    "We can now call the tokenizer on all our texts. This is very simple, using the [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method from the Datasets library. First we define a function that call the tokenizer on our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lS2m25YM3l-z"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12F1ulgT3l_V"
   },
   "source": [
    "We can apply the same tokenization function as before, we just need to update our tokenizer to use the checkpoint we just picked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "DVHs5aCA3l-_"
   },
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpNfGiMw3l_A"
   },
   "source": [
    "Then we write the preprocessing function that will group our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "iaAJy5Hu3l_B"
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGJWXtNv3l_C"
   },
   "source": [
    "First note that we duplicate the inputs for our labels. This is because the model of the ü§ó Transformers library apply the shifting to the right, so we don't need to do it manually.\n",
    "\n",
    "Also note that by default, the `map` method will send a batch of 1,000 examples to be treated by the preprocessing function. So here, we will drop the remainder to make the concatenated tokenized texts a multiple of `block_size` every 1,000 examples. You can adjust this behavior by passing a higher batch size (which will also be processed slower). You can also speed-up the preprocessing by using multiprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTuy8UUs3l_X"
   },
   "source": [
    "And like before, we group texts together and chunk them in samples of length `block_size`. You can skip that step if your dataset is composed of individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFJ49iHJ3l_Z"
   },
   "source": [
    "The rest is very similar to what we had, with two exceptions. First we use a model suitable for masked LM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PM10A9Za3l_Z",
    "outputId": "fff2d5bb-397d-4d5d-9aa9-933090cb6680"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108954466"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of model parameters\n",
    "model_num_param=0\n",
    "for p in model.parameters():\n",
    "    model_num_param+=p.numel()\n",
    "model_num_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lang adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup adapters\n",
    "if train_adapter:\n",
    "        \n",
    "    # new\n",
    "    if madx2:\n",
    "        # do not add adapter in the last transformer layers \n",
    "        leave_out = [len(model.bert.encoder.layer)-1]\n",
    "    else:\n",
    "        leave_out = []\n",
    "        \n",
    "    # new\n",
    "    # task_name = data_args.dataset_name or \"mlm\"\n",
    "    task_name = \"mlm\"\n",
    "        \n",
    "    # check if adapter already exists, otherwise add it\n",
    "    if task_name not in model.config.adapters:\n",
    "            \n",
    "#             # resolve the adapter config\n",
    "#             adapter_config = AdapterConfig.load(\n",
    "#                 adapter_args.adapter_config,\n",
    "#                 non_linearity=adapter_args.adapter_non_linearity,\n",
    "#                 reduction_factor=adapter_args.adapter_reduction_factor,\n",
    "#             )\n",
    "\n",
    "        # new\n",
    "        # resolve adapter config with (eventually) the MAD-X 2.0 option\n",
    "        if adapter_config_name == \"pfeiffer\":\n",
    "            from transformers.adapters.configuration import PfeifferConfig\n",
    "            adapter_config = PfeifferConfig(non_linearity=adapter_non_linearity,\n",
    "                                            reduction_factor=adapter_reduction_factor,\n",
    "                                            leave_out=leave_out)           \n",
    "        elif adapter_config_name == \"pfeiffer+inv\":\n",
    "            from transformers.adapters.configuration import PfeifferInvConfig\n",
    "            adapter_config = PfeifferInvConfig(non_linearity=adapter_non_linearity,\n",
    "                                               reduction_factor=adapter_reduction_factor,\n",
    "                                               leave_out=leave_out)          \n",
    "        elif adapter_config_name == \"houlsby\":\n",
    "            from transformers.adapters.configuration import HoulsbyConfig\n",
    "            adapter_config = HoulsbyConfig(non_linearity=adapter_non_linearity,\n",
    "                                           reduction_factor=adapter_reduction_factor,\n",
    "                                           leave_out=leave_out)\n",
    "        elif adapter_config_name == \"houlsby+inv\":\n",
    "            from transformers.adapters.configuration import HoulsbyInvConfig\n",
    "            adapter_config = HoulsbyInvConfig(non_linearity=adapter_non_linearity,\n",
    "                                              reduction_factor=adapter_reduction_factor,\n",
    "                                              leave_out=leave_out)              \n",
    "            \n",
    "        # load a pre-trained from Hub if specified\n",
    "        if load_adapter:\n",
    "            model.load_adapter(\n",
    "                    load_adapter,\n",
    "                    config=adapter_config,\n",
    "                    load_as=task_name,\n",
    "                    with_head = False\n",
    "                )\n",
    "        # otherwise, add a fresh adapter\n",
    "        else:\n",
    "            model.add_adapter(task_name, config=adapter_config)\n",
    "                \n",
    "    # optionally load another pre-trained language adapter\n",
    "    if load_lang_adapter:\n",
    "        # resolve the language adapter config\n",
    "        lang_adapter_config = AdapterConfig.load(\n",
    "                lang_adapter_config,\n",
    "                non_linearity=lang_adapter_non_linearity,\n",
    "                reduction_factor=lang_adapter_reduction_factor,\n",
    "                leave_out=leave_out,\n",
    "            )\n",
    "        # load the language adapter from Hub\n",
    "        lang_adapter_name = model.load_adapter(\n",
    "                load_lang_adapter,\n",
    "                config=lang_adapter_config,\n",
    "                load_as=language,\n",
    "                with_head = False\n",
    "            )\n",
    "    else:\n",
    "        lang_adapter_name = None\n",
    "    # Freeze all model weights except of those of this adapter\n",
    "    model.train_adapter([task_name])\n",
    "    # Set the adapters to be used in every forward pass\n",
    "    if lang_adapter_name:\n",
    "        model.set_active_adapters([lang_adapter_name, task_name])\n",
    "    else:\n",
    "        model.set_active_adapters([task_name])\n",
    "else:\n",
    "    if load_adapter or load_lang_adapter:\n",
    "        raise ValueError(\n",
    "                \"Adapters can only be loaded in adapters training mode.\"\n",
    "                \"Use --train_adapter to enable adapter training\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put only the adapter after the MHA but not after the FF in the last layer (Houlsby configuration)\n",
    "if houlsby_MHA_lastlayer \\\n",
    "and train_adapter \\\n",
    "and not madx2 \\\n",
    "and task_name in model.config.adapters \\\n",
    "and (adapter_config_name == \"houlsby\" or adapter_config_name == \"houlsby+inv\"):\n",
    "    from torch.nn import ModuleDict\n",
    "    model.bert.encoder.layer[len(model.bert.encoder.layer)-1].output.adapters = ModuleDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (invertible_adapters): ModuleDict(\n",
       "      (mlm): NICECouplingBlock(\n",
       "        (F): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (1): Activation_Function_Class()\n",
       "          (2): Linear(in_features=192, out_features=384, bias=True)\n",
       "        )\n",
       "        (G): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "          (1): Activation_Function_Class()\n",
       "          (2): Linear(in_features=192, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (mlm): Adapter(\n",
       "                (non_linearity): Activation_Function_Class()\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class()\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict()\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=29794, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115751266"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adapter_num_param=0\n",
    "for p in model.parameters():\n",
    "    model_adapter_num_param+=p.numel()\n",
    "model_adapter_num_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jElf8LJ33l_K"
   },
   "source": [
    "## 9. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "YbSwEhQ63l_L"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "if ds:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        do_train=do_train,\n",
    "        do_eval=do_eval,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        eval_accumulation_steps=eval_accumulation_steps,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        adam_beta1=adam_beta1,\n",
    "        adam_beta2=adam_beta2,\n",
    "        adam_epsilon=adam_epsilon,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        warmup_steps=warmup_steps,\n",
    "        logging_dir=logging_dir,         # directory for storing logs\n",
    "        logging_strategy=evaluation_strategy,\n",
    "        logging_steps=logging_steps,     # if strategy = \"steps\"\n",
    "        save_strategy=evaluation_strategy,          # model checkpoint saving strategy\n",
    "        save_steps=logging_steps,        # if strategy = \"steps\"\n",
    "        save_total_limit=save_total_limit,\n",
    "        fp16=fp16,\n",
    "        eval_steps=logging_steps,        # if strategy = \"steps\"\n",
    "        load_best_model_at_end=load_best_model_at_end,\n",
    "        metric_for_best_model=metric_for_best_model,\n",
    "        greater_is_better=greater_is_better,\n",
    "        disable_tqdm=disable_tqdm,\n",
    "        local_rank=gpu,\n",
    "        deepspeed=ds_config\n",
    "        )\n",
    "else:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        do_train=do_train,\n",
    "        do_eval=do_eval,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        eval_accumulation_steps=eval_accumulation_steps,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        adam_beta1=adam_beta1,\n",
    "        adam_beta2=adam_beta2,\n",
    "        adam_epsilon=adam_epsilon,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        warmup_steps=warmup_steps,\n",
    "        logging_dir=logging_dir,         # directory for storing logs\n",
    "        logging_strategy=evaluation_strategy,\n",
    "        logging_steps=logging_steps,     # if strategy = \"steps\"\n",
    "        save_strategy=evaluation_strategy,          # model checkpoint saving strategy\n",
    "        save_steps=logging_steps,        # if strategy = \"steps\"\n",
    "        save_total_limit=save_total_limit,\n",
    "        fp16=fp16,\n",
    "        eval_steps=logging_steps,        # if strategy = \"steps\"\n",
    "        load_best_model_at_end=load_best_model_at_end,\n",
    "        metric_for_best_model=metric_for_best_model,\n",
    "        greater_is_better=greater_is_better,\n",
    "        disable_tqdm=disable_tqdm,\n",
    "        local_rank=gpu,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6uuUnvz3l_b"
   },
   "source": [
    "And second, we use a special `data_collator`. The `data_collator` is a function that is responsible of taking the samples and batching them in tensors. In the previous example, we had nothing special to do, so we just used the default for this argument. Here we want to do the random-masking. We could do it as a pre-processing step (like the tokenization) but then the tokens would always be masked the same way at each epoch. By doing this step inside the `data_collator`, we ensure this random masking is done in a new way each time we go over the data.\n",
    "\n",
    "To do this masking for us, the library provides a `DataCollatorForLanguageModeling`. We can adjust the probability of the masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "nRZ-5v_P3l_b"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a compute metrics (accuracy). Even if it is always better to eveluate a model against a metric, we will not use it to evaluate the best model during the training as it can make a CUDA out of memory. Instead, we will use the validation loss (in the case of fine-tuning a MLM on  a new dataset, it is a common procedure). At the end of the training, we will use our compute metrics (accuracy) to get the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (loss and accuracy) before the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric accuracy\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    indices = [[i for i, x in enumerate(labels[row]) if x != -100] for row in range(len(labels))]\n",
    "\n",
    "    labels = [labels[row][indices[row]] for row in range(len(labels))]\n",
    "    labels = [item for sublist in labels for item in sublist]\n",
    "\n",
    "    predictions = [predictions[row][indices[row]] for row in range(len(predictions))]\n",
    "    predictions = [item for sublist in predictions for item in sublist]\n",
    "    \n",
    "    results = metric.compute(predictions=predictions, references=labels)\n",
    "    results[\"eval_accuracy\"] = results[\"accuracy\"]\n",
    "    results.pop(\"accuracy\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_accuracy: 0.567036068465056\n",
      "eval_loss: 2.532642419832549\n",
      "perplexity: 12.59\n",
      "CPU times: user 4min 43s, sys: 4min 29s, total: 9min 12s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_rows = lm_datasets[\"validation\"].num_rows\n",
    "num_rows_10pct = int(num_rows/10)\n",
    "\n",
    "eval_acc_sum = 0.\n",
    "eval_loss_sum = 0.\n",
    "\n",
    "# validation dataset evaluation\n",
    "for i in range(10):\n",
    "\n",
    "    # indices\"\n",
    "    start = i*num_rows_10pct\n",
    "    if i != 9: end = (i+1)*num_rows_10pct  \n",
    "    else: end = num_rows\n",
    "    indices = list(range(start,end))\n",
    "\n",
    "    # sub dataset eval\n",
    "    dset_eval = lm_datasets[\"validation\"].select(indices)\n",
    "\n",
    "    from transformers import Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"],#.shard(index=1, num_shards=90), #to be used to reduce train to 1/90\n",
    "    #     eval_dataset=lm_datasets[\"validation\"],\n",
    "        eval_dataset=dset_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        )    \n",
    "\n",
    "    # calculation of the performance on the validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "    eval_acc_sum += eval_results['eval_accuracy']*len(indices)\n",
    "    eval_loss_sum += eval_results['eval_loss']*len(indices)\n",
    "\n",
    "eval_acc_mean = eval_acc_sum / num_rows\n",
    "eval_loss_mean = eval_loss_sum / num_rows\n",
    "\n",
    "print('eval_accuracy:',eval_acc_mean)\n",
    "print('eval_loss:',eval_loss_mean)\n",
    "print(f\"perplexity: {math.exp(eval_loss_mean):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqHnWcYC3l_d"
   },
   "source": [
    "Then we just have to pass everything to `Trainer` and begin training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "V-Y3gNqV3l_d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "if metric_for_best_model == \"accuracy\":\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"], # .shard(index=1, num_shards=90), to be used to reduce train to 1/90\n",
    "        eval_dataset=lm_datasets[\"validation\"], #.shard(index=1, num_shards=90), to be used to reduce validation to 1/90\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        do_save_full_model=not train_adapter, \n",
    "        do_save_adapters=train_adapter,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)],\n",
    "        )    \n",
    "else:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"], # .shard(index=1, num_shards=90), to be used to reduce train to 1/90\n",
    "        eval_dataset=lm_datasets[\"validation\"], #.shard(index=1, num_shards=90), to be used to reduce validation to 1/90\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics,\n",
    "        do_save_full_model=not train_adapter, \n",
    "        do_save_adapters=train_adapter,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)],\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 49822\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 311400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93420' max='311400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 93420/311400 3:41:20 < 8:36:29, 7.03 it/s, Epoch 30/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.967700</td>\n",
       "      <td>1.883594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.881300</td>\n",
       "      <td>1.848203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.839000</td>\n",
       "      <td>1.826846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.810900</td>\n",
       "      <td>1.806322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.788400</td>\n",
       "      <td>1.806623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.764900</td>\n",
       "      <td>1.799932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.746100</td>\n",
       "      <td>1.808581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.737300</td>\n",
       "      <td>1.797586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.716000</td>\n",
       "      <td>1.801826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.702700</td>\n",
       "      <td>1.783277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.692600</td>\n",
       "      <td>1.790860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.681700</td>\n",
       "      <td>1.786727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.671100</td>\n",
       "      <td>1.780237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.656700</td>\n",
       "      <td>1.775374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.650200</td>\n",
       "      <td>1.775841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.639100</td>\n",
       "      <td>1.781693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.626400</td>\n",
       "      <td>1.782926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.615600</td>\n",
       "      <td>1.783294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.607900</td>\n",
       "      <td>1.768742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.604800</td>\n",
       "      <td>1.761280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.586900</td>\n",
       "      <td>1.775206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.584200</td>\n",
       "      <td>1.772919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.577100</td>\n",
       "      <td>1.776862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.567400</td>\n",
       "      <td>1.767370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.562800</td>\n",
       "      <td>1.768870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.553200</td>\n",
       "      <td>1.781912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.548300</td>\n",
       "      <td>1.774879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.544800</td>\n",
       "      <td>1.766587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.538100</td>\n",
       "      <td>1.781124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.531700</td>\n",
       "      <td>1.781325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=93420, training_loss=1.6664949842194525, metrics={'train_runtime': 13280.3879, 'train_samples_per_second': 375.155, 'train_steps_per_second': 23.448, 'total_flos': 1.3287075109404672e+17, 'train_loss': 1.6664949842194525, 'epoch': 30.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.args._n_gpu = n_gpu # train on one GPU but as we use local_rank in training_args, it is not needed\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dtype\n",
    "trainer.model.bert.embeddings.word_embeddings.weight.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the weights dtype is float16, use the script `zero_to_fp32.py` to get them in float32 as explained in [Getting The Model Weights Out](https://huggingface.co/transformers/main_classes/deepspeed.html?highlight=deepspeed#getting-the-model-weights-out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the model with adapter: 115751266\n",
      "Number of parameters of the model without adapter: 108954466\n",
      "Number of parameters of the adapter: 6796800\n",
      "Pourcentage of additional parameters through adapter: 6.24 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters of the model with adapter: {model_adapter_num_param:.0f}\")\n",
    "print(f\"Number of parameters of the model without adapter: {model_num_param:.0f}\")\n",
    "print(f\"Number of parameters of the adapter: {model_adapter_num_param - model_num_param:.0f}\")\n",
    "print(f\"Pourcentage of additional parameters through adapter:\",round(((model_adapter_num_param - model_num_param)/model_num_param)*100,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save adapter + head\n",
    "adapters_folder = 'adapters-' + task_name\n",
    "path_to_save_adapter = path_to_outputs/adapters_folder\n",
    "trainer.model.save_adapter(str(path_to_save_adapter), adapter_name=task_name, with_head=True)\n",
    "\n",
    "!ls -lh {path_to_save_adapter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDBi0reX3l_g"
   },
   "source": [
    "Now, you can push the saved adapter + head to the [AdapterHub](https://adapterhub.ml/) (follow instructions at [Contributing to Adapter Hub](https://docs.adapterhub.ml/contributing.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = os.getenv('PATH')\n",
    "# replace xxxx by your username on your server (ex: paulo)\n",
    "# replace yyyy by the name of the virtual environment of this notebook (ex: adapter-transformers)\n",
    "%env PATH=/mnt/home/xxxx/anaconda3/envs/yyyy/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6026;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "%tensorboard --logdir {logging_dir} --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation (loss and accuracy) after the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 796\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_accuracy: 0.6432477194214359\n",
      "eval_loss: 1.7709132447159501\n",
      "perplexity: 5.88\n",
      "CPU times: user 3min 58s, sys: 4min 43s, total: 8min 41s\n",
      "Wall time: 8min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_rows = lm_datasets[\"validation\"].num_rows\n",
    "num_rows_10pct = int(num_rows/10)\n",
    "\n",
    "eval_acc_sum = 0.\n",
    "eval_loss_sum = 0.\n",
    "\n",
    "# validation dataset evaluation\n",
    "for i in range(10):\n",
    "\n",
    "    # indices\"\n",
    "    start = i*num_rows_10pct\n",
    "    if i != 9: end = (i+1)*num_rows_10pct  \n",
    "    else: end = num_rows\n",
    "    indices = list(range(start,end))\n",
    "\n",
    "    # sub dataset eval\n",
    "    dset_eval = lm_datasets[\"validation\"].select(indices)\n",
    "\n",
    "    from transformers import Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_datasets[\"train\"],#.shard(index=1, num_shards=90), #to be used to reduce train to 1/90\n",
    "    #     eval_dataset=lm_datasets[\"validation\"],\n",
    "        eval_dataset=dset_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        )    \n",
    "\n",
    "    # calculation of the performance on the validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "    eval_acc_sum += eval_results['eval_accuracy']*len(indices)\n",
    "    eval_loss_sum += eval_results['eval_loss']*len(indices)\n",
    "\n",
    "eval_acc_mean = eval_acc_sum / num_rows\n",
    "eval_loss_mean = eval_loss_sum / num_rows\n",
    "\n",
    "print('eval_accuracy:',eval_acc_mean)\n",
    "print('eval_loss:',eval_loss_mean)\n",
    "print(f\"perplexity: {math.exp(eval_loss_mean):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Application MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true,
    "id": "Ckl2Fzn3is0F"
   },
   "outputs": [],
   "source": [
    "### import transformers\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"squad11pt\":\n",
    "    \n",
    "    # sentence from the training dataset\n",
    "    text_dataset = \"O pante√≠smo sustenta que Deus √© o universo e o universo √© Deus.\"\n",
    "    dataset_mask = \"Deus\"\n",
    "    text_dataset_mask = \"O pante√≠smo sustenta que [MASK] √© o universo e o universo √© Deus.\"\n",
    "    \n",
    "    # sentence from wikipedia\n",
    "    text_wiki = \"O primeiro caso da COVID-19 foi descoberto em Wuhan, na China.\"\n",
    "    wiki_mask = \"China\"\n",
    "    text_wiki_mask = \"O primeiro caso da COVID-19 foi descoberto em Wuhan, na [MASK].\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model original (without lang adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the model `neuralmind/bert-base-portuguese-cased` and its trainned lang adapter within the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "tokenizer_mlm = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"fill-mask\", model=model_mlm, tokenizer=tokenizer_mlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take one sentence from the SQuAD 1.1 pt dataset and replace the word `Deus` by the token `[MASK]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Deus) O pante√≠smo sustenta que [MASK] √© o universo e o universo √© Deus.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'O pante√≠smo sustenta que Deus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.7392684817314148,\n",
       "  'token': 2538,\n",
       "  'token_str': 'Deus'},\n",
       " {'sequence': 'O pante√≠smo sustenta que deus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.042948465794324875,\n",
       "  'token': 4023,\n",
       "  'token_str': 'deus'},\n",
       " {'sequence': 'O pante√≠smo sustenta que ele √© o universo e o universo √© Deus.',\n",
       "  'score': 0.029601380228996277,\n",
       "  'token': 368,\n",
       "  'token_str': 'ele'},\n",
       " {'sequence': 'O pante√≠smo sustenta que Cristo √© o universo e o universo √© Deus.',\n",
       "  'score': 0.021081821992993355,\n",
       "  'token': 4184,\n",
       "  'token_str': 'Cristo'},\n",
       " {'sequence': 'O pante√≠smo sustenta que tudo √© o universo e o universo √© Deus.',\n",
       "  'score': 0.018854131922125816,\n",
       "  'token': 2745,\n",
       "  'token_str': 'tudo'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'({dataset_mask}) {text_dataset_mask}')\n",
    "nlp(text_dataset_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test now the original model with another sentence and `China` has masked word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(China) O primeiro caso da COVID-19 foi descoberto em Wuhan, na [MASK].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na China.',\n",
       "  'score': 0.9124720096588135,\n",
       "  'token': 3278,\n",
       "  'token_str': 'China'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na √çndia.',\n",
       "  'score': 0.034306950867176056,\n",
       "  'token': 4340,\n",
       "  'token_str': '√çndia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Mal√°sia.',\n",
       "  'score': 0.023240933194756508,\n",
       "  'token': 17753,\n",
       "  'token_str': 'Mal√°sia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Tail√¢ndia.',\n",
       "  'score': 0.013218147680163383,\n",
       "  'token': 15582,\n",
       "  'token_str': 'Tail√¢ndia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Inglaterra.',\n",
       "  'score': 0.0027242223732173443,\n",
       "  'token': 2785,\n",
       "  'token_str': 'Inglaterra'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'({wiki_mask}) {text_wiki_mask}')\n",
    "nlp(text_wiki_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with lang adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_checkpoint.replace('/','-') + '_' + dataset_name + '/' + str(task) + '/'\n",
    "outputs = outputs \\\n",
    "+ 'lr' + str(learning_rate) \\\n",
    "+ '_bs' + str(batch_size) \\\n",
    "+ '_GAS' + str(gradient_accumulation_steps) \\\n",
    "+ '_eps' + str(adam_epsilon) \\\n",
    "+ '_epochs' + str(num_train_epochs) \\\n",
    "+ '_patience' + str(early_stopping_patience) \\\n",
    "+ '_madx2' + str(madx2) \\\n",
    "+ '_houlsby_MHA_lastlayer' + str(houlsby_MHA_lastlayer) \\\n",
    "+ '_ds' + str(ds) \\\n",
    "+ '_fp16' + str(fp16) \\\n",
    "+ '_best' + str(load_best_model_at_end) \\\n",
    "+ '_metric' + str(metric_for_best_model) \\\n",
    "+ '_adapterconfig' + str(adapter_config_name)\n",
    "\n",
    "path_to_outputs = root/'outputs'/outputs\n",
    "\n",
    "# Config of the lang adapter\n",
    "lang_adapter_path = path_to_outputs/'adapters-mlm/'\n",
    "\n",
    "load_lang_adapter = lang_adapter_path\n",
    "lang_adapter_config = str(lang_adapter_path) + \"/adapter_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the language adapter\n",
    "task_mlm_load_as = 'mlm'\n",
    "lang_adapter_name = model_mlm.load_adapter(\n",
    "    str(load_lang_adapter),\n",
    "    config=lang_adapter_config,\n",
    "    load_as=task_mlm_load_as,\n",
    "    with_head=True\n",
    "    )\n",
    "\n",
    "# Set the adapters to be used in every forward pass\n",
    "model_mlm.set_active_adapters([lang_adapter_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"fill-mask\", model=model_mlm, tokenizer=tokenizer_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Deus) O pante√≠smo sustenta que [MASK] √© o universo e o universo √© Deus.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'O pante√≠smo sustenta que Deus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.865125834941864,\n",
       "  'token': 2538,\n",
       "  'token_str': 'Deus'},\n",
       " {'sequence': 'O pante√≠smo sustenta que Cristo √© o universo e o universo √© Deus.',\n",
       "  'score': 0.03503378853201866,\n",
       "  'token': 4184,\n",
       "  'token_str': 'Cristo'},\n",
       " {'sequence': 'O pante√≠smo sustenta que Jesus √© o universo e o universo √© Deus.',\n",
       "  'score': 0.016306772828102112,\n",
       "  'token': 3125,\n",
       "  'token_str': 'Jesus'},\n",
       " {'sequence': 'O pante√≠smo sustenta que tudo √© o universo e o universo √© Deus.',\n",
       "  'score': 0.011215949431061745,\n",
       "  'token': 2745,\n",
       "  'token_str': 'tudo'},\n",
       " {'sequence': 'O pante√≠smo sustenta que ele √© o universo e o universo √© Deus.',\n",
       "  'score': 0.007768187206238508,\n",
       "  'token': 368,\n",
       "  'token_str': 'ele'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'({dataset_mask}) {text_dataset_mask}')\n",
    "nlp(text_dataset_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fine-tuned model scored better (0.865 vs. 0.739) when finding the masked word `Deus`. It seems that our finetuning on the SQuAD 1.1 pt dataset with lang adapter worked as it performs better than the original model on a sentence from its training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test now our fine-tuned model with another sentence and `China` has masked word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(China) O primeiro caso da COVID-19 foi descoberto em Wuhan, na [MASK].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na China.',\n",
       "  'score': 0.7749341726303101,\n",
       "  'token': 3278,\n",
       "  'token_str': 'China'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na √çndia.',\n",
       "  'score': 0.14467883110046387,\n",
       "  'token': 4340,\n",
       "  'token_str': '√çndia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Mal√°sia.',\n",
       "  'score': 0.028336668387055397,\n",
       "  'token': 17753,\n",
       "  'token_str': 'Mal√°sia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Tail√¢ndia.',\n",
       "  'score': 0.01335228607058525,\n",
       "  'token': 15582,\n",
       "  'token_str': 'Tail√¢ndia'},\n",
       " {'sequence': 'O primeiro caso da COVID - 19 foi descoberto em Wuhan, na Indon√©sia.',\n",
       "  'score': 0.007589380722492933,\n",
       "  'token': 13985,\n",
       "  'token_str': 'Indon√©sia'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'({wiki_mask}) {text_wiki_mask}')\n",
    "nlp(text_wiki_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The masked word `China` was found with a high score of 0.775 but lower than the score of the orginal model (0.912). It was expected: by finetuning the original model, we specialized it to the \"language\" of the dataset used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Language Modeling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adapter-transformers",
   "language": "python",
   "name": "adapter-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
