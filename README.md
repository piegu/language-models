# Language Models

<p>
<a href="https://console.tiyaro.ai/explore/pierreguillou-bert-large-cased-squad-v1.1-portuguese"> <img src="https://tiyaro-public-docs.s3.us-west-2.amazonaws.com/assets/try_on_tiyaro_badge.svg"></a>
</p>
Repository of pre-trained Language Models and NLP models.

## HF-LLM.rs ðŸ¦€
[HF-LLM.rs ðŸ¦€](https://github.com/piegu/language-models-hf-llm.rs?tab=readme-ov-file#hf-llmrs-) is a CLI tool for accessing Large Language Models (LLMs) like Llama 3.1, Mistral, Gemma 2, Cohere and much more hosted on Hugging Face. It allows you to interact with various models, provide input, and receive responses in a terminal environment. (credit: [Vaibhav Srivastav](https://github.com/Vaibhavs10))

## unstructured library | Get the JSON and HTML versions of any PDF (legal, financial, medicalâ€¦), even PDF with tables!
- Blog post: [unstructured library | Get the JSON and HTML versions of any PDF (legal, financial, medicalâ€¦), even PDF with tables!](https://medium.com/@pierre_guillou/unstructured-library-get-the-json-and-html-versions-of-any-pdf-legal-financial-medical-fb7738769caa)
- Notebook: [Unstructured_PDF_to_JSON_and_HTML.ipynb](https://github.com/piegu/language-models/blob/master/Unstructured_PDF_to_JSON_and_HTML.ipynb)

## Speech-to-Text | Get transcription WITH SPEAKERS from large audio file in any language (OpenAI Whisper + NeMo Speaker Diarization)
- Blog post: [Speech-to-Text | Get transcription WITH SPEAKERS from large audio file in any language (OpenAI Whisper + NeMo Speaker Diarization)](https://medium.com/@pierre_guillou/speech-to-text-get-transcription-with-speakers-from-large-audio-file-in-any-language-openai-8da2312f1617)
- Notebook: [speech_to_text_transcription_with_speakers_Whisper_Transcription_+_NeMo_Diarization.ipynb](https://github.com/piegu/language-models/blob/master/speech_to_text_transcription_with_speakers_Whisper_Transcription_%2B_NeMo_Diarization.ipynb)

## Video-to-Audio | A notebook and Web APP to get mp3 audio file from any YouTube video
- Blog post: [Video-to-Audio | A notebook and Web APP to get mp3 audio file from any YouTube video](https://medium.com/@pierre_guillou/video-to-audio-a-notebook-and-web-app-to-get-mp3-audio-file-from-any-youtube-video-bb6a1c85390d)
- Notebook: [youtube_video_to_audio.ipynb](https://github.com/piegu/language-models/blob/master/youtube_video_to_audio.ipynb)
- Web APP: [Free YouTube URL Video-to-Audio](https://huggingface.co/spaces/pierreguillou/video-to-audio)

## Speech-to-Text | Quickly get a transcription of a large audio file in any language with "Faster-Whisper"
- Blog post: [Speech-to-Text | Quickly get a transcription of a large audio file in any language with "Faster-Whisper"](https://medium.com/@pierre_guillou/speech-to-text-quickly-get-a-transcription-of-a-large-audio-file-in-any-language-with-e4d4d2daf0cd)
- Notebook: [Speech_to_Text_with_faster_whisper_on_large_audio_file_in_any_language.ipynb](https://github.com/piegu/language-models/blob/master/Speech_to_Text_with_faster_whisper_on_large_audio_file_in_any_language.ipynb)

## Curso | ChatGPT Prompt Engineering for Developers
- Blog post: [IA Generativa | Como controlar o ChatGPT para escrever um texto que atenda Ã s suas expectativas (curso de DeepLearning.AI e OpenAI)](https://medium.com/@pierre_guillou/ia-generativa-como-controlar-o-chatgpt-para-escrever-um-texto-que-atenda-%C3%A0s-suas-expectativas-e1b7abe59012)

## Document AI | Accuracy of layout finetuned models (LiLT and LayoutXLM base) on the dataset DoclayNet base (notebooks)
- Line level
  -  [Document AI | ACCURACY at line level with a Document Understanding layout model (LiLT base) fine-tuned on DocLayNet dataset](https://github.com/piegu/language-models/blob/master/ACCURACY_of_LiLT_base_model_finetuned_on_DocLayNet_base_in_any_language_at_levelline_ml384.ipynb)

- Paragraph level
  - [Document AI | ACCURACY at paragraph level with a Document Understanding layout model (LiLT base) fine-tuned on DocLayNet dataset](https://github.com/piegu/language-models/blob/master/ACCURACY_of_LiLT_base_model_finetuned_on_DocLayNet_base_in_any_language_at_levelparagraphs_ml512.ipynb)
  - [Document AI | ACCURACY at paragraph level with a Document Understanding layout model (Layout XLM base) fine-tuned on DocLayNet dataset](https://github.com/piegu/language-models/blob/master/ACCURACY_of_LayoutXLM_base_model_finetuned_on_DocLayNet_base_in_any_language_at_levelparagraphs_ml512.ipynb)

## Document AI | Inference at paragraph level by using the association of 2 Document Understanding models (LiLT and LayoutXLM base fine-tuned on DocLayNet base dataset)
- Notebook: [Document AI | Inference at paragraph level by using the association of 2 Document Understanding models (LiLT and LayoutXLM base fine-tuned on DocLayNet base dataset)](https://github.com/piegu/language-models/blob/master/inference_on_Ensemble_LiLT_%26_LayoutXLM_base_model_finetuned_on_DocLayNet_base_in_any_language_at_levelparagraphs_ml512.ipynb)
- Notebook: [Document AI | Inference APP at paragraph level by using the association of 2 Document Understanding models (LiLT and LayoutXLM base fine-tuned on DocLayNet base dataset)](https://github.com/piegu/language-models/blob/master/Gradio_inference_on_Ensemble_LiLT_%26_LayoutXLM_base_model_finetuned_on_DocLayNet_base_in_any_language_at_levelparagraphs_ml512.ipynb)

## Document AI | APP to compare the Document Understanding LiLT and LayoutXLM (base) models at paragraphÂ level
- Blog Post: [Document AI | APP to compare the Document Understanding LiLT and LayoutXLM (base) models at paragraphÂ level](https://medium.com/@pierre_guillou/document-ai-app-to-compare-the-document-understanding-lilt-and-layoutxlm-base-models-at-c22bee832c3a)
- Notebook: [Document AI | Inference APP at paragraph level with 2 Document Understanding models (LiLT base and LayoutXLM base fine-tuned on DocLayNet base dataset)](https://github.com/piegu/language-models/blob/master/Gradio_inference_on_LiLT_&_LayoutXLM_base_model_finetuned_on_DocLayNet_base_in_any_language_at_levelparagraphs_ml512.ipynb)

## Document AI | Inference APP and fine-tuning notebook for Document Understanding at paragraph level with LayoutXLM base
- Blog Post: [Document AI | Inference APP and fine-tuning notebook for Document Understanding at paragraph level with LayoutXLM base](https://medium.com/@pierre_guillou/document-ai-inference-app-and-fine-tuning-notebook-for-document-understanding-at-paragraph-level-3507af80573d)
- Notebooks: 
  - [Document AI | Inference at paragraph level with a Document Understanding model (LayoutXLM base fine-tuned on DocLayNet dataset)](https://github.com/piegu/language-models/blob/master/inference_on_LayoutXLM_base_model_finetuned_on_DocLayNet_base_in_any_language_at_levelparagraphs_ml512.ipynb)
  - [Document AI | Inference APP at paragraph level with a Document Understanding model (LayoutXLM base fine-tuned on DocLayNet base dataset)](https://github.com/piegu/language-models/blob/master/Gradio_inference_on_LayoutXLM_base_model_finetuned_on_DocLayNet_base_in_any_language_at_levelparagraphs_ml512.ipynb)
  - [Document AI | Fine-tune LayoutXLM base on DocLayNet base in any language at paragraph level (chunk of 512 tokens with overlap)](https://github.com/piegu/language-models/blob/master/Fine_tune_LayoutXLM_base_on_DocLayNet_base_in_any_language_at_paragraphlevel_ml_512.ipynb)

## Document AI | APP to compare the Document Understanding LiLT and LayoutXLM (base) models at lineÂ level
- Blog Post: [Document AI | APP to compare the Document Understanding LiLT and LayoutXLM (base) models at lineÂ level](https://medium.com/@pierre_guillou/document-ai-app-to-compare-the-document-understanding-lilt-and-layoutxlm-base-models-at-line-1c53eb481a15)
- Notebook: [Document AI | Inference APP at line level with 2 Document Understanding models (LiLT base and LayoutXLM base fine-tuned on DocLayNet base dataset)](https://github.com/piegu/language-models/blob/master/Gradio_inference_on_LiLT_&_LayoutXLM_base_model_finetuned_on_DocLayNet_base_in_any_language_at_levellines_ml384.ipynb)

## Document AI | Inference APP and fine-tuning notebook for Document Understanding at line level with LayoutXLM base
- Blog Post: [Document AI | Inference APP and fine-tuning notebook for Document Understanding at line level with LayoutXLM base](https://medium.com/@pierre_guillou/document-ai-inference-app-and-fine-tuning-notebook-for-document-understanding-at-line-level-with-b08fdca5f4dc)
- Notebooks: 
  - [Document AI | Inference at line level with a Document Understanding model (LayoutXLM base fine-tuned on DocLayNet dataset)](https://github.com/piegu/language-models/blob/master/inference_on_LayoutXLM_base_model_finetuned_on_DocLayNet_base_in_any_language_at_levellines_ml384.ipynb)
  - [Document AI | Inference APP at line level with a Document Understanding model (LayoutXLM base fine-tuned on DocLayNet base dataset)](https://github.com/piegu/language-models/blob/master/Gradio_inference_on_LayoutXLM_base_model_finetuned_on_DocLayNet_base_in_any_language_at_levellines_ml384.ipynb)
  - [Document AI | Fine-tune LayoutXLM base on DocLayNet base in any language at line level (chunk of 384 tokens with overlap)](https://github.com/piegu/language-models/blob/master/Fine_tune_LayoutXLM_base_on_DocLayNet_base_in_any_language_at_linelevel_ml_384.ipynb)

## Document AI | Inference APP and fine-tuning notebook for Document Understanding at paragraph level
- Blog Post: [Document AI | Inference APP and fine-tuning notebook for Document Understanding at paragraph level](https://medium.com/@pierre_guillou/document-ai-inference-app-and-fine-tuning-notebook-for-document-understanding-at-paragraph-level-c18d16e53cf8)
- Notebook: [Document AI | Inference APP at paragraph level with a Document Understanding model (LiLT fine-tuned on DocLayNet dataset)](https://github.com/piegu/language-models/blob/master/Gradio_inference_on_LiLT_model_finetuned_on_DocLayNet_base_in_any_language_at_levelparagraphs_ml512.ipynb)
- Notebook: [Document AI | Inference at paragraph level with a Document Understanding model (LiLT fine-tuned on DocLayNet dataset)](https://github.com/piegu/language-models/blob/master/inference_on_LiLT_model_finetuned_on_DocLayNet_base_in_any_language_at_levelparagraphs_ml512.ipynb)
- Notebook: [Document AI | Fine-tune LiLT on DocLayNet base in any language at paragraph level (chunk of 512 tokens with overlap)](https://github.com/piegu/language-models/blob/master/Fine_tune_LiLT_on_DocLayNet_base_in_any_language_at_paragraphlevel_ml_512.ipynb)

## Document AI | Inference APP for Document Understanding at line level
- Blog Post: [Document AI | Inference APP for Document Understanding at line level](https://medium.com/@pierre_guillou/document-ai-inference-app-for-document-understanding-at-line-level-a35bbfa98893)
- Notebook: [Document AI | Inference APP at line level with a Document Understanding model (LiLT fine-tuned on DocLayNet dataset)](https://github.com/piegu/language-models/blob/master/Gradio_inference_on_LiLT_model_finetuned_on_DocLayNet_base_in_any_language_at_levellines_ml384.ipynb)

## Document AI | Document Understanding model at line level with LiLT, Tesseract and DocLayNet dataset
- Blog Post: [Document AI | Document Understanding model at line level with LiLT, Tesseract and DocLayNet dataset](https://medium.com/@pierre_guillou/document-ai-document-understanding-model-at-line-level-with-lilt-tesseract-and-doclaynet-dataset-347107a643b8)
- Notebook (update on 02/14/2023): [Document AI | Inference at line level with a Document Understanding model (LiLT fine-tuned on DocLayNet dataset)](https://github.com/piegu/language-models/blob/master/inference_on_LiLT_model_finetuned_on_DocLayNet_base_in_any_language_at_levellines_ml384.ipynb)
- Notebook: [Document AI | Fine-tune LiLT on DocLayNet base in any language at line level (chunk of 384 tokens with overlap)](https://github.com/piegu/language-models/blob/master/Fine_tune_LiLT_on_DocLayNet_base_in_any_language_at_linelevel_ml_384.ipynb)

## DocLayNet image viewer APP
- Blog post: [Document AI | DocLayNet image viewerÂ APP](https://medium.com/@pierre_guillou/document-ai-doclaynet-image-viewer-app-3ac54c19956)
- Notebook [DocLayNet image viewer APP](https://github.com/piegu/language-models/blob/master/DocLayNet_image_viewer_APP.ipynb)

## Document AI | Processing of DocLayNet dataset to be used by layout models of the Hugging Face hub (finetuning, inference)
- Blog post: [Document AI | Processing of DocLayNet dataset to be used by layout models of the Hugging Face hub (finetuning, inference)](https://medium.com/@pierre_guillou/document-ai-processing-of-doclaynet-dataset-to-be-used-by-layout-models-of-the-hugging-face-hub-308d8bd81cdb)
- Notebook [Processing of DocLayNet dataset to be used by layout models of the Hugging Face hub (finetuning, inference)](processing_DocLayNet_dataset_to_be_used_by_layout_models_of_HF_hub.ipynb)

## Speech-to-Text & IA | TranscriÃ§Ã£o de qualquer Ã¡udio em portuguÃªs comÂ Whisper
- Blog post: [Speech-to-Text & IA | Transcreva qualquer Ã¡udio para o portuguÃªs com o Whisper (OpenAI)... sem nenhum custo!](https://medium.com/@pierre_guillou/speech-to-text-ia-transcreva-qualquer-%C3%A1udio-para-o-portugu%C3%AAs-com-o-whisper-openai-sem-ad0c17384681)
- Notebook [Whisper em portuguÃªs](Whisper_Medium_Portuguese_GPU.ipynb)
- Notebook [Whisper en franÃ§ais](Whisper_Medium_French_GPU.ipynb)
- Notebook [Inference code for Whisper (example with Whisper Medium in Portuguese)](inference_code_whisper_example_with_Portuguese.ipynb)

## IA & empresas | Diminua o tempo de inferÃªncia de modelos Transformer com BetterTransformer
- Blog post: [IA & empresas | Diminua o tempo de inferÃªncia de modelos Transformer com BetterTransformer](https://medium.com/@pierre_guillou/ia-empresas-diminua-o-tempo-de-infer%C3%AAncia-de-modelos-transformer-com-bettertransformer-584a5a7702c8)
- Notebook [QA in Portuguese with BetterTransformer](https://github.com/piegu/language-models/blob/master/question_answering_portuguese_with_BetterTransformer.ipynb)

## NLP & CÃ³digo para todos | FunÃ§Ã£o de perda ponderada para classificaÃ§Ã£o de texto (multiclasse)
- Blog post: [NLP & CÃ³digo para todos | FunÃ§Ã£o de perda ponderada para classificaÃ§Ã£o de texto (multiclasse)](https://medium.com/@pierre_guillou/nlp-c%C3%B3digo-para-todos-fun%C3%A7%C3%A3o-de-perda-ponderada-para-classifica%C3%A7%C3%A3o-de-texto-multiclasse-6786ad64397c)
- Notebook [Text Classification on GLUE with weighted Loss function](https://github.com/piegu/language-models/blob/master/Text_Classification_on_GLUE_with_weighted_Loss.ipynb)

## NLP nas empresas | Como eu treinei um modelo T5 em portuguÃªs na tarefa QA no GoogleÂ Colab
- Blog post: [NLP nas empresas | Como eu treinei um modelo T5 em portuguÃªs na tarefa QA no GoogleÂ Colab](https://medium.com/@pierre_guillou/nlp-nas-empresas-como-eu-treinei-um-modelo-t5-em-portugu%C3%AAs-na-tarefa-qa-no-google-colab-e8eb0dc38894)
- Notebook: [Finetuning of the language model T5 base on a Question-Answering task (QA) with the dataset SQuAD 1.1 Portuguese](https://github.com/piegu/language-models/blob/master/HuggingFace_Notebook_t5_base_portuguese_vocab_question_answering_QA_squad_v11_pt.ipynb)
- [QA App](https://huggingface.co/spaces/pierreguillou/question-answering-portuguese-t5-base) in the Hugging Face Spaces

## NLP | Modelos e Web App para Reconhecimento de Entidade Nomeada (NER) no domÃ­nio jurÃ­dico brasileiro
- Blog post: [NLP | Modelos e Web App para Reconhecimento de Entidade Nomeada (NER) no domÃ­nio jurÃ­dico brasileiro](https://medium.com/@pierre_guillou/nlp-modelos-e-web-app-para-reconhecimento-de-entidade-nomeada-ner-no-dom%C3%ADnio-jur%C3%ADdico-b658db55edfb)
- [NER App](https://huggingface.co/spaces/pierreguillou/ner-bert-pt-lenerbr) in the Hugging Face Spaces

### Finetuning of the specialized version of the language model BERTimbau on a token classification task (NER) with the dataset LeNER-Br
- notebook: [HuggingFace_Notebook_token_classification_NER_LeNER_Br.ipynb](https://github.com/piegu/language-models/blob/master/HuggingFace_Notebook_token_classification_NER_LeNER_Br.ipynb) ([nbviewer of the notebook](https://nbviewer.org/github/piegu/language-models/blob/master/HuggingFace_Notebook_token_classification_NER_LeNER_Br.ipynb))
- [BERT base NER model in the legal domain in Portuguese (LeNER-Br)](https://huggingface.co/pierreguillou/ner-bert-base-cased-pt-lenerbr) in the Hugging Face model hub
- [BERT large NER model in the legal domain in Portuguese (LeNER-Br)](https://huggingface.co/pierreguillou/ner-bert-large-cased-pt-lenerbr) in the Hugging Face model hub

### Finetuning of the language model BERTimbau on LeNER-Br text files
- notebook: [Finetuning_language_model_BERtimbau_LeNER_Br.ipynb](https://github.com/piegu/language-models/blob/master/Finetuning_language_model_BERtimbau_LeNER_Br.ipynb) ([nbviewer of the notebook](https://nbviewer.org/github/piegu/language-models/blob/master/Finetuning_language_model_BERtimbau_LeNER_Br.ipynb))
- dataset: [pierreguillou/lener_br_finetuning_language_model](https://huggingface.co/datasets/pierreguillou/lener_br_finetuning_language_model)
- [BERT base Language modeling in the legal domain in Portuguese (LeNER-Br)](https://huggingface.co/pierreguillou/bert-base-cased-pt-lenerbr) in the Hugging Face model hub
- [BERT large Language modeling in the legal domain in Portuguese (LeNER-Br)](https://huggingface.co/pierreguillou/bert-large-cased-pt-lenerbr) in the Hugging Face model hub

## NLP nas empresas | TÃ©cnicas para acelerar modelos de Deep Learning para inferÃªncia em produÃ§Ã£o

- Blog post: [NLP nas empresas | TÃ©cnicas para acelerar modelos de Deep Learning para inferÃªncia em produÃ§Ã£o](https://medium.com/@pierre_guillou/nlp-nas-empresas-t%C3%A9cnicas-para-acelerar-modelos-de-deep-learning-para-infer%C3%AAncia-em-produ%C3%A7%C3%A3o-884acbf49f20)
- notebooks:
  - [fast_inference_transformers_on_CPU.ipynb](https://github.com/piegu/language-models/blob/master/fast_inference_transformers_on_CPU.ipynb) ([nbviewer](https://nbviewer.org/github/piegu/language-models/blob/master/fast_inference_transformers_on_CPU.ipynb?flush_cache=true))
  - [fast_inference_transformers_on_GPU.ipynb](https://github.com/piegu/language-models/blob/master/fast_inference_transformers_on_GPU.ipynb) ([nbviewer](https://nbviewer.org/github/piegu/language-models/blob/master/fast_inference_transformers_on_GPU.ipynb?flush_cache=true))

## NLP nas empresas | Reconhecimento de textos com Deep Learning em PDFs e imagens

- notebook [OCR_DeepLearning_Tesseract_DocTR_colab.ipynb](https://github.com/piegu/language-models/blob/master/OCR_DeepLearning_Tesseract_DocTR_colab.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/OCR_DeepLearning_Tesseract_DocTR_colab.ipynb?flush_cache=true))
- Blog post: [NLP nas empresas | Reconhecimento de textos com Deep Learning em PDFs e imagens](https://medium.com/@pierre_guillou/nlp-nas-empresas-reconhecimento-de-textos-com-deep-learning-em-pdfs-e-imagens-14d8b9e8d513)

## NLP nas empresas | Como criar um modelo BERT de Question-Answering (QA) de desempenho aprimorado com AdapterFusion?

- notebook [question_answering_adapter_fusion.ipynb](https://github.com/piegu/language-models/blob/master/adapters/question-answering/question_answering_adapter_fusion.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/adapters/question-answering/question_answering_adapter_fusion.ipynb?flush_cache=true)): finetuning a MLM (Masked Language Model) like BERT (base or large) with the library adapter-transformers on the Question Answering task (QA) with AdapterFusion
- Blog post: [NLP nas empresas | Como criar um modelo BERT de Question-Answering (QA) de desempenho aprimorado com AdapterFusion?](https://medium.com/@pierre_guillou/nlp-nas-empresas-como-criar-um-modelo-bert-de-question-answering-qa-de-desempenho-aprimorado-21b699326c82)

## NLP nas empresas | Como ajustar um modelo de linguagem natural como BERT para a tarefa de Question-Answering (QA) com um Adapter?

- notebooks [question_answering_adapter.ipynb](https://github.com/piegu/language-models/blob/master/adapters/question-answering/question_answering_adapter.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/adapters/question-answering/question_answering_adapter.ipynb?flush_cache=true)) and [question_answering_adapter_script.ipynb](https://github.com/piegu/language-models/blob/master/adapters/question-answering/question_answering_adapter_script.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/adapters/question-answering/question_answering_adapter_script.ipynb?flush_cache=true)): finetuning a MLM (Masked Language Model) like BERT (base or large) with the library adapter-transformers on the Question Answering task (QA)
- Blog post: [NLP nas empresas | Como ajustar um modelo de linguagem natural como BERT para a tarefa de Question-Answering (QA) com um Adapter?](https://medium.com/@pierre_guillou/nlp-nas-empresas-como-ajustar-um-modelo-de-linguagem-natural-como-bert-para-a-tarefa-de-b7d2e7553a01)

## NLP nas empresas | Como ajustar um modelo de linguagem natural como BERT para a tarefa de classificaÃ§Ã£o de tokens (NER) com um Adapter?

- notebook [token_classification_adapter.ipynb](https://github.com/piegu/language-models/blob/master/adapters/token-classification/token_classification_adapter.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/adapters/token-classification/token_classification_adapter.ipynb)): finetuning a MLM (Masked Language Model) like BERT (base or large) with the library adapter-transformers on the Token Classification task (NER)
- Blog post: [NLP nas empresas | Como ajustar um modelo de linguagem natural como BERT para a tarefa de classificaÃ§Ã£o de tokens (NER) com um Adapter?](https://medium.com/@pierre_guillou/nlp-nas-empresas-como-ajustar-um-modelo-de-linguagem-natural-como-bert-para-a-tarefa-de-9c6a704bf536)

## NLP nas empresas | Como ajustar um modelo de linguagem natural como BERT a um novo domÃ­nio linguÃ­stico com um Adapter?

- notebook [language_modeling_adapter.ipynb](https://github.com/piegu/language-models/blob/master/adapters/language-modeling/language_modeling_adapter.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/adapters/language-modeling/language_modeling_adapter.ipynb)): finetuning a MLM (Masked Language Model) like BERT (base or large) with the library adapter-transformers
- Blog post: [NLP nas empresas | Como ajustar um modelo de linguagem natural como BERT a um novo domÃ­nio linguÃ­stico com um Adapter?](https://medium.com/@pierre_guillou/nlp-nas-empresas-como-ajustar-um-modelo-de-linguagem-natural-como-bert-a-um-novo-dom%C3%ADnio-23752b73b185)

## NLP | Modelo de Question Answering em qualquer idioma baseado no BERT large (estudo de caso em portuguÃªs)

- notebook [question_answering_BERT_large_cased_squad_v11_pt.ipynb](https://github.com/piegu/language-models/blob/master/question_answering_BERT_large_cased_squad_v11_pt.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/question_answering_BERT_large_cased_squad_v11_pt.ipynb)): training code of a Portuguese BERT large cased QA (Question Answering), finetuned on SQUAD v1.1
- Blog post: [NLP | Como treinar um modelo de Question Answering em qualquer linguagem baseado no BERT large, melhorando o desempenho do modelo utilizando o BERT base? (estudo de caso em portuguÃªs)](https://medium.com/@pierre_guillou/nlp-como-treinar-um-modelo-de-question-answering-em-qualquer-linguagem-baseado-no-bert-large-1c899262dd96)
- Model in the Model Hub of Hugging Face: [Portuguese BERT large cased QA (Question Answering), finetuned on SQUAD v1.1](https://huggingface.co/pierreguillou/bert-large-cased-squad-v1.1-portuguese)

## NLP | How to add a domain-specific vocabulary (new tokens) to a subword tokenizer already trained like BERT WordPiece
**Summary**: In some cases, it may be crucial to enrich the vocabulary of an already trained natural language model with vocabulary from a specialized domain (medicine, law, etc.) in order to perform new tasks (classification, NER, summary, translation, etc.). While the Hugging Face library allows you to easily add new tokens to the vocabulary of an existing tokenizer like BERT WordPiece, those tokens must be whole words, not subwords. This article explains why and how to obtain these new tokens from a specialized corpus.
- notebook [nlp_how_to_add_a_domain_specific_vocabulary_new_tokens_to_a_subword_tokenizer_already_trained_like_BERT_WordPiece.ipynb](https://github.com/piegu/language-models/blob/master/nlp_how_to_add_a_domain_specific_vocabulary_new_tokens_to_a_subword_tokenizer_already_trained_like_BERT_WordPiece.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/nlp_how_to_add_a_domain_specific_vocabulary_new_tokens_to_a_subword_tokenizer_already_trained_like_BERT_WordPiece.ipynb)) ([notebook in colab](https://colab.research.google.com/drive/1Hlfv5wHbYW863c9MraDy9EBknIQf3uAr?usp=sharing))
- Blog post: [NLP | How to add a domain-specific vocabulary (new tokens) to a subword tokenizer already trained like BERT WordPiece](https://medium.com/@pierre_guillou/nlp-how-to-add-a-domain-specific-vocabulary-new-tokens-to-a-subword-tokenizer-already-trained-33ab15613a41)

## NLP | Modelo de Question Answering em qualquer idioma baseado no BERT base (estudo de caso em portuguÃªs)

- notebook [colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb](https://github.com/piegu/language-models/blob/master/colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb)): training code of a Portuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1
- Blog post: [NLP | Modelo de Question Answering em qualquer idioma baseado no BERT base (estudo de caso em portuguÃªs)](https://medium.com/@pierre_guillou/nlp-modelo-de-question-answering-em-qualquer-idioma-baseado-no-bert-base-estudo-de-caso-em-12093d385e78)
- Model in the Model Hub of Hugging Face: [Portuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1](https://huggingface.co/pierreguillou/bert-base-cased-squad-v1.1-portuguese)

## Portuguese

I trained 1 Portuguese Bidirectional Language Model (PBLM) with the [MultiFit](https://arxiv.org/pdf/1909.04761.pdf) configuration with 1 NVIDIA GPU v100 on [GCP](https://cloud.google.com).

**WARNING**: a Bidirectional LM model using the MultiFiT configuration is a good model to perform text classification but with only 46 millions of parameters, it is far from being a LM that can compete with [GPT-2](https://openai.com/blog/better-language-models/) or [BERT](https://arxiv.org/abs/1810.04805) in NLP tasks like text generation. This my next step ;-) 

**Note**: The training times shown in the tables on this page are the sum of the creation time of Fastai Databunch (forward and backward) and the training duration of the bidirectional model over 10 periods. The download time of the Wikipedia corpus and its preparation time are not counted.

### MultiFiT configuration (architecture 4 QRNN with 1550 hidden parameters by layer / tokenizer SentencePiece (15 000 tokens))
- notebook [lm3-portuguese.ipynb](https://github.com/piegu/language-models/blob/master/lm3-portuguese.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/lm3-portuguese.ipynb)): code used to train a Portuguese Bidirectional LM on a 100 millions corpus extrated from Wikipedia by using the [MultiFiT](https://arxiv.org/pdf/1909.04761.pdf) configuration.
- link to download pre-trained parameters and vocabulary in [models](https://github.com/piegu/language-models/tree/master/models)

| PBLM | accuracy | perplexity | training time |
| ------------- | ------------- | ------------- | ------------- |
| forward   | 39.68%  | 21.76  | 8h |
| backward  | 43.67%  | 22.16  | 8h |

- Applications:
  - notebook [lm3-portuguese-classifier-TCU-jurisprudencia.ipynb](https://github.com/piegu/language-models/blob/master/lm3-portuguese-classifier-TCU-jurisprudencia.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/lm3-portuguese-classifier-TCU-jurisprudencia.ipynb)): code used to fine-tune a Portuguese Bidirectional LM and a Text Classifier on "(reduzido) TCU jurisprudÃªncia" dataset.
  - notebook [lm3-portuguese-classifier-olist.ipynb](https://github.com/piegu/language-models/blob/master/lm3-portuguese-classifier-olist.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/lm3-portuguese-classifier-olist.ipynb)): code used to fine-tune a Portuguese Bidirectional LM and a Sentiment Classifier on "Brazilian E-Commerce Public Dataset by Olist" dataset. 

**[ WARNING ]** The code of this notebook [lm3-portuguese-classifier-olist.ipynb](https://github.com/piegu/language-models/blob/master/lm3-portuguese-classifier-olist.ipynb) must be updated in order to use the SentencePiece model and vocab already trained for the Portuguese Language Model in the notebook [lm3-portuguese.ipynb](https://github.com/piegu/language-models/blob/master/lm3-portuguese.ipynb) as it was done in the notebook [lm3-portuguese-classifier-TCU-jurisprudencia.ipynb](https://github.com/piegu/language-models/blob/master/lm3-portuguese-classifier-TCU-jurisprudencia.ipynb) (see explanations at the top of this notebook).
  
Here's an example of using the classifier to predict the category of a TCU legal text:

![Using the classifier to predict the category of TCU legal texts](https://miro.medium.com/max/6275/1*nKPnG0hJnTrW0xV-T1DQ9A.jpeg "Using the classifier to predict the category of TCU legal texts")

## French

I trained 3 French Bidirectional Language Models (FBLM) with 1 NVIDIA GPU v100 on [GCP](https://cloud.google.com) but **the best is the one trained with the [MultiFit](https://arxiv.org/pdf/1909.04761.pdf) configuration**.

| French Bidirectional Language Models (FBLM) | | accuracy | perplexity | training time |
| ------------- | ------------- | ------------- | ------------- | ------------- |
| [MultiFiT with 4 QRNN + SentencePiece (15 000 tokens)](https://github.com/piegu/language-models/blob/master/lm3-french.ipynb) | forward   | 43.77%  | 16.09  | 8h40 |
| | backward  | 49.29%  | 16.58  | 8h10 | 
| [ULMFiT with 3 QRNN + SentencePiece (15 000 tokens)](https://github.com/piegu/language-models/blob/master/lm2-french.ipynb) | forward   | 40.99%  | 19.96  | 5h30 |
| | backward  | 47.19%  | 19.47  | 5h30 | 
| [ULMFiT with 3 AWD-LSTM + spaCy (60 000 tokens)](https://github.com/piegu/language-models/blob/master/lm-french.ipynb) | forward   | 36.44%  | 25.62  | 11h |
| | backward  | 42.65%  | 27.09  | 11h | 

### 1. MultiFiT configuration (architecture 4 QRNN with 1550 hidden parameters by layer / tokenizer SentencePiece (15 000 tokens))
- notebook [lm3-french.ipynb](https://github.com/piegu/language-models/blob/master/lm3-french.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/lm3-french.ipynb)): code used to train a French Bidirectional LM on a 100 millions corpus extrated from Wikipedia by using the [MultiFiT](https://arxiv.org/pdf/1909.04761.pdf) configuration.
- link to download pre-trained parameters and vocabulary in [models](https://github.com/piegu/language-models/tree/master/models)

| FBLM | accuracy | perplexity | training time |
| ------------- | ------------- | ------------- | ------------- |
| forward   | 43.77%  | 16.09  | 8h40 |
| backward  | 49.29%  | 16.58  | 8h10 | 

- Application: notebook [lm3-french-classifier-amazon.ipynb](https://github.com/piegu/language-models/blob/master/lm3-french-classifier-amazon.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/lm3-french-classifier-amazon.ipynb)): code used to fine-tune a French Bidirectional LM and a Sentiment Classifier on "French Amazon Customer Reviews" dataset.

Here's an example of using the classifier to predict the feeling of comments on an amazon product:

![Using the classifier to predict the feeling of comments on an amazon product](https://miro.medium.com/max/2630/1*HswVRzYjkFfom8BZLUWqjg.png "Using the classifier to predict the feeling of comments on an amazon product")

### 2. Architecture QRNN / tokenizer SentencePiece 
- notebook [lm2-french.ipynb](https://github.com/piegu/language-models/blob/master/lm2-french.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/lm2-french.ipynb)): code used to train a French Bidirectional LM on a 100 millions corpus extrated from Wikipedia
- link to download pre-trained parameters and vocabulary in [models](https://github.com/piegu/language-models/tree/master/models)

| FBLM | accuracy | perplexity | training time |
| ------------- | ------------- | ------------- | ------------- |
| forward   | 40.99%  | 19.96  | 5h30 |
| backward  | 47.19%  | 19.47  | 5h30 | 

- Application: notebook [lm2-french-classifier-amazon.ipynb](https://github.com/piegu/language-models/blob/master/lm2-french-classifier-amazon.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/lm2-french-classifier-amazon.ipynb)): code used to fine-tune a French Bidirectional LM and a Sentiment Classifier on "French Amazon Customer Reviews" dataset.

### 3. Architecture AWD-LSTM / tokenizer spaCy 
- notebook [lm-french.ipynb](https://github.com/piegu/language-models/blob/master/lm-french.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/lm-french.ipynb)): code used to train a French Bidirectional LM on a 100 millions corpus extrated from Wikipedia
- link to download pre-trained parameters and vocabulary in [models](https://github.com/piegu/language-models/tree/master/models)

| FBLM | accuracy | perplexity | training time |
| ------------- | ------------- | ------------- | ------------- |
| forward   | 36.44%  | 25.62  | 11h |
| backward  | 42.65%  | 27.09  | 11h | 

- Application: notebook [lm-french-classifier-amazon.ipynb](https://github.com/piegu/language-models/blob/master/lm-french-classifier-amazon.ipynb) ([nbviewer of the notebook](https://nbviewer.jupyter.org/github/piegu/language-models/blob/master/lm-french-classifier-amazon.ipynb)): code used to fine-tune a French Bidirectional LM and a Sentiment Classifier on "French Amazon Customer Reviews" dataset.
